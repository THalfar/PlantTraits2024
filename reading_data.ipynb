{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "train_df.head()\n",
    "\n",
    "# test_df.info()\n",
    "# test_df.head()  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetV2M\n",
    "import os \n",
    "import numpy as np\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "train_images_path = './data/train_images/'\n",
    "test_images_path = './data/test_images/'    \n",
    "batch_size = 32\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "base_model = EfficientNetV2M(weights='imagenet', include_top=False, pooling='avg')\n",
    "base_model.trainable = False\n",
    "\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "def extract_features_batch(image_paths):\n",
    "    img_batch = np.stack([load_and_preprocess_image(img_path) for img_path in image_paths])\n",
    "    features = base_model.predict(img_batch)\n",
    "    # tf.keras.backend.clear_session()\n",
    "    return features\n",
    "\n",
    "image_paths_train = [os.path.join(train_images_path, f\"{img_id}.jpeg\") for img_id in train_df['id']]\n",
    "features_list = []\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(image_paths_train)\n",
    "train_dataset = train_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# K채y l채pi dataset batcheina ja ker채채 piirteet\n",
    "features_list = []\n",
    "for batch in train_dataset:\n",
    "    batch_features = base_model.predict(batch)\n",
    "    features_list.append(batch_features)\n",
    "\n",
    "all_features = np.vstack(features_list)\n",
    "train_df['features'] = list(all_features)\n",
    "print(f'Train df info: {train_df.info()}')\n",
    "train_df.to_pickle('./data/train_df_with_features.pkl')\n",
    "\n",
    "image_paths_test = [os.path.join(test_images_path, f\"{img_id}.jpeg\") for img_id in test_df['id']]\n",
    "features_list = []\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(image_paths_test)\n",
    "test_dataset = test_dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "features_list = []\n",
    "for batch in train_dataset:\n",
    "    batch_features = base_model.predict(batch)\n",
    "    features_list.append(batch_features)\n",
    "\n",
    "all_features = np.vstack(features_list)\n",
    "test_df['features'] = list(all_features)\n",
    "print(f'Test df info: {test_df.info()}')\n",
    "test_df.to_pickle('./data/test_df_with_features.pkl')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
