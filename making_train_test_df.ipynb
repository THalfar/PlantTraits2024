{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetV2M\n",
    "import numpy as np\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "FEATURE_COLS = test_df.columns[1:].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_columns = [col for col in train_df.columns if col.endswith('_sd')]\n",
    "train_df.drop(columns=sd_columns, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images_path = './data/train_images/'\n",
    "test_images_path = './data/test_images/'    \n",
    "\n",
    "train_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(train_images_path, f'{x}.jpeg'))\n",
    "test_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(test_images_path, f'{x}.jpeg'))\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_data(df, columns_names):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Setting up a grid of plots with 2 columns\n",
    "    n_cols = 6\n",
    "    n_rows = len(columns_names) // n_cols + (len(columns_names) % n_cols > 0)\n",
    "\n",
    "    for i, col in enumerate(columns_names):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_df, mean_columns)\n",
    "\n",
    "for column in mean_columns:\n",
    "    upper_quantile = train_df[column].quantile(0.98)  \n",
    "    train_df = train_df[(train_df[column] < upper_quantile)]\n",
    "    train_df = train_df[(train_df[column] > 0)]    \n",
    "\n",
    "plot_data(train_df, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, trait in enumerate(mean_columns):\n",
    "\n",
    "    # Determine the bin edges dynamically based on the distribution of traits\n",
    "    bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, 5 + 1))\n",
    "    train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n",
    "\n",
    "# Concatenate the bins into a final bin\n",
    "train_df[\"final_bin\"] = (\n",
    "    train_df[[f\"bin_{i}\" for i in range(len(mean_columns))]]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "\n",
    "# Perform the stratified split using final bin\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n",
    "    train_df.loc[valid_idx, \"fold\"] = fold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "base_model = EfficientNetV2M(weights='imagenet', include_top=False, pooling='avg')\n",
    "base_model.trainable = False\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features_batch(image_paths):\n",
    "    img_batch = np.stack([load_and_preprocess_image(img_path) for img_path in image_paths])\n",
    "    features = base_model.predict(img_batch)        \n",
    "    return features\n",
    "\n",
    "image_paths = train_df['image_path'].values\n",
    "\n",
    "features_list = []\n",
    "j = 0\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    batch_features = extract_features_batch(batch_paths)\n",
    "    features_list.append(batch_features)\n",
    "    j += 1\n",
    "    if j % 30 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        print(f'Clearing session')\n",
    "\n",
    "all_features = np.vstack(features_list)\n",
    "train_df['features_avg'] = list(all_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "base_model = EfficientNetV2M(weights='imagenet', include_top=False, pooling='max')\n",
    "base_model.trainable = False\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features_batch(image_paths):\n",
    "    img_batch = np.stack([load_and_preprocess_image(img_path) for img_path in image_paths])\n",
    "    features = base_model.predict(img_batch)        \n",
    "    return features\n",
    "\n",
    "image_paths = train_df['image_path'].values\n",
    "\n",
    "features_list = []\n",
    "j = 0\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    batch_features = extract_features_batch(batch_paths)\n",
    "    features_list.append(batch_features)\n",
    "    j += 1\n",
    "    if j % 30 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        print(f'Clearing session')\n",
    "\n",
    "all_features = np.vstack(features_list)\n",
    "train_df['features_max'] = list(all_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.head(10))\n",
    "print(train_df.describe())\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['features'] = train_df.apply(lambda x: np.concatenate([x['features_avg'], x['features_max']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['features'].head())\n",
    "print(train_df['features'].iloc[0].shape)\n",
    "print(train_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = './data/train_df.pickle'\n",
    "\n",
    "# Save the train_df dataframe as a pickle file\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(train_df, f)\n",
    "\n",
    "\n",
    "# # Load the train_df dataframe from the pickle file\n",
    "# with open(pickle_file_path, 'rb') as f:\n",
    "#     train_df = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "base_model = EfficientNetV2M(weights='imagenet', include_top=False, pooling='avg')\n",
    "base_model.trainable = False\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features_batch(image_paths):\n",
    "    img_batch = np.stack([load_and_preprocess_image(img_path) for img_path in image_paths])\n",
    "    features = base_model.predict(img_batch)        \n",
    "    return features\n",
    "\n",
    "image_paths = test_df['image_path'].values\n",
    "\n",
    "features_list = []\n",
    "j = 0\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    batch_features = extract_features_batch(batch_paths)\n",
    "    features_list.append(batch_features)\n",
    "    j += 1\n",
    "    if j % 30 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        print(f'Clearing session')\n",
    "\n",
    "all_features = np.vstack(features_list)\n",
    "test_df['features_avg'] = list(all_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "base_model = EfficientNetV2M(weights='imagenet', include_top=False, pooling='max')\n",
    "base_model.trainable = False\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features_batch(image_paths):\n",
    "    img_batch = np.stack([load_and_preprocess_image(img_path) for img_path in image_paths])\n",
    "    features = base_model.predict(img_batch)        \n",
    "    return features\n",
    "\n",
    "image_paths = test_df['image_path'].values\n",
    "\n",
    "features_list = []\n",
    "j = 0\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    batch_features = extract_features_batch(batch_paths)\n",
    "    features_list.append(batch_features)\n",
    "    j += 1\n",
    "    if j % 30 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        print(f'Clearing session')\n",
    "\n",
    "all_features = np.vstack(features_list)\n",
    "test_df['features_max'] = list(all_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.shape)\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['features'] = test_df.apply(lambda x: np.concatenate([x['features_avg'], x['features_max']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df['features'].head())\n",
    "print(test_df['features'].iloc[0].shape)\n",
    "print(test_df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = './data/test_df.pickle'\n",
    "\n",
    "# # Save the train_df dataframe as a pickle file\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n",
    "\n",
    "# with open(pickle_file_path, 'rb') as f:\n",
    "#     test_df = pickle.load(f)\n",
    "\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
