{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-17 11:49:27.254932: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-17 11:49:27.854957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m \n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler,  RobustScaler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# import tensorflow_addons as tfa\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, Lambda, GlobalAveragePooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2M, ConvNeXtBase, ConvNeXtLarge\n",
    "\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    " \n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(f'Current GPU allocator: {os.getenv(\"TF_GPU_ALLOCATOR\")}')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(f'Setting memory growth for {gpu}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '514_convnextlarge_maxavg_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Aseta näyttämään rajoittamaton määrä sarakkeita\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_file_path = './data/test_df.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "pickle_file_path = './data/train_df.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING IMAGE AUGEMENTATION\n",
    "# train_df = train_df.sample(1000)\n",
    "# test_df = test_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv('./data/test.csv')\n",
    "FEATURE_COLS = feat.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_feat = RobustScaler()\n",
    "\n",
    "train_original = train_df.copy()\n",
    "train_plot = train_df.copy()\n",
    "sample_df = train_df.copy()\n",
    "\n",
    "train_df = sample_df[sample_df.fold != 2]\n",
    "valid_df = sample_df[sample_df.fold == 2]\n",
    "\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.metrics import MeanAbsoluteError\n",
    "\n",
    "directory_path = './NN_search'\n",
    "pattern = f\"{directory_path}/{study_name}*.h5\"\n",
    "\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "max_r2_score = float('-inf')\n",
    "best_model = None\n",
    "\n",
    "# Käy läpi jokainen tiedosto ja etsi suurin r2_score_inv\n",
    "for file in files:\n",
    "    value = float(file.split('best_val')[1].split('_')[1])\n",
    "    if value > max_r2_score:\n",
    "        max_r2_score = value\n",
    "        best_model = file\n",
    "\n",
    "\n",
    "# Tulosta suurin löydetty r2_score_inv ja vastaava tiedosto\n",
    "print(f\"Best R2-score: {max_r2_score:.5f}\")\n",
    "if best_model:\n",
    "    print(f\"Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"No best model found\")\n",
    "\n",
    "best_log_transforms_name =  f'./NN_search/{study_name}_{max_r2_score:.5f}_best_log_transforms.pickle'\n",
    "best_scalers_name = f'./NN_search/{study_name}_{max_r2_score:.5f}_best_scalers.pickle'\n",
    "\n",
    "print(f'Opening log transforms from {best_log_transforms_name}')\n",
    "with open(best_log_transforms_name, 'rb') as f:\n",
    "    log_transforms = pickle.load(f)\n",
    "\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "    \n",
    "custom_objects = {\n",
    "    \"r2_score_tf\": r2_score_tf\n",
    "    \n",
    "}\n",
    "\n",
    "nas_model  = tf.keras.models.load_model(best_model, custom_objects=custom_objects)\n",
    "\n",
    "nas_model.summary()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_features = nas_model.layers[-2].output\n",
    "nas_features = Model(inputs=nas_model.input, outputs=nas_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in nas_features.layers:\n",
    "#     layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_features.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable_count_nas = sum([tf.size(v).numpy() for v in nas_model.trainable_weights])\n",
    "non_trainable_count_nas = sum([tf.size(v).numpy() for v in nas_model.non_trainable_weights])\n",
    "print(f\"Total parameters nas: {trainable_count_nas + non_trainable_count_nas:,}\")\n",
    "print(f\"Trainable parameters nas: {trainable_count_nas:,}\")\n",
    "print(f\"Non-trainable parameters nas: {non_trainable_count_nas:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scaler are: {scaler_transforms}')\n",
    "print(f'Log transforms are: {log_transforms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def augment_image(img):\n",
    "\n",
    "    img = img / 255.0\n",
    "\n",
    "    \n",
    "    crop_size = tf.random.uniform(shape=[], minval=400, maxval=480, dtype=tf.int32)\n",
    "    img = tf.image.random_crop(img, size=[crop_size, crop_size, 3])\n",
    "    img = tf.image.resize(img, [480, 480]) \n",
    "    \n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "    img = tf.image.rot90(img, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "\n",
    "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
    "    img = tf.image.random_hue(img, max_delta=0.1)\n",
    "    img = tf.image.random_saturation(img, lower=0.9, upper=1.1)\n",
    "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "\n",
    "    img = tf.image.random_jpeg_quality(img, min_jpeg_quality=85, max_jpeg_quality=100)\n",
    "\n",
    "    angle = tf.random.uniform([], minval=-np.pi/8, maxval=np.pi/8, dtype=tf.float32)\n",
    "    img = tfa.image.rotate(img, angle)\n",
    "\n",
    "    \n",
    "    \n",
    "    img = tf.image.resize(img, [480, 480]) \n",
    "    img = img * 255.0\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    img = augment_image(img)  \n",
    "    # img = tf.cast(img, tf.uint8)    \n",
    "\n",
    "    # tf.print(\"Final min and max in process_images:\", tf.reduce_min(img), tf.reduce_max(img))\n",
    "    # tf.print(\"Image type: \", img.dtype)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_image_valid(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    # tf.print(\"Final min and max in process_image_valid:\", tf.reduce_min(img), tf.reduce_max(img))\n",
    "    # img = tf.cast(img, tf.uint8)\n",
    "    return img\n",
    "\n",
    "# Define your dataset processing function\n",
    "def process_path_train(file_path, targets):\n",
    "    img = process_image(file_path)\n",
    "    return img, targets\n",
    "\n",
    "\n",
    "def process_path_valid(file_path, targets):\n",
    "    img = process_image_valid(file_path)\n",
    "    return img, targets\n",
    "\n",
    "def process_path_test(file_path, dummy):\n",
    "    img = process_image_valid(file_path)    \n",
    "    return img, dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[mean_columns]\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n",
    "\n",
    "y_train_transformed = y_train.copy()\n",
    "y_valid_transformed = y_valid.copy()\n",
    "\n",
    "for target, log_base in log_transforms.items():\n",
    "\n",
    "    y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "    y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "\n",
    "y_train_transformed = scaler_transforms.fit_transform(y_train_transformed)\n",
    "y_valid_transformed = scaler_transforms.transform(y_valid_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 55000 \n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, y_train_transformed))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, y_valid_transformed))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "len_dataset = len(train_dataset) \n",
    "len_all_train = len(train_dataset) * EPOCHS\n",
    "\n",
    "print(f'Len dataset: {len_dataset}')\n",
    "print(f'Len all train: {len_all_train}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, decay_steps, alpha=0.0, warmup_learning_rate=0.0, warmup_steps=0, name=None):\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"CustomCosineDecayWithWarmup\"):\n",
    "            # Lämpenemisvaihe\n",
    "            learning_rate = tf.cond(\n",
    "                step < self.warmup_steps,\n",
    "                lambda: self.warmup_learning_rate + step / self.warmup_steps * (self.initial_learning_rate - self.warmup_learning_rate),\n",
    "                lambda: self.initial_learning_rate\n",
    "            )\n",
    "            # Kosinilasku lämpenemisen jälkeen\n",
    "            cosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=self.initial_learning_rate,\n",
    "                decay_steps=self.decay_steps,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            decayed_learning_rate = cosine_decay(step - self.warmup_steps)\n",
    "            \n",
    "            return tf.cond(step < self.warmup_steps, lambda: learning_rate, lambda: decayed_learning_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"warmup_learning_rate\": self.warmup_learning_rate,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "\n",
    "\n",
    "image_input = Input(shape=(480, 480, 3), name='image_input')\n",
    "\n",
    "convNeXt_large =  ConvNeXtLarge(weights='imagenet', include_top=False, pooling=None, input_tensor=image_input)\n",
    "\n",
    "convNeXt_large.trainable = True\n",
    "for layer in convNeXt_large.layers[:-50]:\n",
    "    layer.trainable = False\n",
    "\n",
    "avg_pool = GlobalAveragePooling2D()(convNeXt_large.output)\n",
    "max_pool = GlobalMaxPooling2D()(convNeXt_large.output)\n",
    "\n",
    "nas_features_layer = nas_features([avg_pool, max_pool])\n",
    "pass_through_layer = Lambda(lambda x: x, name='nas_features')(nas_features_layer)\n",
    "\n",
    "nas_output = Dense(6, activation='linear', name='final_tune_output')(pass_through_layer)\n",
    "\n",
    "finetune_model = Model(inputs=image_input, outputs=nas_output, name='finetune_model')\n",
    "\n",
    "lr_schedule = CustomCosineDecayWithWarmup(\n",
    "    initial_learning_rate=5e-5,\n",
    "    decay_steps=len_dataset * (EPOCHS - 1),\n",
    "    alpha=0.0,\n",
    "    warmup_learning_rate=1e-6,\n",
    "    warmup_steps=len_dataset ,\n",
    "    name=\"CosineDecayWithWarmup\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Aseta oppimisnopeuden aikataulu\n",
    "finetune_model.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule), loss='mae', metrics=['mse', 'mae', 'mape', r2_score_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_count = sum([tf.size(v).numpy() for v in finetune_model.trainable_weights])\n",
    "non_trainable_count = sum([tf.size(v).numpy() for v in finetune_model.non_trainable_weights])\n",
    "print(f\"Total parameters: {trainable_count + non_trainable_count:,}\")\n",
    "print(f\"Trainable parameters: {trainable_count:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n",
    "\n",
    "print(f'Total parameters from ConvNext: { (trainable_count + non_trainable_count) - (trainable_count_nas + non_trainable_count_nas):,}')\n",
    "print(f'Trainable from ConvNext: {trainable_count - trainable_count_nas:,}')\n",
    "print(f'Non trainable from ConvNext: {non_trainable_count - non_trainable_count_nas:,}')\n",
    "\n",
    "print(f'Trainable fron NAS: {trainable_count_nas:,}')\n",
    "print(f'Non trainable from NAS: {non_trainable_count_nas:,}')\n",
    "print(F'Total parameters from NAS: {trainable_count_nas + non_trainable_count_nas:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "class TrainImageLoggingCallback(Callback):\n",
    "    def __init__(self, log_dir, data):\n",
    "        super(TrainImageLoggingCallback, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.data = data\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Fetch a batch of images\n",
    "        for imgs, tar in self.data.take(1):  # Adjust depending on your dataset structure\n",
    "            \n",
    "            # augmented_images = tf.map_fn(augment_image, imgs)\n",
    "            augmented_images = tf.cast(imgs, tf.uint8)    \n",
    "        \n",
    "            # Prepare the image to write to TensorBoard\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.image(\"Augmented Images\", augmented_images, step=epoch, max_outputs=20)\n",
    "\n",
    "            self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_folder = f\"./logs/lr_test/trial_{study_name}_516\" # MUUTA!\n",
    "\n",
    "print(f'Logging tensorboard to {log_folder}')\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "# Aseta logitiedostojen hakemisto\n",
    "tensorboard_callback = TensorBoard(log_dir=log_folder, histogram_freq=1, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "checkpoint_filepath = './NN_search/model_epoch_{epoch:02d}.h5'\n",
    "\n",
    "model_checkpoint_callback =  tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',  # Tallenna jokaisen eepokin jälkeen\n",
    "    verbose=1,  # Näytä viesti tallennuksesta\n",
    "    save_best_only=False  # Tallenna jokainen eepokki, älä vain parasta\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    model_checkpoint_callback,\n",
    "    tensorboard_callback,\n",
    "    TrainImageLoggingCallback(log_folder, train_dataset)    \n",
    "]\n",
    "\n",
    "history = finetune_model.fit(train_dataset, validation_data=valid_dataset, epochs=EPOCHS, verbose=1, callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            self.init_values * tf.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "class StochasticDepth(layers.Layer):\n",
    "    \"\"\"Stochastic Depth module.\n",
    "\n",
    "    It performs batch-wise dropping rather than sample-wise. In libraries like\n",
    "    `timm`, it's similar to `DropPath` layers that drops residual paths\n",
    "    sample-wise.\n",
    "\n",
    "    References:\n",
    "      - https://github.com/rwightman/pytorch-image-models\n",
    "\n",
    "    Args:\n",
    "      drop_path_rate (float): Probability of dropping paths. Should be within\n",
    "        [0, 1].\n",
    "\n",
    "    Returns:\n",
    "      Tensor either with the residual path dropped or kept.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_path_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_path_rate\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"drop_path_rate\": self.drop_path_rate})\n",
    "        return config\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "        try: \n",
    "            ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "            ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "            r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "            r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "            return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "        except Exception as e:\n",
    "            # print(f'Error in r2_score_tf: {e}')\n",
    "            return float('-inf')\n",
    "\n",
    "\n",
    "    \n",
    "custom_objects = {\"r2_score_tf\": r2_score_tf, \"LayerScale\": LayerScale, \"StochasticDepth\": StochasticDepth, \"CustomCosineDecayWithWarmup\": CustomCosineDecayWithWarmup}\n",
    "\n",
    "for epoch in range(1, EPOCHS+2):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    model = tf.keras.models.load_model(f'./NN_search/model_epoch_{epoch:02d}.h5', custom_objects=custom_objects)\n",
    "    valid_pred = model.predict(valid_dataset, verbose=1)\n",
    "\n",
    "    valid_pred = scaler_transforms.inverse_transform(valid_pred)\n",
    "\n",
    "\n",
    "    for i, target in enumerate(mean_columns):\n",
    "        log_base = log_transforms[target]\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "\n",
    "\n",
    "    R2_valid = r2_score(y_valid, valid_pred)\n",
    "    MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "    MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "    MAPE_valid = mean_absolute_percentage_error(y_valid, valid_pred)\n",
    "    print(f'Study name {study_name}')\n",
    "    print(f'Model at epoch {epoch}:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}, MAPE : {MAPE_valid:.5f}')\n",
    "\n",
    "    del model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = tf.keras.models.load_model('./NN_search/model_epoch_05.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, log_base in log_transforms.items():\n",
    "   \n",
    "    train_plot[target] = np.log(train_plot[target]) / np.log(log_base)\n",
    "   \n",
    "        \n",
    "train_plot[mean_columns] = scaler_transforms.transform(train_plot[mean_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_transforms)\n",
    "print(scaler_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, columns_names):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Setting up a grid of plots with 2 columns\n",
    "    n_cols = 6\n",
    "    n_rows = len(columns_names) // n_cols + (len(columns_names) % n_cols > 0)\n",
    "\n",
    "    for i, col in enumerate(columns_names):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_original, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_plot, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training_name = './data/results_finetune_images.pickle'\n",
    "\n",
    "if os.path.exists(results_training_name):\n",
    "    results_training = pd.read_pickle(results_training_name)\n",
    "else:\n",
    "    columns = ['Train R2', 'Train MSE', 'Train MAE', 'Train MAPE', 'Valid R2', 'Valid MSE', 'Valid MAE', 'Valid MAPE', 'Train preds Desc', 'Valid preds Desc', 'Test preds Desc' , 'Original data Desc' 'Kaggle R2', 'Scalers', 'Log/Pot transforms']\n",
    "    results_training = pd.DataFrame(columns = columns)\n",
    "    results_training.index.name = 'Study name'\n",
    "\n",
    "study_name_result = f'{study_name}_finetuned'\n",
    "\n",
    "if study_name_result not in results_training.index:    \n",
    "    results_training.loc[study_name] = [None]*len(results_training.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "submission_df = test_df_copy[['id']].copy()\n",
    "\n",
    "test_images_path = test_df_copy['image_path'].values\n",
    "\n",
    "dummy_y = np.zeros((len(test_df_copy), 6))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_path, dummy_y))\n",
    "test_dataset = test_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(16).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "predictions = finetune_model.predict(test_dataset, verbose=1)\n",
    "\n",
    "predictions = scaler_transforms.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target: : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target] \n",
    "    predictions[:, i] = np.power(log_base, predictions[:, i])\n",
    " \n",
    "\n",
    "# test_preds_desc =  pd.DataFrame(predictions, columns = mean_columns).describe().to_json()\n",
    "# results_training.at[study_name_result, 'Test preds Desc'] = test_preds_desc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns = mean_columns)\n",
    "predictions_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_columns = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n",
    "\n",
    "submission_df[target_columns] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_desc = train_original[mean_columns].describe().to_json()\n",
    "results_training.at[study_name_result, 'Original data Desc'] = original_data_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{str(log_transforms.items())}')\n",
    "print(f'{str(scaler_transforms)}')\n",
    "\n",
    "results_training.at[study_name_result, 'Scalers'] = f'{scaler_transforms}'\n",
    "results_training.at[study_name_result, 'Log/Pot transforms'] = f'{str(log_transforms.items())}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_training.at[study_name_result, 'Kaggle R2'] = None\n",
    "\n",
    "\n",
    "# # results_training.drop('423_std_powerlog_3', inplace=True)\n",
    "# # results_training.head()\n",
    "\n",
    "# for index, row in results_training.iterrows():\n",
    "#     print(f\"Study Name: {index}\")\n",
    "#     print(f'Kaggle R2: {row[\"Kaggle R2\"]}')\n",
    "#     print(f\"Train R2: {row['Train R2']}, Train MSE: {row['Train MSE']}, Train MAE : {row['Train MAE']}, Train MAPE: {row['Train MAPE']}\")\n",
    "#     print(f'Valid R2: {row[\"Valid R2\"]}, Valid MSE: {row[\"Valid MSE\"]}, Valid MAE: {row[\"Valid MAE\"]}, Valid MAPE: {row[\"Valid MAPE\"]}')\n",
    "#     print(\"-\" * 50)\n",
    "#     print(\"Train preds Description:\")\n",
    "#     display(pd.read_json(row['Train preds Desc']))\n",
    "#     print(\"Valid preds Description:\")\n",
    "#     display(pd.read_json(row['Valid preds Desc']))\n",
    "#     print(\"Test preds Description:\")\n",
    "#     display(pd.read_json(row['Test preds Desc']))\n",
    "#     print(\"Original data Description:\")\n",
    "#     display(pd.read_json(row['Original data Desc']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df.info())\n",
    "\n",
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_training_name, 'wb') as f:\n",
    "    results_training.to_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model_output = finetune_model.get_layer('nas_features')\n",
    "feature_model = Model(inputs=finetune_model.input, outputs=feature_model_output.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_paths = train_original['image_path'].values\n",
    "dummy_y = np.zeros((len(train_original), 6))\n",
    "\n",
    "train_all_dataset = tf.data.Dataset.from_tensor_slices((train_all_paths, dummy_y))\n",
    "train_all_dataset = train_all_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_all_dataset = train_all_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_features = feature_model.predict(train_all_dataset, verbose=1)\n",
    "\n",
    "train_original[f'model_features_{study_name}'] = train_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = f'./data/train_df.pickle'\n",
    "\n",
    "print(f'Saving train_df to {pickle_file_path}')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(train_original, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_paths = test_df['image_path'].values\n",
    "dummy_y = np.zeros((len(test_df), 6))\n",
    "\n",
    "test_all_dataset = tf.data.Dataset.from_tensor_slices((test_all_paths, dummy_y))\n",
    "test_all_dataset = test_all_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_all_dataset = test_all_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_features = feature_model.predict(test_all_dataset, verbose=1)\n",
    "\n",
    "test_df[f'model_features_{study_name}'] = test_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[f'model_features_{study_name}'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testa = test_df[f'model_features_{study_name}'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testa = np.array(testa)\n",
    "testa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = f'./data/test_df.pickle'\n",
    "\n",
    "print(f'Saving test_df to {pickle_file_path}')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
