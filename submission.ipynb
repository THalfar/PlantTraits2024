{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = './data/test_df.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "pickle_file_path = './data/train_df.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "    \n",
    "\n",
    "study_name = '414_std_3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "\n",
    "\n",
    "selected_features_pickle_path = './data/selected_features_list.pickle'\n",
    "with open(selected_features_pickle_path, 'rb') as f:\n",
    "    FEATURE_COLS = pickle.load(f)\n",
    "\n",
    "print(FEATURE_COLS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, columns_names):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Setting up a grid of plots with 2 columns\n",
    "    n_cols = 6\n",
    "    n_rows = len(columns_names) // n_cols + (len(columns_names) % n_cols > 0)\n",
    "\n",
    "    for i, col in enumerate(columns_names):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_df, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler, PowerTransformer, QuantileTransformer\n",
    "\n",
    "# TODO automatize this\n",
    "train_plot = train_df.copy()\n",
    "\n",
    "train_plot['X4_mean'] = np.log(train_plot['X4_mean']) / np.log(2)\n",
    "train_plot['X11_mean'] = np.log(train_plot['X11_mean']) / np.log(15)\n",
    "train_plot['X18_mean'] = np.sqrt(train_plot['X18_mean'])\n",
    "train_plot['X26_mean'] = np.sqrt(train_plot['X26_mean'])\n",
    "train_plot['X3112_mean'] = np.log(train_plot['X3112_mean']) / np.log(15)\n",
    "\n",
    "train_plot['X11_mean'] = StandardScaler().fit_transform(train_plot[['X11_mean']])\n",
    "train_plot['X26_mean'] = MinMaxScaler().fit_transform(train_plot[['X26_mean']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'optimizer': 'rmsprop', 'Log_X4_mean': 'log2', 'Log_X11_mean': 'log15', 'Log_X18_mean': 'sqrt', 'Log_X50_mean': 'none', 'Log_X26_mean': 'sqrt', 'Log_X3112_mean': 'log15', 'Scaler_X4_mean': 'None', 'Scaler_X11_mean': 'Std', 'Scaler_X18_mean': 'None', 'Scaler_X50_mean': 'None', 'Scaler_X26_mean': 'minmax', 'Scaler_X3112_mean': 'None'}. Best is trial 285 with value: 0.2963744779721767."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_plot, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[FEATURE_COLS].describe()\n",
    "train_df_original = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "print(train_df['fold'].value_counts())\n",
    "\n",
    "# scaler = StandardScaler() # TODO testaa robustscaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "sample_df = train_df.copy()\n",
    "train_df = sample_df[sample_df.fold != 3]\n",
    "valid_df = sample_df[sample_df.fold == 3]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "\n",
    "train_df[FEATURE_COLS] = scaler.fit_transform(train_df[FEATURE_COLS].values)\n",
    "valid_df[FEATURE_COLS] = scaler.transform(valid_df[FEATURE_COLS].values)\n",
    "\n",
    "scaler_tabufeatures_name = f'./NN_search/scaler_tabufeatures_{study_name}_train.pickle'\n",
    "print(f\"Saving scaler to {scaler_tabufeatures_name}\")\n",
    "with open(f'{scaler_tabufeatures_name}', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[FEATURE_COLS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[FEATURE_COLS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tab = train_df[FEATURE_COLS].values\n",
    "X_train_feat = np.stack(train_df['features'].values)\n",
    "# y_train = train_df[mean_columns].values\n",
    "y_train = train_df[mean_columns]\n",
    "\n",
    "X_valid_tab = valid_df[FEATURE_COLS].values \n",
    "X_valid_feat = np.stack(valid_df['features'].values)\n",
    "# y_valid = valid_df[mean_columns].values\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '415_std_minmax_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "directory_path = './NN_search'\n",
    "pattern = f\"{directory_path}/{study_name}_best_val_*.h5\"\n",
    "\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "max_r2_score = float('-inf')\n",
    "best_model = None\n",
    "\n",
    "# Käy läpi jokainen tiedosto ja etsi suurin r2_score_inv\n",
    "for file in files:\n",
    "    value = float(file.split('best_val')[1].split('_')[1])\n",
    "    if value > max_r2_score:\n",
    "        max_r2_score = value\n",
    "        best_model = file\n",
    "\n",
    "\n",
    "# Tulosta suurin löydetty r2_score_inv ja vastaava tiedosto\n",
    "print(f\"Best R2-score: {max_r2_score:.5f}\")\n",
    "if best_model:\n",
    "    print(f\"Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"No best model found\")\n",
    "\n",
    "best_log_transforms_name =  f'./NN_search/{study_name}_{max_r2_score:.5f}_best_log_transforms.pickle'\n",
    "best_scalers_name = f'./NN_search/{study_name}_{max_r2_score:.5f}_best_scalers.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "    r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "    r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2)  # Korvaa NaN-arvot nollilla\n",
    "    return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "\n",
    "custom_objects = {\"r2_score_tf\": r2_score_tf}\n",
    "\n",
    "\n",
    "with open(f'./NN_search/scaler_tabufeatures_{study_name}_train.pickle', 'rb') as f:\n",
    "    scaler_tabular = pickle.load(f)\n",
    "\n",
    "print(f'Tabu features scaler: {scaler}')\n",
    "\n",
    "\n",
    "\n",
    "best_model = tf.keras.models.load_model(f'{best_model}', custom_objects=custom_objects)\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "\n",
    "test_df_copy[FEATURE_COLS] = scaler_tabular.transform(test_df_copy[FEATURE_COLS].values)\n",
    "\n",
    "submission_df = test_df_copy[['id']].copy()\n",
    "\n",
    "X_test_tab = test_df_copy[FEATURE_COLS].values\n",
    "X_test_feat = np.stack(test_df_copy['features'].values) \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "predictions = best_model.predict([X_test_feat, X_test_tab])\n",
    "\n",
    "print(f'Opening log transforms from {best_log_transforms_name}')\n",
    "with open(best_log_transforms_name, 'rb') as f:\n",
    "    log_transforms = pickle.load(f)\n",
    "\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "        \n",
    "\n",
    "print(log_transforms)\n",
    "print(scaler_transforms)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        predictions[:, i] = scaler.inverse_transform(predictions[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target: : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        predictions[:, i] = np.power(log_base, predictions[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        predictions[:, i] = np.square(predictions[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        predictions[:, i] = np.power(predictions[:, i], 3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_columns = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n",
    "\n",
    "submission_df[target_columns] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training_name = './data/results_training.pickle'\n",
    "\n",
    "if os.path.exists(results_training_name):\n",
    "    results_training = pd.read_pickle(results_training_name)\n",
    "else:\n",
    "    columns = ['Train R2', 'Train MSE', 'Train MAE', 'Valid R2', 'Valid MSE', 'Valid MAE', 'Train preds Desc', 'Valid preds Desc', 'Test preds Desc' , 'Original data Desc' 'Kaggle R2', 'Scalers', 'Log/Pot transforms', 'NN Search space', 'Tabular scaler']\n",
    "    results_training = pd.DataFrame(columns = columns)\n",
    "    results_training.index.name = 'Study name'\n",
    "\n",
    "if study_name not in results_training.index:    \n",
    "    results_training.loc[study_name] = [None]*len(results_training.columns)\n",
    "\n",
    "\n",
    "test_preds_desc = submission_df[target_columns].describe().to_json()\n",
    "results_training.at[study_name, 'Test preds Desc'] = test_preds_desc \n",
    "\n",
    "original_data_desc = train_df_original[mean_columns].describe().to_json()\n",
    "results_training.at[study_name, 'Original data Desc'] = original_data_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{str(log_transforms.items())}')\n",
    "print(f'{str(scaler_transforms.items())}')\n",
    "print(f'{str(scaler_tabular)}')\n",
    "\n",
    "results_training.at[study_name, 'Scalers'] = f'{scaler_transforms}'\n",
    "results_training.at[study_name, 'Log/Pot transforms'] = f'{str(log_transforms.items())}'\n",
    "results_training.at[study_name, 'Tabular scaler'] = f'{scaler_tabular}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JÄRKEVYYSKOKEILU TESTATAAN train dataan\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "## TEST DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "train_pred = best_model.predict([X_train_feat, X_train_tab])\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        train_pred[:, i] = scaler.inverse_transform(train_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_pred[:, i] = np.power(log_base, train_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        train_pred[:, i] = np.square(train_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        train_pred[:, i] = np.power(train_pred[:, i], 3)\n",
    "\n",
    "R2_train = r2_score(y_train, train_pred)\n",
    "MSE_train = mean_squared_error(y_train, train_pred)\n",
    "MAE_train = mean_absolute_error(y_train, train_pred)\n",
    "\n",
    "print(f'Train scores:\\nR2 : {R2_train:.5f}, MSE : {MSE_train:.5f}, MAE : {MAE_train:.5f}')\n",
    "\n",
    "results_training.at[study_name, 'Train R2'] = R2_train\n",
    "results_training.at[study_name, 'Train MSE'] = MSE_train\n",
    "results_training.at[study_name, 'Train MAE'] = MAE_train\n",
    "\n",
    "trainining_preds_desc = pd.DataFrame(train_pred, columns = mean_columns).describe().to_json()\n",
    "results_training.at[study_name, 'Train preds Desc'] = trainining_preds_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## VALIDATION DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "valid_pred = best_model.predict([X_valid_feat, X_valid_tab])\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        valid_pred[:, i] = scaler.inverse_transform(valid_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        valid_pred[:, i] = np.square(valid_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        valid_pred[:, i] = np.power(valid_pred[:, i], 3)\n",
    "\n",
    "R2_valid = r2_score(y_valid, valid_pred)\n",
    "MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "\n",
    "print(f'Valid scores:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}')\n",
    "\n",
    "results_training.at[study_name, 'Valid R2'] = R2_valid\n",
    "results_training.at[study_name, 'Valid MSE'] = MSE_valid\n",
    "results_training.at[study_name, 'Valid MAE'] = MAE_valid\n",
    "\n",
    "valid_preds_desc = pd.DataFrame(valid_pred, columns = mean_columns).describe().to_json()\n",
    "results_training.at[study_name, 'Valid preds Desc'] = valid_preds_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(results_training.head(100))\n",
    "logpot = results_training['Log/Pot transforms'].to_list()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df.info())\n",
    "\n",
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle = 0.0 \n",
    "# results_training.at[study_name, 'Kaggle R2'] = Kaggle\n",
    "\n",
    "for index, row in results_training.iterrows():\n",
    "    print(f\"Study Name: {index}\")\n",
    "    print(f'Kaggle R2: {row[\"Kaggle R2\"]}')\n",
    "    print(f\"Train R2: {row['Train R2']}, Train MSE: {row['Train MSE']}, Train MAE : {row['Train MAE']}\")\n",
    "    print(f'Valid R2: {row[\"Valid R2\"]}, Valid MSE: {row[\"Valid MSE\"]}, Valid MAE: {row[\"Valid MAE\"]}')\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Train preds Description:\")\n",
    "    display(pd.read_json(row['Train preds Desc']))\n",
    "    print(\"Valid preds Description:\")\n",
    "    display(pd.read_json(row['Valid preds Desc']))\n",
    "    print(\"Test preds Description:\")\n",
    "    display(pd.read_json(row['Test preds Desc']))\n",
    "    print(\"Original data Description:\")\n",
    "    display(pd.read_json(row['Original data Desc']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_training_name, 'wb') as f:\n",
    "    results_training.to_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
