{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2M\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(f'Current GPU allocator: {os.getenv(\"TF_GPU_ALLOCATOR\")}')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(f'Setting memory growth for {gpu}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '423_std_powerlog_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Aseta näyttämään rajoittamaton määrä sarakkeita\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_file_path = './data/test_df.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "pickle_file_path = './data/train_df.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING IMAGE AUGEMENTATION\n",
    "# train_df = train_df.sample(1000)\n",
    "# test_df = test_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = test_df.copy()\n",
    "FEATURE_COLS = feat.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_path = './data/train_images/'\n",
    "# test_images_path = './data/test_images/'    \n",
    "\n",
    "# train_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(train_images_path, f'{x}.jpeg'))\n",
    "# test_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(test_images_path, f'{x}.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in mean_columns:\n",
    "#     lower_quantile = train_df[column].quantile(0.005)\n",
    "#     upper_quantile = train_df[column].quantile(0.985)  \n",
    "#     train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# for i, trait in enumerate(mean_columns):\n",
    "\n",
    "#     # Determine the bin edges dynamically based on the distribution of traits\n",
    "#     bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, 5 + 1))\n",
    "#     train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n",
    "\n",
    "# # Concatenate the bins into a final bin\n",
    "# train_df[\"final_bin\"] = (\n",
    "#     train_df[[f\"bin_{i}\" for i in range(len(mean_columns))]]\n",
    "#     .astype(str)\n",
    "#     .agg(\"\".join, axis=1)\n",
    "# )\n",
    "\n",
    "# # Perform the stratified split using final bin\n",
    "# train_df = train_df.reset_index(drop=True)\n",
    "# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n",
    "#     train_df.loc[valid_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_feat = RobustScaler()\n",
    "\n",
    "train_original = train_df.copy()\n",
    "train_plot = train_df.copy()\n",
    "sample_df = train_df.copy()\n",
    "\n",
    "train_df = sample_df[sample_df.fold != 3]\n",
    "valid_df = sample_df[sample_df.fold == 3]\n",
    "\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "scaler_tabufeatures_name = f'./NN_search/scaler_tabufeatures_{study_name}_train.pickle'\n",
    "print(f'Opening feature scaler from {scaler_tabufeatures_name}')\n",
    "with open(scaler_tabufeatures_name, 'rb') as f:\n",
    "    scaler_feat = pickle.load(f)\n",
    "\n",
    "train_df[FEATURE_COLS] = scaler_feat.transform(train_df[FEATURE_COLS].values)\n",
    "valid_df[FEATURE_COLS] = scaler_feat.transform(valid_df[FEATURE_COLS].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "directory_path = './NN_search'\n",
    "pattern = f\"{directory_path}/{study_name}*.h5\"\n",
    "\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "max_r2_score = float('-inf')\n",
    "best_model = None\n",
    "\n",
    "# Käy läpi jokainen tiedosto ja etsi suurin r2_score_inv\n",
    "for file in files:\n",
    "    value = float(file.split('best_val')[1].split('_')[1])\n",
    "    if value > max_r2_score:\n",
    "        max_r2_score = value\n",
    "        best_model = file\n",
    "\n",
    "\n",
    "# Tulosta suurin löydetty r2_score_inv ja vastaava tiedosto\n",
    "print(f\"Best R2-score: {max_r2_score:.5f}\")\n",
    "if best_model:\n",
    "    print(f\"Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"No best model found\")\n",
    "\n",
    "best_log_transforms_name =  f'./NN_search/{study_name}_{max_r2_score:.5f}_best_log_transforms.pickle'\n",
    "best_scalers_name = f'./NN_search/{study_name}_{max_r2_score:.5f}_best_scalers.pickle'\n",
    "\n",
    "print(f'Opening log transforms from {best_log_transforms_name}')\n",
    "with open(best_log_transforms_name, 'rb') as f:\n",
    "    log_transforms = pickle.load(f)\n",
    "\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "    \n",
    "custom_objects = {\"r2_score_tf\": r2_score_tf}\n",
    "\n",
    "nas_model  = tf.keras.models.load_model(best_model, custom_objects=custom_objects)\n",
    "\n",
    "nas_model.summary()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable_count_nas = sum([tf.size(v).numpy() for v in nas_model.trainable_weights])\n",
    "non_trainable_count_nas = sum([tf.size(v).numpy() for v in nas_model.non_trainable_weights])\n",
    "print(f\"Total parameters nas: {trainable_count_nas + non_trainable_count_nas:,}\")\n",
    "print(f\"Trainable parameters nas: {trainable_count_nas:,}\")\n",
    "print(f\"Non-trainable parameters nas: {non_trainable_count_nas:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scaler are: {scaler_transforms}')\n",
    "print(f'Log transforms are: {log_transforms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "\n",
    "    img = img / 255.0\n",
    "    \n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "    img = tf.image.rot90(img, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "\n",
    "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
    "    img = tf.image.random_hue(img, max_delta=0.1)\n",
    "    img = tf.image.random_saturation(img, lower=0.9, upper=1.1)\n",
    "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "\n",
    "    img = tf.image.random_jpeg_quality(img, min_jpeg_quality=85, max_jpeg_quality=100)\n",
    "\n",
    "    # angle = tf.random.uniform([], minval=-np.pi/8, maxval=np.pi/8, dtype=tf.float32)\n",
    "    # img = tfa.image.rotate(img, angles=angle)\n",
    "\n",
    "    crop_size = tf.random.uniform(shape=[], minval=420, maxval=480, dtype=tf.int32)\n",
    "    img = tf.image.random_crop(img, size=[crop_size, crop_size, 3])\n",
    "    img = tf.image.resize(img, [480, 480]) \n",
    "\n",
    "\n",
    "    theta = tf.random.uniform([], minval=-0.05, maxval=0.05)\n",
    "    tx = tf.random.uniform([], minval=-45, maxval=45)\n",
    "    ty = tf.random.uniform([], minval=-45, maxval=45)\n",
    "    # Adding a small rotation component\n",
    "    cos_theta = tf.cos(theta)\n",
    "    sin_theta = tf.sin(theta)\n",
    "    transformation_matrix = [cos_theta, -sin_theta, tx,\n",
    "                                sin_theta, cos_theta, ty,\n",
    "                                0, 0]\n",
    "    img = tfa.image.transform(img, transformation_matrix, interpolation=\"BILINEAR\")\n",
    "    \n",
    "    img = img * 255.0\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    img = augment_image(img)  \n",
    "    # img = tf.cast(img, tf.uint8)    \n",
    "\n",
    "    # tf.print(\"Final min and max in process_images:\", tf.reduce_min(img), tf.reduce_max(img))\n",
    "    # tf.print(\"Image type: \", img.dtype)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_image_valid(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    # tf.print(\"Final min and max in process_image_valid:\", tf.reduce_min(img), tf.reduce_max(img))\n",
    "    # img = tf.cast(img, tf.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# Define your dataset processing function\n",
    "def process_path_train(file_path, tabular_data, targets):\n",
    "    img = process_image(file_path)\n",
    "    return (img, img, tabular_data), targets\n",
    "\n",
    "\n",
    "def process_path_valid(file_path, tabular_data, targets):\n",
    "    img = process_image_valid(file_path)\n",
    "    return (img, img, tabular_data), targets\n",
    "\n",
    "def process_path_test(file_path, tabular_data, dummy):\n",
    "    img = process_image_valid(file_path)    \n",
    "    return (img, img, tabular_data), dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[mean_columns]\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n",
    "\n",
    "y_train_transformed = y_train.copy()\n",
    "y_valid_transformed = y_valid.copy()\n",
    "\n",
    "\n",
    "for target, log_base in log_transforms.items():\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "        y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "    elif log_base == 'sqrt':\n",
    "        y_train_transformed[target] = np.sqrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.sqrt(y_valid[target])\n",
    "\n",
    "    elif log_base == 'cbrt':\n",
    "        y_train_transformed[target] = np.cbrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.cbrt(y_valid[target])\n",
    "\n",
    "    else:\n",
    "        y_train_transformed[target] = y_train[target]\n",
    "        y_valid_transformed[target] = y_valid[target]    \n",
    "\n",
    "\n",
    "    y_train_transformed = scaler_transforms.transform(y_train_transformed)\n",
    "    y_valid_transformed = scaler_transforms.transform(y_valid_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000 \n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_tabular = train_df[FEATURE_COLS].values\n",
    "valid_tabular = valid_df[FEATURE_COLS].values\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, train_tabular ,y_train_transformed.values))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, valid_tabular, y_valid_transformed.values))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "len_train = len(train_dataset) * EPOCHS\n",
    "\n",
    "print(f'LR schedule steps: {len_train}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  KAKSIHAARAINEN AVG MAX IMAGELLA #####\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "\n",
    "# Asetetaan syötteet\n",
    "image_input_avg = Input(shape=(480, 480, 3), name='image_input_avg')\n",
    "image_input_max = Input(shape=(480, 480, 3), name='image_input_max')\n",
    "tabular_input = Input(shape=(train_tabular.shape[1],), name='tabular_input')\n",
    "\n",
    "# Luo perus EfficientNetV2M mallit\n",
    "eff_avg_base = EfficientNetV2M(weights='imagenet', include_top=False, pooling='avg', input_tensor=image_input_avg)\n",
    "eff_max_base = EfficientNetV2M(weights='imagenet', include_top=False, pooling='max', input_tensor=image_input_max)\n",
    "\n",
    "\n",
    "# # Kloonaa ja nimeä uudelleen mallit\n",
    "def clone_and_rename(model, prefix):\n",
    "    # Kloonaa malli ja nimeä kaikki kerrokset uudelleen\n",
    "    for layer in model.layers:\n",
    "        layer._name = prefix + '_' + layer.name\n",
    "    return model\n",
    "\n",
    "eff_avg_cloned = clone_and_rename(eff_avg_base, 'eff_avg')\n",
    "eff_max_cloned = clone_and_rename(eff_max_base, 'eff_max')\n",
    "\n",
    "\n",
    "eff_avg_cloned.trainable = True\n",
    "for layer in eff_avg_cloned.layers[:-52]:\n",
    "    layer.trainable = False\n",
    "eff_max_cloned.trainable = True\n",
    "for layer in eff_max_cloned.layers[:-52]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "\n",
    "img_concatenated = Concatenate()([eff_avg_cloned.output, eff_max_cloned.output])\n",
    "nas_output = nas_model([img_concatenated, tabular_input], name = 'nas_output')\n",
    "\n",
    "finetune_model = Model(inputs=[image_input_avg, image_input_max, tabular_input], outputs=nas_output, name='finetune_model')\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    1e-4,    \n",
    "    alpha=0.05,\n",
    "    name=\"CosineDecay\",\n",
    "    decay_steps=len_train\n",
    ")\n",
    "\n",
    "# Aseta oppimisnopeuden aikataulu\n",
    "finetune_model.compile(optimizer=optimizers.RMSprop(learning_rate=lr_schedule), loss='mae', metrics=['mse', 'mae', 'mape', r2_score_tf])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_count = sum([tf.size(v).numpy() for v in finetune_model.trainable_weights])\n",
    "non_trainable_count = sum([tf.size(v).numpy() for v in finetune_model.non_trainable_weights])\n",
    "print(f\"Total parameters: {trainable_count + non_trainable_count:,}\")\n",
    "print(f\"Trainable parameters: {trainable_count:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n",
    "\n",
    "print(f'Total parameters from EfficientNetV2M: { (trainable_count + non_trainable_count) - (trainable_count_nas + non_trainable_count_nas):,}')\n",
    "print(f'Trainable from EfficientNetV2M: {trainable_count - trainable_count_nas:,}')\n",
    "print(f'Non trainable from EfficientNetV2M: {non_trainable_count - non_trainable_count_nas:,}')\n",
    "\n",
    "print(f'Trainable fron NAS: {trainable_count_nas:,}')\n",
    "print(f'Non trainable from NAS: {non_trainable_count_nas:,}')\n",
    "print(F'Total parameters from NAS: {trainable_count_nas + non_trainable_count_nas:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "class TrainImageLoggingCallback(Callback):\n",
    "    def __init__(self, log_dir, data):\n",
    "        super(TrainImageLoggingCallback, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.data = data\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Fetch a batch of images\n",
    "        for (imgs, im2, tab) , tar in self.data.take(1):  # Adjust depending on your dataset structure\n",
    "            \n",
    "            # augmented_images = tf.map_fn(augment_image, imgs)\n",
    "            augmented_images = tf.cast(imgs, tf.uint8)    \n",
    "        \n",
    "            # Prepare the image to write to TensorBoard\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.image(\"Augmented Images\", augmented_images, step=epoch, max_outputs=10)\n",
    "\n",
    "            self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_folder = f\"./logs/all/trial_{study_name}\"\n",
    "print(f'Logging tensorboard to {log_folder}')\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "# Aseta logitiedostojen hakemisto\n",
    "tensorboard_callback = TensorBoard(log_dir=log_folder, histogram_freq=1, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=f'./NN_search/testifinetus_{study_name}.h5', monitor='val_mae', save_best_only=True, save_weights_only=True, mode = 'min',  verbose = 1),\n",
    "    tensorboard_callback,\n",
    "    TrainImageLoggingCallback(log_folder, train_dataset)    \n",
    "]\n",
    "\n",
    "history = finetune_model.fit(train_dataset, validation_data=valid_dataset, epochs=EPOCHS, verbose=1, callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.load_weights(f'./NN_search/testifinetus_{study_name}.h5')\n",
    "# finetune_model.save(f'./NN_search/koe', save_format='tf') # TODO tässä ongelmaa vielä, mutta ei väliä. Malli on jo olemassa ja sitä voi käyttää. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, log_base in log_transforms.items():\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_plot[target] = np.log(train_plot[target]) / np.log(log_base)\n",
    "        \n",
    "    elif log_base == 'sqrt':\n",
    "        train_plot[target] = np.sqrt(train_plot[target])\n",
    "        \n",
    "    elif log_base == 'cbrt':\n",
    "        train_plot[target] = np.cbrt(train_plot[target])\n",
    "        \n",
    "    else:\n",
    "        train_plot[target] = train_plot[target]\n",
    "        \n",
    "train_plot[mean_columns] = scaler_transforms.transform(train_plot[mean_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_transforms)\n",
    "print(scaler_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, columns_names):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Setting up a grid of plots with 2 columns\n",
    "    n_cols = 6\n",
    "    n_rows = len(columns_names) // n_cols + (len(columns_names) % n_cols > 0)\n",
    "\n",
    "    for i, col in enumerate(columns_names):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_original, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_plot, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training_name = './data/results_finetune_images.pickle'\n",
    "\n",
    "if os.path.exists(results_training_name):\n",
    "    results_training = pd.read_pickle(results_training_name)\n",
    "else:\n",
    "    columns = ['Train R2', 'Train MSE', 'Train MAE', 'Train MAPE', 'Valid R2', 'Valid MSE', 'Valid MAE', 'Valid MAPE', 'Train preds Desc', 'Valid preds Desc', 'Test preds Desc' , 'Original data Desc' 'Kaggle R2', 'Scalers', 'Log/Pot transforms']\n",
    "    results_training = pd.DataFrame(columns = columns)\n",
    "    results_training.index.name = 'Study name'\n",
    "\n",
    "study_name_result = f'{study_name}_finetuned'\n",
    "\n",
    "if study_name_result not in results_training.index:    \n",
    "    results_training.loc[study_name] = [None]*len(results_training.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, train_tabular, y_train_transformed.values))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, valid_tabular ,y_valid_transformed.values))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TRAINING DATA TEST\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "train_pred = finetune_model.predict(train_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        train_pred[:, i] = scaler.inverse_transform(train_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_pred[:, i] = np.power(log_base, train_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        train_pred[:, i] = np.square(train_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        train_pred[:, i] = np.power(train_pred[:, i], 3)\n",
    "\n",
    "R2_train = r2_score(y_train, train_pred)\n",
    "MSE_train = mean_squared_error(y_train, train_pred)\n",
    "MAE_train = mean_absolute_error(y_train, train_pred)\n",
    "MAPE_train = mean_absolute_percentage_error(y_train, train_pred)\n",
    "\n",
    "print(f'Train scores:\\nR2 : {R2_train:.5f}, MSE : {MSE_train:.5f}, MAE : {MAE_train:.5f}, MAPE : {MAPE_train:.5f}')\n",
    "results_training.at[study_name_result, 'Train R2'] = R2_train\n",
    "results_training.at[study_name_result, 'Train MSE'] = MSE_train\n",
    "results_training.at[study_name_result, 'Train MAE'] = MAE_train\n",
    "results_training.at[study_name_result, 'Train MAPE'] = MAPE_train\n",
    "\n",
    "trainining_preds_desc = pd.DataFrame(train_pred, columns = mean_columns).describe().to_json()\n",
    "results_training.at[study_name_result, 'Train preds Desc'] = trainining_preds_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## VALIDATION DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "valid_pred = finetune_model.predict(valid_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        valid_pred[:, i] = scaler.inverse_transform(valid_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        valid_pred[:, i] = np.square(valid_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        valid_pred[:, i] = np.power(valid_pred[:, i], 3)\n",
    "\n",
    "R2_valid = r2_score(y_valid, valid_pred)\n",
    "MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "MAPE_valid = mean_absolute_percentage_error(y_valid, valid_pred)\n",
    "\n",
    "print(f'Valid scores:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}, MAPE : {MAPE_valid:.5f}')\n",
    "results_training.at[study_name_result, 'Valid R2'] = R2_valid\n",
    "results_training.at[study_name_result, 'Valid MSE'] = MSE_valid\n",
    "results_training.at[study_name_result, 'Valid MAE'] = MAE_valid\n",
    "results_training.at[study_name_result, 'Valid MAPE'] = MAPE_valid\n",
    "\n",
    "valid_preds_desc = pd.DataFrame(valid_pred, columns = mean_columns).describe().to_json()\n",
    "results_training.at[study_name_result, 'Valid preds Desc'] = valid_preds_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "submission_df = test_df_copy[['id']].copy()\n",
    "\n",
    "\n",
    "test_df_copy[FEATURE_COLS] = scaler_feat.transform(test_df_copy[FEATURE_COLS].values)\n",
    "test_features = test_df_copy[FEATURE_COLS].values\n",
    "test_images_path = test_df_copy['image_path'].values\n",
    "\n",
    "dummy_y = np.zeros((len(test_df_copy), 6))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_path, test_features, dummy_y))\n",
    "test_dataset = test_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "predictions = finetune_model.predict(test_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        predictions[:, i] = scaler.inverse_transform(predictions[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target: : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        predictions[:, i] = np.power(log_base, predictions[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        predictions[:, i] = np.square(predictions[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        predictions[:, i] = np.power(predictions[:, i], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_columns = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n",
    "\n",
    "submission_df[target_columns] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_preds_desc = submission_df[target_columns].describe().to_json()\n",
    "results_training.at[study_name_result, 'Test preds Desc'] = test_preds_desc \n",
    "\n",
    "original_data_desc = train_original[mean_columns].describe().to_json()\n",
    "results_training.at[study_name_result, 'Original data Desc'] = original_data_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{str(log_transforms.items())}')\n",
    "print(f'{str(scaler_transforms.items())}')\n",
    "\n",
    "results_training.at[study_name_result, 'Scalers'] = f'{scaler_transforms}'\n",
    "results_training.at[study_name_result, 'Log/Pot transforms'] = f'{str(log_transforms.items())}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training.at[study_name_result, 'Kaggle R2'] = None\n",
    "\n",
    "for index, row in results_training.iterrows():\n",
    "    print(f\"Study Name: {index}\")\n",
    "    print(f'Kaggle R2: {row[\"Kaggle R2\"]}')\n",
    "    print(f\"Train R2: {row['Train R2']}, Train MSE: {row['Train MSE']}, Train MAE : {row['Train MAE']}, Train MAPE: {row['Train MAPE']}\")\n",
    "    print(f'Valid R2: {row[\"Valid R2\"]}, Valid MSE: {row[\"Valid MSE\"]}, Valid MAE: {row[\"Valid MAE\"]}, Valid MAPE: {row[\"Valid MAPE\"]}')\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Train preds Description:\")\n",
    "    display(pd.read_json(row['Train preds Desc']))\n",
    "    print(\"Valid preds Description:\")\n",
    "    display(pd.read_json(row['Valid preds Desc']))\n",
    "    print(\"Test preds Description:\")\n",
    "    display(pd.read_json(row['Test preds Desc']))\n",
    "    print(\"Original data Description:\")\n",
    "    display(pd.read_json(row['Original data Desc']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df.info())\n",
    "\n",
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_training_name, 'wb') as f:\n",
    "    results_training.to_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_output = finetune_model.layers[-2].output\n",
    "feature_model = Model(inputs=finetune_model.inputs, outputs=feature_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_paths = train_original['image_path'].values\n",
    "train_all_features = train_original[FEATURE_COLS].values\n",
    "dummy_y = np.zeros((len(train_original), 6))\n",
    "\n",
    "train_all_dataset = tf.data.Dataset.from_tensor_slices((train_all_paths, train_all_features, dummy_y))\n",
    "train_all_dataset = train_all_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_all_dataset = train_all_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_features = feature_model.predict(train_all_dataset, verbose=1)\n",
    "\n",
    "train_original[f'model_features_{study_name_result}'] = list(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_file_path = './data/train_df.pickle'\n",
    "\n",
    "# print(f'Saving train_df to {pickle_file_path}')\n",
    "# with open(pickle_file_path, 'wb') as f:\n",
    "#     pickle.dump(train_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_paths = test_df['image_path'].values\n",
    "test_all_features = test_df[FEATURE_COLS].values\n",
    "dummy_y = np.zeros((len(test_df), 6))\n",
    "\n",
    "test_all_dataset = tf.data.Dataset.from_tensor_slices((test_all_paths, test_all_features, dummy_y))\n",
    "test_all_dataset = test_all_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_all_dataset = test_all_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_features = feature_model.predict(test_all_dataset, verbose=1)\n",
    "\n",
    "test_df[f'model_features_{study_name_result}'] = list(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_file_path = './data/test_df.pickle'\n",
    "\n",
    "# print(f'Saving test_df to {pickle_file_path}')\n",
    "# with open(pickle_file_path, 'wb') as f:\n",
    "#     pickle.dump(test_df, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
