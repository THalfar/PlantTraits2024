{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2M, ConvNeXtBase\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(f'Current GPU allocator: {os.getenv(\"TF_GPU_ALLOCATOR\")}')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(f'Setting memory growth for {gpu}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '426_convnextbase_003_998_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Aseta näyttämään rajoittamaton määrä sarakkeita\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_file_path = './data/test_convnextbase_df_003_998.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "pickle_file_path = './data/train_convnextbase_df_003_998.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING IMAGE AUGEMENTATION\n",
    "# train_df = train_df.sample(1000)\n",
    "# test_df = test_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv('./data/test.csv')\n",
    "FEATURE_COLS = feat.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_path = './data/train_images/'\n",
    "# test_images_path = './data/test_images/'    \n",
    "\n",
    "# train_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(train_images_path, f'{x}.jpeg'))\n",
    "# test_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(test_images_path, f'{x}.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in mean_columns:\n",
    "#     lower_quantile = train_df[column].quantile(0.005)\n",
    "#     upper_quantile = train_df[column].quantile(0.985)  \n",
    "#     train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# for i, trait in enumerate(mean_columns):\n",
    "\n",
    "#     # Determine the bin edges dynamically based on the distribution of traits\n",
    "#     bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, 5 + 1))\n",
    "#     train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n",
    "\n",
    "# # Concatenate the bins into a final bin\n",
    "# train_df[\"final_bin\"] = (\n",
    "#     train_df[[f\"bin_{i}\" for i in range(len(mean_columns))]]\n",
    "#     .astype(str)\n",
    "#     .agg(\"\".join, axis=1)\n",
    "# )\n",
    "\n",
    "# # Perform the stratified split using final bin\n",
    "# train_df = train_df.reset_index(drop=True)\n",
    "# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n",
    "#     train_df.loc[valid_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_feat = RobustScaler()\n",
    "\n",
    "train_original = train_df.copy()\n",
    "train_plot = train_df.copy()\n",
    "sample_df = train_df.copy()\n",
    "\n",
    "train_df = sample_df[sample_df.fold != 1]\n",
    "valid_df = sample_df[sample_df.fold == 1]\n",
    "\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "directory_path = './NN_search'\n",
    "pattern = f\"{directory_path}/{study_name}*.h5\"\n",
    "\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "max_r2_score = float('-inf')\n",
    "best_model = None\n",
    "\n",
    "# Käy läpi jokainen tiedosto ja etsi suurin r2_score_inv\n",
    "for file in files:\n",
    "    value = float(file.split('best_val')[1].split('_')[1])\n",
    "    if value > max_r2_score:\n",
    "        max_r2_score = value\n",
    "        best_model = file\n",
    "\n",
    "\n",
    "# Tulosta suurin löydetty r2_score_inv ja vastaava tiedosto\n",
    "print(f\"Best R2-score: {max_r2_score:.5f}\")\n",
    "if best_model:\n",
    "    print(f\"Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"No best model found\")\n",
    "\n",
    "best_log_transforms_name =  f'./NN_search/{study_name}_{max_r2_score:.5f}_best_log_transforms.pickle'\n",
    "best_scalers_name = f'./NN_search/{study_name}_{max_r2_score:.5f}_best_scalers.pickle'\n",
    "\n",
    "\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "\n",
    "log_transforms = {'X4_mean': 10, 'X11_mean': 10, 'X18_mean': 10, 'X50_mean': 4, 'X26_mean': 7, 'X3112_mean': 2}\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scaler are: {scaler_transforms}')\n",
    "print(f'Log transforms are: {log_transforms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "\n",
    "    img = img / 255.0\n",
    "    \n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "    # img = tf.image.rot90(img, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "\n",
    "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
    "    img = tf.image.random_hue(img, max_delta=0.1)\n",
    "    img = tf.image.random_saturation(img, lower=0.9, upper=1.1)\n",
    "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "\n",
    "    img = tf.image.random_jpeg_quality(img, min_jpeg_quality=85, max_jpeg_quality=100)\n",
    "\n",
    "    # angle = tf.random.uniform([], minval=-np.pi/8, maxval=np.pi/8, dtype=tf.float32)\n",
    "    # img = tfa.image.rotate(img, angles=angle)\n",
    "\n",
    "    crop_size = tf.random.uniform(shape=[], minval=420, maxval=480, dtype=tf.int32)\n",
    "    img = tf.image.random_crop(img, size=[crop_size, crop_size, 3])\n",
    "    img = tf.image.resize(img, [480, 480]) \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    theta = tf.random.uniform([], minval=-0.2, maxval=0.2)\n",
    "    tx = tf.random.uniform([], minval=-45, maxval=45)\n",
    "    ty = tf.random.uniform([], minval=-45, maxval=45)\n",
    "    cos_theta = tf.cos(theta)\n",
    "    sin_theta = tf.sin(theta)\n",
    "    \n",
    "    transformation_matrix = [cos_theta, -sin_theta, tx,\n",
    "                                sin_theta, cos_theta, ty,\n",
    "                                0, 0]\n",
    "\n",
    "    img = tfa.image.transform(img, transformation_matrix, interpolation=\"BILINEAR\")\n",
    "    \n",
    "    img = img * 255.0\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    img = augment_image(img)  \n",
    "    # img = tf.cast(img, tf.uint8)    \n",
    "\n",
    "    # tf.print(\"Final min and max in process_images:\", tf.reduce_min(img), tf.reduce_max(img))\n",
    "    # tf.print(\"Image type: \", img.dtype)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_image_valid(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    # tf.print(\"Final min and max in process_image_valid:\", tf.reduce_min(img), tf.reduce_max(img))\n",
    "    # img = tf.cast(img, tf.uint8)\n",
    "    return img\n",
    "\n",
    "# Define your dataset processing function\n",
    "def process_path_train(file_path, targets):\n",
    "    img = process_image(file_path)\n",
    "    return img, targets\n",
    "\n",
    "\n",
    "def process_path_valid(file_path, targets):\n",
    "    img = process_image_valid(file_path)\n",
    "    return img, targets\n",
    "\n",
    "def process_path_test(file_path, dummy):\n",
    "    img = process_image_valid(file_path)    \n",
    "    return img, dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[mean_columns]\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n",
    "\n",
    "y_train_transformed = y_train.copy()\n",
    "y_valid_transformed = y_valid.copy()\n",
    "\n",
    "for target, log_base in log_transforms.items():\n",
    "\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "        y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "    elif log_base == 'sqrt':\n",
    "        y_train_transformed[target] = np.sqrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.sqrt(y_valid[target])\n",
    "\n",
    "    elif log_base == 'cbrt':\n",
    "        y_train_transformed[target] = np.cbrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.cbrt(y_valid[target])\n",
    "\n",
    "    else:\n",
    "        y_train_transformed[target] = y_train[target]\n",
    "        y_valid_transformed[target] = y_valid[target]    \n",
    "\n",
    "\n",
    "y_train_transformed = scaler_transforms.fit_transform(y_train_transformed)\n",
    "y_valid_transformed = scaler_transforms.transform(y_valid_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 55000 \n",
    "EPOCHS = 6\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, y_train_transformed))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, y_valid_transformed))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "len_train = len(train_dataset) * EPOCHS\n",
    "\n",
    "print(f'LR schedule steps: {len_train}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "\n",
    "\n",
    "image_input_avg = Input(shape=(480, 480, 3), name='image_input_avg')\n",
    "\n",
    "eff_avg_base =  ConvNeXtBase(weights='imagenet', include_top=False, pooling='avg', input_tensor=image_input_avg)\n",
    "\n",
    "eff_avg_base.trainable = True\n",
    "for layer in eff_avg_base.layers[:-100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "eff_avg_base = Dropout(0.5)(eff_avg_base.output)\n",
    "\n",
    "# eff_drouput = Dropout(0.5)(eff_avg_base.output)\n",
    "\n",
    "first_layer = Dense(1202, activation='sigmoid', name='first_layer', kernel_initializer='random_uniform')(eff_avg_base)\n",
    "first_drouput = Dropout(0.611)(first_layer)\n",
    "\n",
    "second_layer = Dense(626, activation='tanh', name='second_layer', kernel_initializer='random_uniform')(first_drouput)\n",
    "second_norm = tf.keras.layers.LayerNormalization()(second_layer)\n",
    "second_dropout = Dropout(0.0268)(second_norm)\n",
    "\n",
    "third_layer = Dense(402, activation='selu', name='third_layer', kernel_initializer='he_uniform')(second_dropout)\n",
    "third_norm = tf.keras.layers.LayerNormalization()(third_layer)\n",
    "third_dropout = Dropout(0.008)(third_norm)\n",
    "\n",
    "fourth_layer = Dense(241, activation='tanh', name='fourth_layer', kernel_initializer='lecun_normal')(third_dropout)\n",
    "fourth_norm = tf.keras.layers.LayerNormalization()(fourth_layer)\n",
    "fourth_dropout = Dropout(0.0034)(fourth_norm)\n",
    "\n",
    "model_out = Dense(6, activation='linear', name='final_tune_output')(fourth_dropout)\n",
    "\n",
    "finetune_model = Model(inputs=image_input_avg, outputs=model_out, name='finetune_model')\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    1e-4,    \n",
    "    alpha=0.01,\n",
    "    name=\"CosineDecay\",\n",
    "    decay_steps=len_train\n",
    ")\n",
    "\n",
    "# Aseta oppimisnopeuden aikataulu\n",
    "finetune_model.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule), loss='mse', metrics=['mse', 'mae', 'mape', r2_score_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_count = sum([tf.size(v).numpy() for v in finetune_model.trainable_weights])\n",
    "non_trainable_count = sum([tf.size(v).numpy() for v in finetune_model.non_trainable_weights])\n",
    "print(f\"Total parameters: {trainable_count + non_trainable_count:,}\")\n",
    "print(f\"Trainable parameters: {trainable_count:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "class TrainImageLoggingCallback(Callback):\n",
    "    def __init__(self, log_dir, data):\n",
    "        super(TrainImageLoggingCallback, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.data = data\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Fetch a batch of images\n",
    "        for imgs, tar in self.data.take(1):  # Adjust depending on your dataset structure\n",
    "            \n",
    "            # augmented_images = tf.map_fn(augment_image, imgs)\n",
    "            augmented_images = tf.cast(imgs, tf.uint8)    \n",
    "        \n",
    "            # Prepare the image to write to TensorBoard\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.image(\"Augmented Images\", augmented_images, step=epoch, max_outputs=20)\n",
    "\n",
    "            self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_folder = f\"./logs/all/trial_{study_name}_429\" \n",
    "\n",
    "print(f'Logging tensorboard to {log_folder}')\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "# Aseta logitiedostojen hakemisto\n",
    "tensorboard_callback = TensorBoard(log_dir=log_folder, histogram_freq=1, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=f'./NN_search/testifinetus_{study_name}.h5', monitor='val_mse', save_best_only=True, save_weights_only=True, mode = 'min',  verbose = 1),\n",
    "    tensorboard_callback,\n",
    "    TrainImageLoggingCallback(log_folder, train_dataset)    \n",
    "]\n",
    "\n",
    "history = finetune_model.fit(train_dataset, validation_data=valid_dataset, epochs=EPOCHS, verbose=1, callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.load_weights(f'./NN_search/testifinetus_{study_name}.h5')\n",
    "# finetune_model.save(f'./NN_search/koe', save_format='tf') # TODO tässä ongelmaa vielä, mutta ei väliä. Malli on jo olemassa ja sitä voi käyttää. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, log_base in log_transforms.items():\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_plot[target] = np.log(train_plot[target]) / np.log(log_base)\n",
    "        \n",
    "    elif log_base == 'sqrt':\n",
    "        train_plot[target] = np.sqrt(train_plot[target])\n",
    "        \n",
    "    elif log_base == 'cbrt':\n",
    "        train_plot[target] = np.cbrt(train_plot[target])\n",
    "        \n",
    "    else:\n",
    "        train_plot[target] = train_plot[target]\n",
    "        \n",
    "train_plot[mean_columns] = scaler_transforms.transform(train_plot[mean_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_transforms)\n",
    "print(scaler_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path,  y_train_transformed))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, y_valid_transformed))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TRAINING DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "train_pred = finetune_model.predict(train_dataset, verbose=1)\n",
    "\n",
    "train_pred = scaler_transforms.inverse_transform(train_pred)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_pred[:, i] = np.power(log_base, train_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        train_pred[:, i] = np.square(train_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        train_pred[:, i] = np.power(train_pred[:, i], 3)\n",
    "\n",
    "R2_train = r2_score(y_train, train_pred)\n",
    "MSE_train = mean_squared_error(y_train, train_pred)\n",
    "MAE_train = mean_absolute_error(y_train, train_pred)\n",
    "MAPE_train = mean_absolute_percentage_error(y_train, train_pred)\n",
    "\n",
    "print(f'Train scores:\\nR2 : {R2_train:.5f}, MSE : {MSE_train:.5f}, MAE : {MAE_train:.5f}, MAPE : {MAPE_train:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## VALIDATION DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "valid_pred = finetune_model.predict(valid_dataset, verbose=1)\n",
    "\n",
    "valid_pred = scaler_transforms.inverse_transform(valid_pred)\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        valid_pred[:, i] = np.square(valid_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        valid_pred[:, i] = np.power(valid_pred[:, i], 3)\n",
    "\n",
    "R2_valid = r2_score(y_valid, valid_pred)\n",
    "MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "MAPE_valid = mean_absolute_percentage_error(y_valid, valid_pred)\n",
    "\n",
    "print(f'Valid scores:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}, MAPE : {MAPE_valid:.5f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "submission_df = test_df_copy[['id']].copy()\n",
    "\n",
    "test_images_path = test_df_copy['image_path'].values\n",
    "\n",
    "dummy_y = np.zeros((len(test_df_copy), 6))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_path, dummy_y))\n",
    "test_dataset = test_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "predictions = finetune_model.predict(test_dataset, verbose=1)\n",
    "\n",
    "predictions = scaler_transforms.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target: : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        predictions[:, i] = np.power(log_base, predictions[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        predictions[:, i] = np.square(predictions[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        predictions[:, i] = np.power(predictions[:, i], 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_columns = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n",
    "\n",
    "submission_df[target_columns] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df.info())\n",
    "\n",
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model_output = finetune_model.get_layer('fourth_layer')\n",
    "feature_model = Model(inputs=finetune_model.input, outputs=feature_model_output.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_paths = train_original['image_path'].values\n",
    "dummy_y = np.zeros((len(train_original), 6))\n",
    "\n",
    "train_all_dataset = tf.data.Dataset.from_tensor_slices((train_all_paths, dummy_y))\n",
    "train_all_dataset = train_all_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_all_dataset = train_all_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_features = feature_model.predict(train_all_dataset, verbose=1)\n",
    "\n",
    "train_original[f'model_features_{study_name}'] = train_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = f'./data/train_{study_name}.pickle'\n",
    "\n",
    "print(f'Saving train_df to {pickle_file_path}')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(train_original, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_paths = test_df['image_path'].values\n",
    "dummy_y = np.zeros((len(test_df), 6))\n",
    "\n",
    "test_all_dataset = tf.data.Dataset.from_tensor_slices((test_all_paths, dummy_y))\n",
    "test_all_dataset = test_all_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_all_dataset = test_all_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_features = feature_model.predict(test_all_dataset, verbose=1)\n",
    "\n",
    "test_df[f'model_features_{study_name}'] = test_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df[f'model_features_{study_name}'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = f'./data/test_{study_name}.pickle'\n",
    "\n",
    "print(f'Saving test_df to {pickle_file_path}')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
