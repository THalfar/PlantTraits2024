{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetV2M\n",
    "import numpy as np\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')\n",
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "FEATURE_COLS = test_df.columns[1:-1].tolist()\n",
    "\n",
    "train_df.info()\n",
    "train_df.head()\n",
    "train_df.describe()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_columns = [col for col in train_df.columns if col.endswith('_sd')]\n",
    "train_df.drop(columns=sd_columns, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_images_path = './data/train_images/'\n",
    "test_images_path = './data/test_images/'    \n",
    "\n",
    "train_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(train_images_path, f'{x}.jpeg'))\n",
    "test_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(test_images_path, f'{x}.jpeg'))\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_data(df, columns_names):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Setting up a grid of plots with 2 columns\n",
    "    n_cols = 6\n",
    "    n_rows = len(columns_names) // n_cols + (len(columns_names) % n_cols > 0)\n",
    "\n",
    "    for i, col in enumerate(columns_names):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_df, mean_columns)\n",
    "\n",
    "for column in mean_columns:\n",
    "    upper_quantile = train_df[column].quantile(0.98)  \n",
    "    train_df = train_df[(train_df[column] < upper_quantile)]\n",
    "    train_df = train_df[(train_df[column] > 0)]    \n",
    "\n",
    "plot_data(train_df, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, trait in enumerate(mean_columns):\n",
    "\n",
    "    # Determine the bin edges dynamically based on the distribution of traits\n",
    "    bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, 5 + 1))\n",
    "    train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n",
    "\n",
    "# Concatenate the bins into a final bin\n",
    "train_df[\"final_bin\"] = (\n",
    "    train_df[[f\"bin_{i}\" for i in range(len(mean_columns))]]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "\n",
    "# Perform the stratified split using final bin\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n",
    "    train_df.loc[valid_idx, \"fold\"] = fold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_df[mean_columns] = scaler.fit_transform(train_df[mean_columns])\n",
    "\n",
    "with open('./data/scaler_targets_train.pickle', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "\n",
    "\n",
    "# plot_data(train_df, mean_columns)\n",
    "\n",
    "# with open('./data/scaler_train.pickle', 'rb') as f:\n",
    "#     loaded_scaler = pickle.load(f)    \n",
    "#     train_df[mean_columns] = loaded_scaler.inverse_transform(train_df[mean_columns])\n",
    "\n",
    "# plot_data(train_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "base_model = EfficientNetV2M(weights='imagenet', include_top=False, pooling='avg')\n",
    "base_model.trainable = False\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features_batch(image_paths):\n",
    "    img_batch = np.stack([load_and_preprocess_image(img_path) for img_path in image_paths])\n",
    "    features = base_model.predict(img_batch)        \n",
    "    return features\n",
    "\n",
    "image_paths = train_df['image_path'].values\n",
    "\n",
    "features_list = []\n",
    "j = 0\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    batch_features = extract_features_batch(batch_paths)\n",
    "    features_list.append(batch_features)\n",
    "    j += 1\n",
    "    if j % 30 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        print(f'Clearing session')\n",
    "\n",
    "all_features = np.vstack(features_list)\n",
    "train_df['features'] = list(all_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.head(10))\n",
    "print(train_df.describe())\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = './data/train_df.pickle'\n",
    "\n",
    "# Save the train_df dataframe as a pickle file\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(train_df, f)\n",
    "\n",
    "\n",
    "# # Load the train_df dataframe from the pickle file\n",
    "# with open(pickle_file_path, 'rb') as f:\n",
    "#     train_df = pickle.load(f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df['fold'].value_counts())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "sample_df = train_df.copy()\n",
    "train_df = sample_df[sample_df.fold != 3]\n",
    "valid_df = sample_df[sample_df.fold == 3]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "\n",
    "train_df[FEATURE_COLS] = scaler.fit_transform(train_df[FEATURE_COLS].values)\n",
    "valid_df[FEATURE_COLS] = scaler.transform(valid_df[FEATURE_COLS].values)\n",
    "\n",
    "with open('./data/scaler_tabufeatures_train.pickle', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tab = train_df[FEATURE_COLS].values\n",
    "X_train_feat = np.stack(train_df['features'].values)\n",
    "y_train = train_df[mean_columns].values\n",
    "\n",
    "X_valid_tab = valid_df[FEATURE_COLS].values \n",
    "X_valid_feat = np.stack(valid_df['features'].values)\n",
    "y_valid = valid_df[mean_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import optuna\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "    r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "    r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2)  # Korvaa NaN-arvot nollilla\n",
    "    return tf.reduce_mean(tf.maximum(r2, 0.0)) \n",
    "\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "\n",
    "    image_features_input = Input(shape=(X_train_feat.shape[1],), name='image_features_input')\n",
    "    tabular_data_input = Input(shape=(X_train_tab.shape[1],), name='tabular_data_input')\n",
    "\n",
    "    img_num_layers = trial.suggest_int('Imgage layers', 1, 3)\n",
    "    max_img_units = 1024\n",
    "    img_dense = image_features_input\n",
    "    for i in range(img_num_layers):\n",
    "\n",
    "        num_img_units = trial.suggest_int(f'Num_img_{i}', 32, max_img_units)\n",
    "        activation_img = trial.suggest_categorical(f'Act_img_{i}', ['relu', 'tanh', 'selu', 'LeakyReLU', 'swish', 'mish'])\n",
    "        drop_img = trial.suggest_float(f'Drop_img_{i}', 0.2, 0.7, step=0.1)\n",
    "\n",
    "        img_dense = Dense(num_img_units, activation=activation_img)(img_dense)\n",
    "        img_dense = Dropout(drop_img)(img_dense)\n",
    "\n",
    "        max_img_units = min(max_img_units, num_img_units)\n",
    "\n",
    "\n",
    "    tab_num_layers = trial.suggest_int('Tabular layers', 1, 3)\n",
    "    max_tab_units = 1024   \n",
    "    tab_dense = tabular_data_input\n",
    "    for i in range(tab_num_layers):\n",
    "\n",
    "        num_tab_units = trial.suggest_int(f'Num_tab_{i}', 32, max_tab_units)\n",
    "        activation_tab = trial.suggest_categorical(f'Act_tab_{i}', ['relu', 'tanh', 'selu', 'LeakyReLU', 'swish', 'mish'])\n",
    "        drop_tab = trial.suggest_float(f'Drop_tab_{i}', 0.2, 0.7, step = 0.1)\n",
    "\n",
    "        tab_dense = Dense(num_tab_units, activation=activation_tab)(tab_dense)\n",
    "        tab_dense = Dropout(drop_tab)(tab_dense)\n",
    "\n",
    "        max_tab_units = min(max_tab_units, num_tab_units)\n",
    "\n",
    "\n",
    "    concatenated = Concatenate()([img_dense, tab_dense])\n",
    "    com_num_layers = trial.suggest_int('Concat layers', 1, 3)\n",
    "    max_com_units = 2048\n",
    "    \n",
    "    for i in range(com_num_layers):\n",
    "\n",
    "        num_common_units = trial.suggest_int(f'Num_con_{i}', 32, max_com_units)\n",
    "        activation_common = trial.suggest_categorical(f'Act_con_{i}', ['relu', 'tanh', 'selu', 'LeakyReLU', 'swish', 'mish'])\n",
    "        drop_common = trial.suggest_float(f'Drop_con_{i}', 0.2, 0.7, step = 0.1)\n",
    "\n",
    "        concatenated = Dense(num_common_units, activation=activation_common)(concatenated)\n",
    "        concatenated = Dropout(drop_common)(concatenated)\n",
    "\n",
    "        max_com_units = min(max_com_units, num_common_units)\n",
    "\n",
    "    output = Dense(6, activation='linear')(concatenated)  # Käytä linear aktivointifunktiota, jos kyseessä on regressio-ongelma\n",
    "    model = Model(inputs=[image_features_input, tabular_data_input], outputs=output)\n",
    "    \n",
    "    optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax', 'Adagrad', 'Adadelta']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "    \n",
    "    if optimizer_selected == 'adam':\n",
    "        optimizer = optimizers.Adam()\n",
    "    elif optimizer_selected == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop()\n",
    "    elif optimizer_selected == 'Nadam':\n",
    "        optimizer = optimizers.Nadam()\n",
    "    elif optimizer_selected == 'Adagrad':\n",
    "        optimizer = optimizers.Adagrad()\n",
    "    elif optimizer_selected == 'Adadelta':\n",
    "        optimizer = optimizers.Adadelta()\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax()\n",
    "\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae', r2_score])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model = create_model(trial)\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"./data/{study_name}_search_model.h5\",            \n",
    "            monitor='val_r2_score',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True)\n",
    "\n",
    "\n",
    "    callbacks = [TFKerasPruningCallback(trial, 'val_r2_score'),\n",
    "                 ReduceLROnPlateau('val_loss', patience=5, factor=0.5), \n",
    "                 TerminateOnNaN(),\n",
    "                 model_checkpoint_callback]\n",
    "\n",
    "\n",
    "    history = model.fit([X_train_feat, X_train_tab], y_train, validation_data=([X_valid_feat, X_valid_tab], y_valid), batch_size=512, epochs=100, callbacks=callbacks, verbose = 0)\n",
    "\n",
    "    best_val_r2 = max(history.history['val_r2_score'])\n",
    "\n",
    "    best_epoch = history.history['val_r2_score'].index(max(history.history['val_r2_score'])) + 1\n",
    "    \n",
    "\n",
    "    if trial.number > 0:\n",
    "        if best_val_r2 > study.best_value:\n",
    "\n",
    "            print(\"*\" * 50)\n",
    "            print(f'Old best R2 : {study.best_value:.5f}')\n",
    "            print(f'New best R2 : {best_val_r2:.5f}')\n",
    "            mse, mae, r2 = model.evaluate([X_valid_feat, X_valid_tab], y_valid, verbose=0)\n",
    "            print(f'Last epoch scores : MSE {mse:.5f}, MAE {mae:.5f}, R2 {r2:.5f}')\n",
    "            \n",
    "            model.load_weights(f'./data/{study_name}_search_model.h5')\n",
    "            \n",
    "            mse, mae, r2 = model.evaluate([X_valid_feat, X_valid_tab], y_valid, verbose=0)\n",
    "            print(f'Best model scores : MSE {mse:.5f}, MAE {mae:.5f}, R2 {r2:.5f}')\n",
    "            print(f'Best epoch : {best_epoch}')\n",
    "\n",
    "            best_filename = f'./data/{study_name}_best_val_{best_val_r2:.5f}_model.h5'\n",
    "            if os.path.exists(best_filename):\n",
    "                os.remove(best_filename)\n",
    "\n",
    "            print(f'Saving model to {best_filename}')    \n",
    "            model.save(best_filename)\n",
    "            print(\"*\" * 50)\n",
    "            \n",
    "    return best_val_r2\n",
    "\n",
    "\n",
    "study_name = '404_ekayo_fold_3'\n",
    "num_random_trials = 10\n",
    "num_tpe_trial = 1\n",
    "search_time_max = 60\n",
    "\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps = 5),\n",
    "                            study_name=study_name,\n",
    "                            storage=f'sqlite:///kukat404_ajot.db',\n",
    "                            load_if_exists=True\n",
    "                            )\n",
    "\n",
    "search_time_taken = 0\n",
    "search_start = time.time()\n",
    "round = 0\n",
    "while search_time_taken < search_time_max:\n",
    "\n",
    "    round_start = time.time()\n",
    "\n",
    "    print(f'Starting study with {num_random_trials} random trials, round {round}')\n",
    "    print(f'Search time so far taken : {timedelta(seconds=search_time_taken)}')\n",
    "    print('-' * 50)\n",
    "    study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False) \n",
    "    study.optimize(objective, n_trials=num_random_trials)\n",
    "    print(f'Time taken for random trials: {timedelta(seconds= (time.time() - round_start) / num_random_trials)}')\n",
    "    print(f'Starting TPE {num_tpe_trial} trials...')    \n",
    "    print(f'Time taken for one trial: {timedelta(seconds= (time.time() - round_start) / (num_random_trials + num_tpe_trial))}')    \n",
    "    print(f'Time this round: {timedelta(seconds= time.time() - round_start)}')\n",
    "    \n",
    "    search_time_taken = time.time() - search_start\n",
    "    round += 1\n",
    "\n",
    "print(f'Search time total : {timedelta(seconds=time.time() - search_start)}')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
