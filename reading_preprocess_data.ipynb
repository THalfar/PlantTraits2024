{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = './data/test_df.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "pickle_file_path = './data/train_df.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "    \n",
    "\n",
    "study_name = '413_pruneritesti_yo_3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "\n",
    "\n",
    "selected_features_pickle_path = './data/selected_features_list.pickle'\n",
    "with open(selected_features_pickle_path, 'rb') as f:\n",
    "    FEATURE_COLS = pickle.load(f)\n",
    "\n",
    "print(FEATURE_COLS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, columns_names):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Setting up a grid of plots with 2 columns\n",
    "    n_cols = 6\n",
    "    n_rows = len(columns_names) // n_cols + (len(columns_names) % n_cols > 0)\n",
    "\n",
    "    for i, col in enumerate(columns_names):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_df, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[FEATURE_COLS].describe()\n",
    "train_df_original = train_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "print(train_df['fold'].value_counts())\n",
    "\n",
    "# scaler = StandardScaler() # TODO testaa robustscaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "sample_df = train_df.copy()\n",
    "train_df = sample_df[sample_df.fold != 3]\n",
    "valid_df = sample_df[sample_df.fold == 3]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "\n",
    "train_df[FEATURE_COLS] = scaler.fit_transform(train_df[FEATURE_COLS].values)\n",
    "valid_df[FEATURE_COLS] = scaler.transform(valid_df[FEATURE_COLS].values)\n",
    "\n",
    "scaler_tabufeatures_name = f'./NN_search/scaler_tabufeatures_{study_name}_train.pickle'\n",
    "print(f\"Saving scaler to {scaler_tabufeatures_name}\")\n",
    "with open(f'{scaler_tabufeatures_name}', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[FEATURE_COLS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df[FEATURE_COLS].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tab = train_df[FEATURE_COLS].values\n",
    "X_train_feat = np.stack(train_df['features'].values)\n",
    "# y_train = train_df[mean_columns].values\n",
    "y_train = train_df[mean_columns]\n",
    "\n",
    "X_valid_tab = valid_df[FEATURE_COLS].values \n",
    "X_valid_feat = np.stack(valid_df['features'].values)\n",
    "# y_valid = valid_df[mean_columns].values\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_tab.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from optuna.integration import TFKerasPruningCallback\n",
    "# import optuna\n",
    "# from keras import regularizers, layers, optimizers, initializers\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "# from datetime import timedelta\n",
    "# import time\n",
    "# import os\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler, PowerTransformer, QuantileTransformer\n",
    "\n",
    "# # os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "\n",
    "\n",
    "# def r2_score(y_true, y_pred):\n",
    "#     ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "#     ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "#     r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "#     r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2)  # Korvaa NaN-arvot nollilla\n",
    "#     return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "\n",
    "\n",
    "# def huber_loss_wrapper(delta):\n",
    "#     def huber_loss(y_true, y_pred):\n",
    "#         return tf.keras.losses.Huber(delta=delta)(y_true, y_pred)\n",
    "#     return huber_loss\n",
    "\n",
    "\n",
    "# global current_delta\n",
    "# def create_model(trial):\n",
    "\n",
    "#     image_features_input = Input(shape=(X_train_feat.shape[1],), name='image_features_input')\n",
    "#     tabular_data_input = Input(shape=(X_train_tab.shape[1],), name='tabular_data_input')\n",
    "\n",
    "#     img_num_layers = trial.suggest_int('Imgage layers', 1, 2)\n",
    "#     max_img_units = 2000\n",
    "#     img_dense = image_features_input\n",
    "\n",
    "#     image_init = trial.suggest_categorical(f'Img_init', choices = ['glorot_uniform', 'he_normal', 'he_uniform', 'lecun_normal', 'lecun_uniform',  'random_normal', 'random_uniform'])\n",
    "#     activation_img = trial.suggest_categorical(f'Act_img', choices = ['relu', 'tanh', 'selu', 'LeakyReLU', 'swish', 'elu'])\n",
    "#     drop_img = trial.suggest_float(f'Drop_img', 0.0, 0.9, step=0.1)\n",
    "#     batch_norm_img = trial.suggest_categorical(f'Img_BatchN', choices = ['On', 'Off'])\n",
    "#     for i in range(img_num_layers):\n",
    "\n",
    "#         num_img_units = trial.suggest_int(f'Num_img_{i}', 128, max_img_units, log = True)\n",
    "#         img_dense = Dense(num_img_units, activation=activation_img, kernel_initializer = image_init)(img_dense)\n",
    "#         if batch_norm_img == 'On':\n",
    "#             img_dense = layers.BatchNormalization()(img_dense)\n",
    "#         img_dense = Dropout(drop_img)(img_dense)\n",
    "#         max_img_units = min(max_img_units, num_img_units)\n",
    "\n",
    "\n",
    "#     tab_num_layers = trial.suggest_int('Tabular layers', 1, 2)\n",
    "#     max_tab_units = 1000\n",
    "#     tab_dense = tabular_data_input\n",
    "#     tab_init = trial.suggest_categorical(f'Tab_init', choices = ['glorot_uniform', 'he_normal', 'he_uniform', 'lecun_normal', 'lecun_uniform',  'random_normal', 'random_uniform'])\n",
    "#     activation_tab = trial.suggest_categorical(f'Act_tab', choices = ['relu', 'tanh', 'selu', 'LeakyReLU', 'swish', 'elu'])\n",
    "#     drop_tab = trial.suggest_float(f'Drop_tab', 0.0, 0.9, step = 0.1)\n",
    "#     batch_norm_tab = trial.suggest_categorical(f'Tab_BatchN', choices = ['On', 'Off'])\n",
    "#     for i in range(tab_num_layers):\n",
    "\n",
    "#         num_tab_units = trial.suggest_int(f'Num_tab_{i}', 64, max_tab_units, log = True)\n",
    "#         tab_dense = Dense(num_tab_units, activation=activation_tab, kernel_initializer = tab_init)(tab_dense)\n",
    "#         if batch_norm_tab == 'On':\n",
    "#             tab_dense = layers.BatchNormalization()(tab_dense)\n",
    "#         tab_dense = Dropout(drop_tab)(tab_dense)\n",
    "\n",
    "#         max_tab_units = min(max_tab_units, num_tab_units)\n",
    "\n",
    "\n",
    "#     concatenated = Concatenate()([img_dense, tab_dense])\n",
    "#     com_num_layers = trial.suggest_int('Concat layers', 1, 2)\n",
    "#     max_com_units = 3000\n",
    "#     con_init = trial.suggest_categorical(f'Con_init', choices = ['glorot_uniform', 'he_normal', 'he_uniform', 'lecun_normal', 'lecun_uniform', 'random_normal', 'random_uniform'])\n",
    "#     activation_common = trial.suggest_categorical(f'Act_con',  choices = ['relu', 'tanh', 'selu', 'LeakyReLU', 'swish', 'elu'])\n",
    "#     drop_common = trial.suggest_float(f'Drop_con', 0.0, 0.9, step = 0.1)\n",
    "#     batch_norm_common = trial.suggest_categorical(f'Com_BatchN', ['On', 'Off'])\n",
    "#     for i in range(com_num_layers):\n",
    "\n",
    "#         num_common_units = trial.suggest_int(f'Num_con_{i}', 128, max_com_units, log = True)\n",
    "#         concatenated = Dense(num_common_units, activation=activation_common, kernel_initializer = con_init)(concatenated)\n",
    "#         if batch_norm_common == 'On':\n",
    "#             concatenated = layers.BatchNormalization()(concatenated)\n",
    "#         concatenated = Dropout(drop_common)(concatenated)\n",
    "\n",
    "#         max_com_units = min(max_com_units, num_common_units)\n",
    "\n",
    "#     output = Dense(6, activation='linear')(concatenated)\n",
    "#     model = Model(inputs=[image_features_input, tabular_data_input], outputs=output)\n",
    "\n",
    "\n",
    "#     # optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax']\n",
    "#     # optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "\n",
    "#     # if optimizer_selected == 'adam':\n",
    "#     #     optimizer = optimizers.Adam()\n",
    "#     # elif optimizer_selected == 'rmsprop':\n",
    "#     #     optimizer = optimizers.RMSprop()\n",
    "#     # elif optimizer_selected == 'Nadam':\n",
    "#     #     optimizer = optimizers.Nadam()\n",
    "#     # else:\n",
    "#     #     optimizer = optimizers.Adamax()\n",
    "\n",
    "#     # delta = trial.suggest_float('delta', 0.0, 2.5)\n",
    "#     # global current_delta\n",
    "#     # current_delta = delta\n",
    "\n",
    "#     model.compile(optimizer=optimizers.Nadam(), loss='mse', metrics=['mse','mae', r2_score])\n",
    "#     # model.compile(optimizer=optimizers.Adam(), loss=huber_loss_wrapper(delta), metrics=['mse','mae', r2_score])\n",
    "#     # model.compile(optimizer= optimizers.Adam(), loss='mse', metrics=['mse','mae', r2_score])\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     model = create_model(trial)\n",
    "\n",
    "#     y_train_transformed = y_train.copy()\n",
    "#     y_valid_transformed = y_valid.copy()\n",
    "\n",
    "\n",
    "#     log_base_options = {'none': None, 'log2': 2, 'log10': 10, 'sqrt': 'sqrt', 'cbrt': 'cbrt'}\n",
    "#     log_transforms = {'X11_mean': None, 'X18_mean': 2, 'X26_mean': 2, 'X50_mean': 'cbrt', 'X4_mean': 2, 'X3112_mean': 10}\n",
    "#     # for target in mean_columns:        \n",
    "#         # log_transforms[target] = log_base_options[log_base]\n",
    "    \n",
    "\n",
    "#     model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#             filepath=f\"./NN_search/{study_name}_search_model.h5\",\n",
    "#             monitor='val_r2_score',\n",
    "#             mode='max',\n",
    "#             save_best_only=True,\n",
    "#             save_weights_only=True,\n",
    "#             verbose=0)\n",
    "\n",
    "\n",
    "#     callbacks = [\n",
    "#                  ReduceLROnPlateau('val_r2_score', patience=2, factor=0.7, mode = 'max'),\n",
    "#                  TerminateOnNaN(),\n",
    "#                  model_checkpoint_callback,\n",
    "#                  EarlyStopping(monitor='val_r2_score', patience=8, mode='max', verbose = 1)\n",
    "#                  ]\n",
    "\n",
    "#     for target, log_base in log_transforms.items():\n",
    "#         if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "#             y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "#             y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "#         elif log_base == 'sqrt':\n",
    "#             y_train_transformed[target] = np.sqrt(y_train[target])\n",
    "#             y_valid_transformed[target] = np.sqrt(y_valid[target])\n",
    "\n",
    "#         elif log_base == 'cbrt':\n",
    "#             y_train_transformed[target] = np.cbrt(y_train[target])\n",
    "#             y_valid_transformed[target] = np.cbrt(y_valid[target])\n",
    "\n",
    "#         else:\n",
    "#             y_train_transformed[target] = y_train[target]\n",
    "#             y_valid_transformed[target] = y_valid[target]\n",
    "\n",
    "#     scaler_base_options = {'Std': StandardScaler(), 'Minmax': MinMaxScaler(), 'None': None, 'Robust': RobustScaler(), 'Power': PowerTransformer(), 'Quantile': QuantileTransformer()}\n",
    "#     scaler_transforms = {'X11_mean': PowerTransformer(), 'X18_mean': StandardScaler(), 'X26_mean':  PowerTransformer(), 'X50_mean': StandardScaler(), 'X4_mean': None, 'X3112_mean': PowerTransformer()}\n",
    "    \n",
    "#     for target, scaler in scaler_transforms.items():\n",
    "#         if scaler is not None:\n",
    "#             y_train_transformed[target] = scaler.fit_transform(y_train_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "#             y_valid_transformed[target] = scaler.transform(y_valid_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "#     # minmax_scaler = MinMaxScaler()\n",
    "#     # y_train_transformed = minmax_scaler.fit_transform(y_train_transformed)\n",
    "#     # y_valid_transformed = minmax_scaler.transform(y_valid_transformed)\n",
    "\n",
    "#     history = model.fit([X_train_feat, X_train_tab], y_train_transformed, validation_data=([X_valid_feat, X_valid_tab], y_valid_transformed), batch_size=256, epochs=50, callbacks=callbacks, verbose = 0)\n",
    "\n",
    "#     best_epoch = history.history['val_r2_score'].index(max(history.history['val_r2_score'])) + 1\n",
    "\n",
    "#     model.load_weights(f'./NN_search/{study_name}_search_model.h5')\n",
    "\n",
    "#     preds = model.predict([X_valid_feat, X_valid_tab], verbose = 0)\n",
    "#     import warnings\n",
    "#     try: \n",
    "#         warnings.filterwarnings('error')\n",
    "#         preds_transformed = preds.copy()\n",
    "\n",
    "#         for i, target in enumerate(mean_columns):\n",
    "#             scaler = scaler_transforms[target]\n",
    "#             if scaler is not None:\n",
    "#                 preds_transformed[:, i] = scaler.inverse_transform(preds_transformed[:, i].reshape(-1, 1)).flatten()\n",
    "#                 y_valid_transformed[target] = scaler.inverse_transform(y_valid_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "#         # preds_transformed = minmax_scaler.inverse_transform(preds_transformed)\n",
    "#         # y_valid_transformed = minmax_scaler.inverse_transform(y_valid_transformed)\n",
    "\n",
    "#         for i, target in enumerate(mean_columns):\n",
    "#             log_base = log_transforms[target]\n",
    "#             if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "#                 preds_transformed[:, i] = np.power(log_base, preds_transformed[:, i])\n",
    "#                 y_valid_transformed[target] = np.power(log_base, y_valid_transformed[target])\n",
    "#             elif log_base == 'sqrt':   \n",
    "#                 preds_transformed[:, i] = np.square(preds_transformed[:, i])\n",
    "#                 y_valid_transformed[target] = np.square(y_valid_transformed[target])\n",
    "#             elif log_base == 'cbrt':\n",
    "#                 preds_transformed[:, i] = np.power(preds_transformed[:, i], 3)\n",
    "#                 y_valid_transformed[target] = np.power(y_valid_transformed[target], 3)\n",
    "        \n",
    "#         r2_score_inv = r2_score(y_valid_transformed, preds_transformed)\n",
    "\n",
    "#     except RuntimeWarning as e:\n",
    "#         # Jos ylivuodon varoitus tapahtuu, logataan varoitus ja palautetaan erityinen arvo\n",
    "#         print(f\"Overflow detected: {e}\")\n",
    "#         r2_score_inv = 0.0\n",
    "\n",
    "#     finally:\n",
    "#         # Palauttaa varoitusten oletuskäsittelyn\n",
    "#         warnings.filterwarnings('default')\n",
    "    \n",
    "#     if trial.number > 0:\n",
    "#         if r2_score_inv > study.best_value:\n",
    "\n",
    "#             print(\"*\" * 50)\n",
    "#             print(f'Old best R2 : {study.best_value:.5f}')\n",
    "#             print(f'New best R2 : {r2_score_inv:.5f}')\n",
    "\n",
    "#             r2 = r2_score(y_valid_transformed, preds_transformed)\n",
    "#             mse  = tf.keras.losses.MeanSquaredError()(y_valid_transformed, preds_transformed)\n",
    "#             mae = tf.keras.losses.MeanAbsoluteError()(y_valid_transformed, preds_transformed)\n",
    "#             # huberloss = tf.keras.losses.Huber(delta=current_delta)(y_valid_transformed, preds_transformed)\n",
    "#             print(f'Best epoch power errors R2 : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}')\n",
    "\n",
    "\n",
    "#             # model.load_weights(f'./data/{study_name}_search_model.h5')\n",
    "\n",
    "#             # preds = model.predict([X_valid_feat, X_valid_tab])\n",
    "#             # preds_2 = np.square(preds)\n",
    "#             # # y_valid_2 = np.square(y_valid)\n",
    "#             # preds = model.predict([X_valid_feat, X_valid_tab], verbose = 0)\n",
    "\n",
    "#             # r2 = r2_score(y_valid_transformed, preds)\n",
    "#             # mse  = tf.keras.losses.MeanSquaredError()(y_valid, preds)\n",
    "#             # mae = tf.keras.losses.MeanAbsoluteError()(y_valid, preds)\n",
    "#             # huberloss = tf.keras.losses.Huber(delta=current_delta)(y_valid_2, preds_2)\n",
    "#             # # print(f'Best epoch raw R2 : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}')\n",
    "#             # print(f'Best epoch raw errors : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}, huber_loss : {huberloss:.5f}')\n",
    "\n",
    "#             # preds_inv = scaler_minmax.inverse_transform(preds)\n",
    "#             # y_valid_inv = scaler_minmax.inverse_transform(y_valid)\n",
    "#             # r2 = r2_score(y_valid_inv, preds_inv)\n",
    "#             # mse  = tf.keras.losses.MeanSquaredError()(y_valid_inv, preds_inv)\n",
    "#             # mae = tf.keras.losses.MeanAbsoluteError()(y_valid_inv, preds_inv)\n",
    "#             # huberloss = tf.keras.losses.Huber(delta=current_delta)(y_valid_2, preds_2)\n",
    "#             # print(f'Best epoch minmax.inv R2 : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}')\n",
    "#             # print(f'Best epoch raw R2 : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}, huber_loss : {huberloss:.5f}')\n",
    "\n",
    "#             # preds = model.predict([X_valid_feat, X_valid_tab])\n",
    "#             # r2 = r2_score(y_valid, preds)\n",
    "#             # mse  = tf.keras.losses.MeanSquaredError()(y_valid, preds)\n",
    "#             # mae = tf.keras.losses.MeanAbsoluteError()(y_valid, preds)\n",
    "#             # huberloss = tf.keras.losses.Huber(delta=current_delta)(y_valid, preds)\n",
    "#             # print(f'Best epoch raw R2 : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}, huber_loss : {huberloss:.5f}')\n",
    "#             print(f'Best epoch : {best_epoch}')\n",
    "\n",
    "#             best_filename = f'./NN_search/{study_name}_best_val_{r2_score_inv:.5f}_model.h5'\n",
    "#             if os.path.exists(best_filename):\n",
    "#                 os.remove(best_filename)\n",
    "\n",
    "#             print(f'Saving model to {best_filename}')\n",
    "#             model.save(best_filename)\n",
    "    \n",
    "#             best_log_transforms_name = f'./NN_search/{study_name}_{r2_score_inv:.5f}_best_log_transforms.pickle'\n",
    "#             print(f'Saving log transforms to {best_log_transforms_name}')\n",
    "#             with open(best_log_transforms_name, 'wb') as f:\n",
    "#                 pickle.dump(log_transforms, f)\n",
    "\n",
    "#             best_minmax = f'./NN_search/{study_name}_{r2_score_inv:.5f}_best_scalers.pickle'\n",
    "#             print(f'Saving scalers to {best_minmax}')\n",
    "#             with open(best_minmax, 'wb') as f:\n",
    "#                 pickle.dump(scaler_transforms, f)\n",
    "\n",
    "#             print(\"*\" * 50)\n",
    "\n",
    "#     if os.path.exists(f'./NN_search/{study_name}_search_model.h5'):\n",
    "#         os.remove(f'./NN_search/{study_name}_search_model.h5')\n",
    "\n",
    "#     tf.keras.backend.clear_session()\n",
    "#     gc.collect()\n",
    "\n",
    "#     return r2_score_inv\n",
    "\n",
    "\n",
    "# study_name = '410_yota_haetuillalogjututkivaahaetpaikkoihinonmukavaelama_fold_3'\n",
    "# num_random_trials = 10\n",
    "# num_tpe_trial = 5\n",
    "# search_time_max = 3600 * 18\n",
    "\n",
    "# study = optuna.create_study(direction='maximize',\n",
    "#                             study_name=study_name,\n",
    "#                             storage=f'sqlite:///409_logpolselect.db',\n",
    "#                             load_if_exists=True\n",
    "#                             )\n",
    "\n",
    "# search_time_taken = 0\n",
    "# search_start = time.time()\n",
    "# round = 0\n",
    "# trials_done = 0\n",
    "\n",
    "# while search_time_taken < search_time_max:\n",
    "\n",
    "#     round_start = time.time()\n",
    "\n",
    "#     print('-' * 50)\n",
    "#     print(f'Starting study with {num_random_trials} random trials, round {round}')\n",
    "#     print(f'Search time so far taken : {timedelta(seconds=search_time_taken)}')\n",
    "#     print('-' * 50)\n",
    "#     study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "#     study.optimize(objective, n_trials=num_random_trials)\n",
    "#     print(f'Time taken for random trials: {timedelta(seconds= (time.time() - round_start))}')\n",
    "#     print(f'Starting TPE {num_tpe_trial} trials...')\n",
    "#     study.sampler = optuna.samplers.TPESampler(n_startup_trials=0, multivariate=True, warn_independent_sampling = False)\n",
    "#     study.optimize(objective, n_trials=num_tpe_trial)\n",
    "#     print(f'Time taken for one trial this round: {timedelta(seconds= (time.time() - round_start) / (num_random_trials + num_tpe_trial))}')\n",
    "#     print(f'Time this round: {timedelta(seconds= time.time() - round_start)}')\n",
    "\n",
    "#     print('-' * 50)\n",
    "#     trials_done += num_random_trials + num_tpe_trial\n",
    "#     print(f'Trials done so far: {trials_done}')\n",
    "#     search_time_taken = time.time() - search_start\n",
    "#     print(f'Time taken for one trials all rounds: {timedelta(seconds= search_time_taken / trials_done)}')\n",
    "#     round += 1\n",
    "\n",
    "# print(f'Search time total : {timedelta(seconds=time.time() - search_start)}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "import optuna\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "def r2_score_safe(y_true, y_pred):\n",
    "    # Turvallinen R2 laskenta, joka palauttaa -inf, jos laskennassa ilmenee virheitä\n",
    "    try:\n",
    "        return r2_score(y_true, y_pred)\n",
    "    except Exception as e: \n",
    "        print(f'Error in r2_score_safe: {e}')        \n",
    "        return float('-inf')\n",
    "\n",
    "def mean_squared_error_safe(y_true, y_pred):\n",
    "    # Turvallinen MSE laskenta, joka palauttaa inf, jos laskennassa ilmenee virheitä\n",
    "    try:\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "    except Exception as e:\n",
    "        print(f'Error in mean_squared_error_safe: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "def mean_absolute_error_safe(y_true, y_pred):\n",
    "    # Turvallinen MAE laskenta, joka palauttaa inf, jos laskennassa ilmenee virheitä\n",
    "    try:\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "    except Exception as e:\n",
    "        print(f'Erros in mean_absolute_error_safe: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2)  # Korvaa NaN-arvot nollilla\n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "\n",
    "def create_model(trial):\n",
    "\n",
    "    image_features_input = Input(shape=(X_train_feat.shape[1],), name='image_features_input')\n",
    "    tabular_data_input = Input(shape=(X_train_tab.shape[1],), name='tabular_data_input')\n",
    "\n",
    "    num_img_units = 1477\n",
    "    img_dense = Dense(num_img_units, activation='elu', kernel_initializer = 'glorot_uniform')(image_features_input)\n",
    "    img_dense = layers.BatchNormalization()(img_dense)\n",
    "    img_dense = Dropout(0.7)(img_dense)\n",
    "    \n",
    "    tab_dense = Dense(402, activation='selu', kernel_initializer = 'random_normal')(tabular_data_input)\n",
    "    tab_dense = Dropout(0.6)(tab_dense)\n",
    "\n",
    "    concatenated = Concatenate()([img_dense, tab_dense])\n",
    "    \n",
    "    \n",
    "    concatenated = Dense(910, activation='swish', kernel_initializer = 'he_uniform')(concatenated)\n",
    "    concatenated = layers.BatchNormalization()(concatenated)\n",
    "    concatenated = Dropout(0.3)(concatenated)\n",
    "\n",
    "    concatenated = Dense(793, activation='swish', kernel_initializer = 'he_uniform')(concatenated)\n",
    "    concatenated = layers.BatchNormalization()(concatenated)\n",
    "    concatenated = Dropout(0.3)(concatenated)\n",
    "\n",
    "    output = Dense(6, activation='linear')(concatenated)\n",
    "    model = Model(inputs=[image_features_input, tabular_data_input], outputs=output)\n",
    "\n",
    "\n",
    "    optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax']\n",
    "    optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "\n",
    "    if optimizer_selected == 'adam':\n",
    "        optimizer = optimizers.Adam()\n",
    "    elif optimizer_selected == 'rmsprop':\n",
    "        optimizer = optimizers.RMSprop()\n",
    "    elif optimizer_selected == 'Nadam':\n",
    "        optimizer = optimizers.Nadam()\n",
    "    else:\n",
    "        optimizer = optimizers.Adamax()\n",
    "\n",
    "    # delta = trial.suggest_float('delta', 0.0, 2.5)\n",
    "    # global current_delta\n",
    "    # current_delta = delta\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse','mae', r2_score_tf])\n",
    "    # model.compile(optimizer=optimizers.Nadam(), loss='mse', metrics=['mse','mae', r2_score])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    " \n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    model = create_model(trial)\n",
    "\n",
    "    y_train_transformed = y_train.copy()\n",
    "    y_valid_transformed = y_valid.copy()\n",
    "\n",
    "\n",
    "    log_base_options = {'none': None, 'log2': 2, 'log10': 10, 'log5': 5, 'log15': 15, 'sqrt': 'sqrt', 'cbrt': 'cbrt'}\n",
    "    log_transforms = {}\n",
    "    for target in mean_columns:\n",
    "        log_base = trial.suggest_categorical(f'Log_{target}', list(log_base_options.keys()))\n",
    "        log_transforms[target] = log_base_options[log_base]\n",
    "\n",
    "    # log_transforms = {'X11_mean': None, 'X18_mean': 2, 'X26_mean': 2, 'X50_mean': 'cbrt', 'X4_mean': 2, 'X3112_mean': 10}\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=f\"./NN_search/{study_name}_search_model.h5\",\n",
    "            monitor='val_r2_score_tf',\n",
    "            mode='max',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            verbose=1)\n",
    "\n",
    "\n",
    "    callbacks = [\n",
    "                 ReduceLROnPlateau('val_r2_score_tf', patience=2, factor=0.7, mode = 'max', verbose = 1),\n",
    "                 TerminateOnNaN(),                 \n",
    "                 EarlyStopping(monitor='val_r2_score_tf', patience=8, mode='max', verbose = 1)\n",
    "                 ]\n",
    "\n",
    "    for target, log_base in log_transforms.items():\n",
    "        if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "            y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "            y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "        elif log_base == 'sqrt':\n",
    "            y_train_transformed[target] = np.sqrt(y_train[target])\n",
    "            y_valid_transformed[target] = np.sqrt(y_valid[target])\n",
    "\n",
    "        elif log_base == 'cbrt':\n",
    "            y_train_transformed[target] = np.cbrt(y_train[target])\n",
    "            y_valid_transformed[target] = np.cbrt(y_valid[target])\n",
    "\n",
    "        else:\n",
    "            y_train_transformed[target] = y_train[target]\n",
    "            y_valid_transformed[target] = y_valid[target]\n",
    "    \n",
    "    scaler_base_options = {'Std': StandardScaler(), 'None': None, 'minmax': MinMaxScaler(), 'Robust': RobustScaler(), 'Power': PowerTransformer(), 'Quantile': QuantileTransformer()}\n",
    "    scaler_transforms = {}\n",
    "    for target in mean_columns:\n",
    "        scaler_base = trial.suggest_categorical(f'Scaler_{target}', list(scaler_base_options.keys()))\n",
    "        scaler_transforms[target] = scaler_base_options[scaler_base]\n",
    "\n",
    "    for target, scaler in scaler_transforms.items():\n",
    "        if scaler is not None:\n",
    "            y_train_transformed[target] = scaler.fit_transform(y_train_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "            y_valid_transformed[target] = scaler.transform(y_valid_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    for epoch in range(50):\n",
    "\n",
    "\n",
    "        model.fit([X_train_feat, X_train_tab], y_train_transformed, validation_data=([X_valid_feat, X_valid_tab], y_valid_transformed), batch_size=256, epochs=1, callbacks=callbacks, verbose = 0)\n",
    "        preds_transformed = model.predict([X_valid_feat, X_valid_tab], verbose = 0)        \n",
    "\n",
    "    \n",
    "        try:        \n",
    "            for i, target in enumerate(mean_columns):\n",
    "                scaler = scaler_transforms[target]\n",
    "                if scaler is not None:\n",
    "                    preds_transformed[:, i] = scaler.inverse_transform(preds_transformed[:, i].reshape(-1, 1)).flatten()                \n",
    "\n",
    "            for i, target in enumerate(mean_columns):\n",
    "                log_base = log_transforms[target]\n",
    "                if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "                    preds_transformed[:, i] = np.power(log_base, preds_transformed[:, i])                \n",
    "                elif log_base == 'sqrt':   \n",
    "                    preds_transformed[:, i] = np.square(preds_transformed[:, i])    \n",
    "                elif log_base == 'cbrt':\n",
    "                    preds_transformed[:, i] = np.power(preds_transformed[:, i], 3)\n",
    "\n",
    "            r2_score_inv = r2_score_safe(y_valid, preds_transformed)                        \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error in inverse transformation: {e}')\n",
    "            print(f'Trial number {trial.number} epoch {epoch}')\n",
    "            r2_score_inv = float('-inf')\n",
    "            \n",
    "        if np.isnan(preds_transformed).any():\n",
    "            print(f'Nan values in predictions')\n",
    "            print(f'Trial number {trial.number} epoch {epoch}')\n",
    "            r2_score_inv = float('-inf')                        \n",
    "\n",
    "        if np.isinf(preds_transformed).any():\n",
    "            print(f'Inf values in predictions')\n",
    "            print(f'Trial number {trial.number} epoch {epoch}')\n",
    "            r2_score_inv = float('-inf')\n",
    "        \n",
    "\n",
    "        trial.report(r2_score_inv, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "\n",
    "            print(f'Trial pruned at epoch {epoch} with R2 {r2_score_inv:.5f}')\n",
    "\n",
    "            if trial.number > 0:\n",
    "                if r2_score_inv > study.best_value:\n",
    "\n",
    "                    print(\"*\" * 50)\n",
    "                    print(f'Old best R2 : {study.best_value:.5f}')\n",
    "                    print(f'New best R2 : {r2_score_inv:.5f}')\n",
    "\n",
    "                    r2 = r2_score_safe(y_valid, preds_transformed)\n",
    "                    mse  = mean_squared_error(y_valid, preds_transformed)\n",
    "                    mae = mean_absolute_error(y_valid, preds_transformed)\n",
    "                    \n",
    "                    print(f'Best epoch all errors R2 : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}')\n",
    "                    print(f'Best epoch : {epoch}')\n",
    "\n",
    "                    best_filename = f'./NN_search/{study_name}_best_val_{r2_score_inv:.5f}_model.h5'\n",
    "                    if os.path.exists(best_filename):\n",
    "                        os.remove(best_filename)\n",
    "\n",
    "                    print(f'Saving model to {best_filename}')\n",
    "                    model.save(best_filename)\n",
    "            \n",
    "                    best_log_transforms_name = f'./NN_search/{study_name}_{r2_score_inv:.5f}_best_log_transforms.pickle'\n",
    "                    print(f'Saving log transforms to {best_log_transforms_name}')\n",
    "                    with open(best_log_transforms_name, 'wb') as f:\n",
    "                        pickle.dump(log_transforms, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                    scaler_transforms_name = f'./NN_search/{study_name}_{r2_score_inv:.5f}_best_scalers.pickle'\n",
    "                    print(f'Saving scalers to {scaler_transforms_name}')\n",
    "                    with open(scaler_transforms_name, 'wb') as f:\n",
    "                        pickle.dump(scaler_transforms, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                    print(\"*\" * 50)\n",
    "                \n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "                \n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        if trial.number > 0:\n",
    "            if r2_score_inv > study.best_value:\n",
    "\n",
    "                print(\"*\" * 50)\n",
    "                print(f'Old best R2 : {study.best_value:.5f}')\n",
    "                print(f'New best R2 : {r2_score_inv:.5f}')\n",
    "\n",
    "                r2 = r2_score_safe(y_valid, preds_transformed)\n",
    "                mse  = mean_squared_error(y_valid, preds_transformed)\n",
    "                mae = mean_absolute_error(y_valid, preds_transformed)\n",
    "                \n",
    "                print(f'Best epoch all errors R2 : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}')\n",
    "                print(f'Best epoch : {epoch}')\n",
    "\n",
    "                best_filename = f'./NN_search/{study_name}_best_val_{r2_score_inv:.5f}_model.h5'\n",
    "                if os.path.exists(best_filename):\n",
    "                    os.remove(best_filename)\n",
    "\n",
    "                print(f'Saving model to {best_filename}')\n",
    "                model.save(best_filename)\n",
    "        \n",
    "                best_log_transforms_name = f'./NN_search/{study_name}_{r2_score_inv:.5f}_best_log_transforms.pickle'\n",
    "                print(f'Saving log transforms to {best_log_transforms_name}')\n",
    "                with open(best_log_transforms_name, 'wb') as f:\n",
    "                    pickle.dump(log_transforms, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                scaler_transforms_name = f'./NN_search/{study_name}_{r2_score_inv:.5f}_best_scalers.pickle'\n",
    "                print(f'Saving scalers to {scaler_transforms_name}')\n",
    "                with open(scaler_transforms_name, 'wb') as f:\n",
    "                    pickle.dump(scaler_transforms, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                print(\"*\" * 50)\n",
    "        \n",
    "    if os.path.exists(f'./NN_search/{study_name}_search_model.h5'):\n",
    "        os.remove(f'./NN_search/{study_name}_search_model.h5')\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    return r2_score_inv\n",
    "\n",
    "\n",
    "\n",
    "num_random_trials = 10\n",
    "num_tpe_trial = 3\n",
    "search_time_max = 3600 * 18\n",
    "\n",
    "study = optuna.create_study(direction='maximize',\n",
    "                            study_name=study_name,\n",
    "                            storage=f'sqlite:///412_bugipois.db',\n",
    "                            load_if_exists=True,\n",
    "                            pruner=optuna.pruners.MedianPruner(n_startup_trials=20, n_warmup_steps=3, interval_steps=5)\n",
    "                            )\n",
    "\n",
    "search_time_taken = 0\n",
    "search_start = time.time()\n",
    "round = 0\n",
    "trials_done = 0\n",
    "\n",
    "while search_time_taken < search_time_max:\n",
    "\n",
    "    round_start = time.time()\n",
    "\n",
    "    print('-' * 50)\n",
    "    print(f'Starting study with {num_random_trials} random trials, round {round}')\n",
    "    print(f'Search time so far taken : {timedelta(seconds=search_time_taken)}')\n",
    "    print('-' * 50)\n",
    "    study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "    study.optimize(objective, n_trials=num_random_trials)\n",
    "    print(f'Time taken for random trials: {timedelta(seconds= (time.time() - round_start))}')\n",
    "    print(f'Starting TPE {num_tpe_trial} trials...')\n",
    "    study.sampler = optuna.samplers.TPESampler(n_startup_trials=0, multivariate=True, warn_independent_sampling = False, n_ei_candidates=42)\n",
    "    study.optimize(objective, n_trials=num_tpe_trial)\n",
    "    print(f'Time taken for one trial this round: {timedelta(seconds= (time.time() - round_start) / (num_random_trials + num_tpe_trial))}')\n",
    "    print(f'Time this round: {timedelta(seconds= time.time() - round_start)}')\n",
    "\n",
    "    print('-' * 50)\n",
    "    trials_done += num_random_trials + num_tpe_trial\n",
    "    print(f'Trials done so far: {trials_done}')\n",
    "    search_time_taken = time.time() - search_start\n",
    "    print(f'Time taken for one trials all rounds: {timedelta(seconds= search_time_taken / trials_done)}')\n",
    "    round += 1\n",
    "\n",
    "print(f'Search time total : {timedelta(seconds=time.time() - search_start)}')\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "# from tensorflow.keras.models import Model\n",
    "# from optuna.integration import TFKerasPruningCallback\n",
    "# import optuna\n",
    "# from keras import regularizers, layers, optimizers, initializers\n",
    "# from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "# from datetime import timedelta\n",
    "# import time\n",
    "# import os\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer, RobustScaler\n",
    "# from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# def r2_score_safe(y_true, y_pred):\n",
    "#     # Turvallinen R2 laskenta, joka palauttaa -inf, jos laskennassa ilmenee virheitä\n",
    "#     try:\n",
    "#         return r2_score(y_true, y_pred)\n",
    "#     except Exception as e: \n",
    "#         print(f'Error in r2_score_safe: {e}')\n",
    "#         return float('-inf')\n",
    "\n",
    "# def mean_squared_error_safe(y_true, y_pred):\n",
    "#     # Turvallinen MSE laskenta, joka palauttaa inf, jos laskennassa ilmenee virheitä\n",
    "#     try:\n",
    "#         return mean_squared_error(y_true, y_pred)\n",
    "#     except Exception as e:\n",
    "#         print(f'Error in mean_squared_error_safe: {e}')\n",
    "#         return float('inf')\n",
    "\n",
    "# def mean_absolute_error_safe(y_true, y_pred):\n",
    "#     # Turvallinen MAE laskenta, joka palauttaa inf, jos laskennassa ilmenee virheitä\n",
    "#     try:\n",
    "#         return mean_absolute_error(y_true, y_pred)\n",
    "#     except Exception as e:\n",
    "#         print(f'Erros in mean_absolute_error_safe: {e}')\n",
    "#         return float('inf')\n",
    "\n",
    "# def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "#     try: \n",
    "#         ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "#         ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "#         r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "#         r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2)  # Korvaa NaN-arvot nollilla\n",
    "#         return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "#     except Exception as e:\n",
    "#         print(f'Error in r2_score_tf: {e}')\n",
    "#         return float('-inf')\n",
    "\n",
    "\n",
    "# def create_model(trial):\n",
    "\n",
    "#     image_features_input = Input(shape=(X_train_feat.shape[1],), name='image_features_input')\n",
    "#     tabular_data_input = Input(shape=(X_train_tab.shape[1],), name='tabular_data_input')\n",
    "\n",
    "#     num_img_units = 1477\n",
    "#     img_dense = Dense(num_img_units, activation='elu', kernel_initializer = 'glorot_uniform')(image_features_input)\n",
    "#     img_dense = layers.BatchNormalization()(img_dense)\n",
    "#     img_dense = Dropout(0.7)(img_dense)\n",
    "    \n",
    "#     tab_dense = Dense(402, activation='selu', kernel_initializer = 'random_normal')(tabular_data_input)\n",
    "#     tab_dense = Dropout(0.6)(tab_dense)\n",
    "\n",
    "#     concatenated = Concatenate()([img_dense, tab_dense])\n",
    "    \n",
    "    \n",
    "#     concatenated = Dense(910, activation='swish', kernel_initializer = 'he_uniform')(concatenated)\n",
    "#     concatenated = layers.BatchNormalization()(concatenated)\n",
    "#     concatenated = Dropout(0.3)(concatenated)\n",
    "\n",
    "#     concatenated = Dense(793, activation='swish', kernel_initializer = 'he_uniform')(concatenated)\n",
    "#     concatenated = layers.BatchNormalization()(concatenated)\n",
    "#     concatenated = Dropout(0.3)(concatenated)\n",
    "\n",
    "#     output = Dense(6, activation='linear')(concatenated)\n",
    "#     model = Model(inputs=[image_features_input, tabular_data_input], outputs=output)\n",
    "\n",
    "\n",
    "#     optimizer_options = ['adam', 'rmsprop', 'Nadam', 'adamax']\n",
    "#     optimizer_selected = trial.suggest_categorical('optimizer', optimizer_options)\n",
    "\n",
    "#     if optimizer_selected == 'adam':\n",
    "#         optimizer = optimizers.Adam()\n",
    "#     elif optimizer_selected == 'rmsprop':\n",
    "#         optimizer = optimizers.RMSprop()\n",
    "#     elif optimizer_selected == 'Nadam':\n",
    "#         optimizer = optimizers.Nadam()\n",
    "#     else:\n",
    "#         optimizer = optimizers.Adamax()\n",
    "\n",
    "#     model.compile(optimizer=optimizer, loss='mse', metrics=['mse','mae', r2_score_tf])\n",
    "#     # model.compile(optimizer=optimizers.Nadam(), loss='mse', metrics=['mse','mae', r2_score])\n",
    "    \n",
    "    \n",
    "#     return model\n",
    "\n",
    " \n",
    "\n",
    "# def objective(trial):\n",
    "\n",
    "#     model = create_model(trial)\n",
    "\n",
    "#     y_train_transformed = y_train.copy()\n",
    "#     y_valid_transformed = y_valid.copy()\n",
    "\n",
    "\n",
    "#     log_base_options = {'none': None, 'log2': 2, 'log10': 10, 'log5': 5, 'log15': 15, 'sqrt': 'sqrt', 'cbrt': 'cbrt'}\n",
    "#     log_transforms = {}\n",
    "#     for target in mean_columns:\n",
    "#         log_base = trial.suggest_categorical(f'Log_{target}', list(log_base_options.keys()))\n",
    "#         log_transforms[target] = log_base_options[log_base]\n",
    "\n",
    "#     # log_transforms = {'X11_mean': None, 'X18_mean': 2, 'X26_mean': 2, 'X50_mean': 'cbrt', 'X4_mean': 2, 'X3112_mean': 10}\n",
    "\n",
    "#     model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#             filepath=f\"./NN_search/{study_name}_search_model.h5\",\n",
    "#             monitor='val_r2_score_tf',\n",
    "#             mode='max',\n",
    "#             save_best_only=True,\n",
    "#             save_weights_only=True,\n",
    "#             verbose=0)\n",
    "\n",
    "\n",
    "#     callbacks = [\n",
    "#                  ReduceLROnPlateau('val_r2_score_tf', patience=2, factor=0.5, mode = 'max', verbose = 1),\n",
    "#                  TerminateOnNaN(),\n",
    "#                  model_checkpoint_callback,\n",
    "#                  EarlyStopping(monitor='val_r2_score_tf', patience=3, mode='max', verbose = 1)\n",
    "#                  ]\n",
    "\n",
    "#     for target, log_base in log_transforms.items():\n",
    "#         if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "#             y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "#             y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "#         elif log_base == 'sqrt':\n",
    "#             y_train_transformed[target] = np.sqrt(y_train[target])\n",
    "#             y_valid_transformed[target] = np.sqrt(y_valid[target])\n",
    "\n",
    "#         elif log_base == 'cbrt':\n",
    "#             y_train_transformed[target] = np.cbrt(y_train[target])\n",
    "#             y_valid_transformed[target] = np.cbrt(y_valid[target])\n",
    "\n",
    "#         else:\n",
    "#             y_train_transformed[target] = y_train[target]\n",
    "#             y_valid_transformed[target] = y_valid[target]\n",
    "    \n",
    "#     scaler_base_options = {'Std': StandardScaler(), 'None': None, 'minmax': MinMaxScaler(), 'Robust': RobustScaler(), 'Power': PowerTransformer(), 'Quantile': QuantileTransformer()}\n",
    "#     scaler_transforms = {}\n",
    "#     for target in mean_columns:\n",
    "#         scaler_base = trial.suggest_categorical(f'Scaler_{target}', list(scaler_base_options.keys()))\n",
    "#         scaler_transforms[target] = scaler_base_options[scaler_base]\n",
    "\n",
    "#     for target, scaler in scaler_transforms.items():\n",
    "#         if scaler is not None:\n",
    "#             y_train_transformed[target] = scaler.fit_transform(y_train_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "#             y_valid_transformed[target] = scaler.transform(y_valid_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    \n",
    "#     history = model.fit([X_train_feat, X_train_tab], y_train_transformed, validation_data=([X_valid_feat, X_valid_tab], y_valid_transformed), batch_size=256, epochs=50, callbacks=callbacks, verbose = 0)\n",
    "#     best_epoch = history.history['val_r2_score_tf'].index(max(history.history['val_r2_score_tf'])) + 1\n",
    "\n",
    "#     model.load_weights(f'./NN_search/{study_name}_search_model.h5')\n",
    "\n",
    "#     preds_transformed = model.predict([X_valid_feat, X_valid_tab], verbose = 0)        \n",
    "\n",
    "#     r2_score_inv = float('-inf')\n",
    "\n",
    "#     try:        \n",
    "#         for i, target in enumerate(mean_columns):\n",
    "#             scaler = scaler_transforms[target]\n",
    "#             if scaler is not None:\n",
    "#                 preds_transformed[:, i] = scaler.inverse_transform(preds_transformed[:, i].reshape(-1, 1)).flatten()                \n",
    "\n",
    "#         for i, target in enumerate(mean_columns):\n",
    "#             log_base = log_transforms[target]\n",
    "#             if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "#                 preds_transformed[:, i] = np.power(log_base, preds_transformed[:, i])                \n",
    "#             elif log_base == 'sqrt':   \n",
    "#                 preds_transformed[:, i] = np.square(preds_transformed[:, i])    \n",
    "#             elif log_base == 'cbrt':\n",
    "#                 preds_transformed[:, i] = np.power(preds_transformed[:, i], 3)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f'Error in inverse transformation: {e}')\n",
    "#         r2_score_inv = float('-inf')\n",
    "    \n",
    "#     if np.isnan(preds_transformed).any():\n",
    "#         r2_score_inv = float('-inf')                        \n",
    "\n",
    "#     if np.isinf(preds_transformed).any():\n",
    "#         r2_score_inv = float('-inf')\n",
    "\n",
    "#     if r2_score_inv != float('-inf'):\n",
    "#         r2_score_inv = r2_score_safe(y_valid, preds_transformed)\n",
    "\n",
    "#     if trial.number > 0:\n",
    "#         if r2_score_inv > study.best_value:\n",
    "\n",
    "#             print(\"*\" * 50)\n",
    "#             print(f'Old best R2 : {study.best_value:.5f}')\n",
    "#             print(f'New best R2 : {r2_score_inv:.5f}')\n",
    "\n",
    "#             r2 = r2_score_safe(y_valid, preds_transformed)\n",
    "#             mse  = mean_squared_error(y_valid, preds_transformed)\n",
    "#             mae = mean_absolute_error(y_valid, preds_transformed)\n",
    "            \n",
    "#             print(f'Best epoch all errors R2 : {r2:.5f}, MSE : {mse:.5f}, MAE : {mae:.5f}')\n",
    "#             print(f'Best epoch : {best_epoch}')\n",
    "\n",
    "#             best_filename = f'./NN_search/{study_name}_best_val_{r2_score_inv:.5f}_model.h5'\n",
    "#             if os.path.exists(best_filename):\n",
    "#                 os.remove(best_filename)\n",
    "\n",
    "#             print(f'Saving model to {best_filename}')\n",
    "#             model.save(best_filename)\n",
    "    \n",
    "#             best_log_transforms_name = f'./NN_search/{study_name}_{r2_score_inv:.5f}_best_log_transforms.pickle'\n",
    "#             print(f'Saving log transforms to {best_log_transforms_name}')\n",
    "#             with open(best_log_transforms_name, 'wb') as f:\n",
    "#                 pickle.dump(log_transforms, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#             scaler_transforms_name = f'./NN_search/{study_name}_{r2_score_inv:.5f}_best_scalers.pickle'\n",
    "#             print(f'Saving scalers to {scaler_transforms_name}')\n",
    "#             with open(scaler_transforms_name, 'wb') as f:\n",
    "#                 pickle.dump(scaler_transforms, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#         print(\"*\" * 50)\n",
    "        \n",
    "#     tf.keras.backend.clear_session()\n",
    "#     gc.collect()\n",
    "        \n",
    "    \n",
    "#     return r2_score_inv\n",
    "\n",
    "\n",
    "\n",
    "# num_random_trials = 10\n",
    "# num_tpe_trial = 3\n",
    "# search_time_max = 3600 * 18\n",
    "\n",
    "# study = optuna.create_study(direction='maximize',\n",
    "#                             study_name=study_name,\n",
    "#                             storage=f'sqlite:///412_bugipois.db',\n",
    "#                             load_if_exists=True,\n",
    "#                             pruner=optuna.pruners.MedianPruner(n_startup_trials=3, n_warmup_steps=2, interval_steps=2)\n",
    "#                             )\n",
    "\n",
    "# search_time_taken = 0\n",
    "# search_start = time.time()\n",
    "# round = 0\n",
    "# trials_done = 0\n",
    "\n",
    "# while search_time_taken < search_time_max:\n",
    "\n",
    "#     round_start = time.time()\n",
    "\n",
    "#     print('-' * 50)\n",
    "#     print(f'Starting study with {num_random_trials} random trials, round {round}')\n",
    "#     print(f'Search time so far taken : {timedelta(seconds=search_time_taken)}')\n",
    "#     print('-' * 50)\n",
    "#     study.sampler = optuna.samplers.QMCSampler(warn_independent_sampling = False)\n",
    "#     study.optimize(objective, n_trials=num_random_trials)\n",
    "#     print(f'Time taken for random trials: {timedelta(seconds= (time.time() - round_start))}')\n",
    "#     print(f'Starting TPE {num_tpe_trial} trials...')\n",
    "#     study.sampler = optuna.samplers.TPESampler(n_startup_trials=0, multivariate=True, warn_independent_sampling = False, n_ei_candidates=42)\n",
    "#     study.optimize(objective, n_trials=num_tpe_trial)\n",
    "#     print(f'Time taken for one trial this round: {timedelta(seconds= (time.time() - round_start) / (num_random_trials + num_tpe_trial))}')\n",
    "#     print(f'Time this round: {timedelta(seconds= time.time() - round_start)}')\n",
    "\n",
    "#     print('-' * 50)\n",
    "#     trials_done += num_random_trials + num_tpe_trial\n",
    "#     print(f'Trials done so far: {trials_done}')\n",
    "#     search_time_taken = time.time() - search_start\n",
    "#     print(f'Time taken for one trials all rounds: {timedelta(seconds= search_time_taken / trials_done)}')\n",
    "#     round += 1\n",
    "\n",
    "# print(f'Search time total : {timedelta(seconds=time.time() - search_start)}')\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.visualization.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.visualization.plot_slice(study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "    ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "    ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "    r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "    r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2)  # Korvaa NaN-arvot nollilla\n",
    "    return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "\n",
    "custom_objects = {\"r2_score_tf\": r2_score_tf}\n",
    "\n",
    "\n",
    "with open(f'./NN_search/scaler_tabufeatures_{study_name}_train.pickle', 'rb') as f:\n",
    "    scaler_tabular = pickle.load(f)\n",
    "\n",
    "\n",
    "print(f'Tabu features scaler: {scaler}')\n",
    "\n",
    "best_trial = study.best_value\n",
    "\n",
    "best_model_name = f'./NN_search/{study_name}_best_val_{best_trial:.5f}_model.h5'\n",
    "\n",
    "best_model = tf.keras.models.load_model(f'{best_model_name}', custom_objects=custom_objects)\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "\n",
    "test_df_copy[FEATURE_COLS] = scaler_tabular.transform(test_df_copy[FEATURE_COLS].values)\n",
    "\n",
    "submission_df = test_df_copy[['id']].copy()\n",
    "\n",
    "X_test_tab = test_df_copy[FEATURE_COLS].values\n",
    "X_test_feat = np.stack(test_df_copy['features'].values) \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "predictions = best_model.predict([X_test_feat, X_test_tab])\n",
    "\n",
    "best_log_transforms_name = f'./NN_search/{study_name}_{best_trial:.5f}_best_log_transforms.pickle'\n",
    "print(f'Opening log transforms from {best_log_transforms_name}')\n",
    "with open(best_log_transforms_name, 'rb') as f:\n",
    "    log_transforms = pickle.load(f)\n",
    "\n",
    "best_scalers_name = f'./NN_search/{study_name}_{best_trial:.5f}_best_scalers.pickle'\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "\n",
    "# for target, scaler in scaler_transforms.items():\n",
    "#     print(f'Target {target} scaler {scaler}')\n",
    "#     if scaler is not None:\n",
    "#         if isinstance(scaler, StandardScaler):\n",
    "#             print(\"  Mean:\", scaler.mean_)\n",
    "#             print(\"  Scale:\", scaler.scale_)\n",
    "#         elif isinstance(scaler, MinMaxScaler):\n",
    "#             print(\"  Data min:\", scaler.data_min_)\n",
    "#             print(\"  Data max:\", scaler.data_max_)\n",
    "#             print(\"  Data range:\", scaler.data_range_)\n",
    "        \n",
    "\n",
    "print(log_transforms)\n",
    "print(scaler_transforms)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        predictions[:, i] = scaler.inverse_transform(predictions[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target: : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        predictions[:, i] = np.power(log_base, predictions[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        predictions[:, i] = np.square(predictions[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        predictions[:, i] = np.power(predictions[:, i], 3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_columns = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n",
    "\n",
    "submission_df[target_columns] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training_name = './data/results_training.pickle'\n",
    "\n",
    "if os.path.exists(results_training_name):\n",
    "    results_training = pd.read_pickle(results_training_name)\n",
    "else:\n",
    "    columns = ['Train R2', 'Train MSE', 'Train MAE', 'Valid R2', 'Valid MSE', 'Valid MAE', 'Train preds Desc', 'Valid preds Desc', 'Test preds Desc' , 'Original data Desc' 'Kaggle R2', 'Scalers', 'Log/Pot transforms', 'NN Search space', 'Tabular scaler']\n",
    "    results_training = pd.DataFrame(columns = columns)\n",
    "    results_training.index.name = 'Study name'\n",
    "\n",
    "if study_name not in results_training.index:    \n",
    "    results_training.loc[study_name] = [None]*len(results_training.columns)\n",
    "\n",
    "\n",
    "test_preds_desc = submission_df[target_columns].describe().to_json()\n",
    "results_training.at[study_name, 'Test preds Desc'] = test_preds_desc \n",
    "\n",
    "original_data_desc = train_df_original[mean_columns].describe().to_json()\n",
    "results_training.at[study_name, 'Original data Desc'] = original_data_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{str(log_transforms.items())}')\n",
    "print(f'{str(scaler_transforms.items())}')\n",
    "print(f'{str(scaler_tabular)}')\n",
    "\n",
    "results_training.at[study_name, 'Scalers'] = f'{scaler_transforms}'\n",
    "results_training.at[study_name, 'Log/Pot transforms'] = f'{str(log_transforms.items())}'\n",
    "results_training.at[study_name, 'Tabular scaler'] = f'{scaler_tabular}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## JÄRKEVYYSKOKEILU TESTATAAN train dataan\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "## TEST DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "train_pred = best_model.predict([X_train_feat, X_train_tab])\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        train_pred[:, i] = scaler.inverse_transform(train_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_pred[:, i] = np.power(log_base, train_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        train_pred[:, i] = np.square(train_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        train_pred[:, i] = np.power(train_pred[:, i], 3)\n",
    "\n",
    "R2_train = r2_score(y_train, train_pred)\n",
    "MSE_train = mean_squared_error(y_train, train_pred)\n",
    "MAE_train = mean_absolute_error(y_train, train_pred)\n",
    "\n",
    "print(f'Train scores:\\nR2 : {R2_train:.5f}, MSE : {MSE_train:.5f}, MAE : {MAE_train:.5f}')\n",
    "\n",
    "results_training.at[study_name, 'Train R2'] = R2_train\n",
    "results_training.at[study_name, 'Train MSE'] = MSE_train\n",
    "results_training.at[study_name, 'Train MAE'] = MAE_train\n",
    "\n",
    "trainining_preds_desc = pd.DataFrame(train_pred, columns = mean_columns).describe().to_json()\n",
    "results_training.at[study_name, 'Train preds Desc'] = trainining_preds_desc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## VALIDATION DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "valid_pred = best_model.predict([X_valid_feat, X_valid_tab])\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        valid_pred[:, i] = scaler.inverse_transform(valid_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        valid_pred[:, i] = np.square(valid_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        valid_pred[:, i] = np.power(valid_pred[:, i], 3)\n",
    "\n",
    "R2_valid = r2_score(y_valid, valid_pred)\n",
    "MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "\n",
    "print(f'Valid scores:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}')\n",
    "\n",
    "results_training.at[study_name, 'Valid R2'] = R2_valid\n",
    "results_training.at[study_name, 'Valid MSE'] = MSE_valid\n",
    "results_training.at[study_name, 'Valid MAE'] = MAE_valid\n",
    "\n",
    "valid_preds_desc = pd.DataFrame(valid_pred, columns = mean_columns).describe().to_json()\n",
    "results_training.at[study_name, 'Valid preds Desc'] = valid_preds_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(results_training.head(100))\n",
    "logpot = results_training['Log/Pot transforms'].to_list()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df.info())\n",
    "\n",
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle = 0.0 \n",
    "# results_training.at[study_name, 'Kaggle R2'] = Kaggle\n",
    "\n",
    "results_training.drop('412_bugikaipoistestiviela_scalers_3', inplace=True)\n",
    "\n",
    "for index, row in results_training.iterrows():\n",
    "    print(f\"Study Name: {index}\")\n",
    "    print(f'Kaggle R2: {row[\"Kaggle R2\"]}')\n",
    "    print(f\"Train R2: {row['Train R2']}, Valid R2: {row['Valid R2']}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Train preds Description:\")\n",
    "    display(pd.read_json(row['Train preds Desc']))\n",
    "    print(\"Valid preds Description:\")\n",
    "    display(pd.read_json(row['Valid preds Desc']))\n",
    "    print(\"Test preds Description:\")\n",
    "    display(pd.read_json(row['Test preds Desc']))\n",
    "    print(\"Original data Description:\")\n",
    "    display(pd.read_json(row['Original data Desc']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_training_name, 'wb') as f:\n",
    "    results_training.to_pickle(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
