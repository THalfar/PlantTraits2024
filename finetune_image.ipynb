{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2M\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(f'Current GPU allocator: {os.getenv(\"TF_GPU_ALLOCATOR\")}')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(f'Setting memory growth for {gpu}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '419_stdminmax_lrred_images_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_columns = [col for col in train_df.columns if col.endswith('_sd')]\n",
    "train_df.drop(columns=sd_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = './data/train_images/'\n",
    "test_images_path = './data/test_images/'    \n",
    "\n",
    "train_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(train_images_path, f'{x}.jpeg'))\n",
    "test_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(test_images_path, f'{x}.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in mean_columns:\n",
    "    lower_quantile = train_df[column].quantile(0.005)\n",
    "    upper_quantile = train_df[column].quantile(0.985)  \n",
    "    train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, trait in enumerate(mean_columns):\n",
    "\n",
    "    # Determine the bin edges dynamically based on the distribution of traits\n",
    "    bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, 5 + 1))\n",
    "    train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n",
    "\n",
    "# Concatenate the bins into a final bin\n",
    "train_df[\"final_bin\"] = (\n",
    "    train_df[[f\"bin_{i}\" for i in range(len(mean_columns))]]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "\n",
    "# Perform the stratified split using final bin\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n",
    "    train_df.loc[valid_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original = train_df.copy()\n",
    "train_plot = train_df.copy()\n",
    "sample_df = train_df.copy()\n",
    "train_df = sample_df[sample_df.fold != 3]\n",
    "valid_df = sample_df[sample_df.fold == 3]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "directory_path = './NN_search'\n",
    "pattern = f\"{directory_path}/{study_name}*.h5\"\n",
    "\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "max_r2_score = float('-inf')\n",
    "best_model = None\n",
    "\n",
    "# Käy läpi jokainen tiedosto ja etsi suurin r2_score_inv\n",
    "for file in files:\n",
    "    value = float(file.split('best_val')[1].split('_')[1])\n",
    "    if value > max_r2_score:\n",
    "        max_r2_score = value\n",
    "        best_model = file\n",
    "\n",
    "\n",
    "# Tulosta suurin löydetty r2_score_inv ja vastaava tiedosto\n",
    "print(f\"Best R2-score: {max_r2_score:.5f}\")\n",
    "if best_model:\n",
    "    print(f\"Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"No best model found\")\n",
    "\n",
    "best_log_transforms_name =  f'./NN_search/{study_name}_{max_r2_score:.5f}_best_log_transforms.pickle'\n",
    "best_scalers_name = f'./NN_search/{study_name}_{max_r2_score:.5f}_best_scalers.pickle'\n",
    "\n",
    "print(f'Opening log transforms from {best_log_transforms_name}')\n",
    "with open(best_log_transforms_name, 'rb') as f:\n",
    "    log_transforms = pickle.load(f)\n",
    "\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "    \n",
    "custom_objects = {\"r2_score_tf\": r2_score_tf}\n",
    "\n",
    "nas_model  = tf.keras.models.load_model(best_model, custom_objects=custom_objects)\n",
    "\n",
    "nas_model.summary()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable_count_nas = sum([tf.size(v).numpy() for v in nas_model.trainable_weights])\n",
    "non_trainable_count_nas = sum([tf.size(v).numpy() for v in nas_model.non_trainable_weights])\n",
    "print(f\"Total parameters nas: {trainable_count_nas + non_trainable_count_nas:,}\")\n",
    "print(f\"Trainable parameters nas: {trainable_count_nas:,}\")\n",
    "print(f\"Non-trainable parameters nas: {non_trainable_count_nas:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scaler are: {scaler_transforms}')\n",
    "print(f'Log transforms are: {log_transforms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.rot90(img, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = tf.image.random_hue(img, max_delta=0.1)\n",
    "    img = tf.image.random_saturation(img, lower=0.8, upper=1.2)\n",
    "    img = tf.image.random_contrast(img, lower=0.8, upper=1.2)\n",
    "\n",
    "    # crop_size = tf.random.uniform(shape=[], minval=25, maxval=150, dtype=tf.int32)\n",
    "    # img = tf.image.random_crop(img, size=[crop_size, crop_size, 3])\n",
    "    # img = tf.image.resize(img, [480, 480])\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    img = augment_image(img)  \n",
    "    # img = tf.cast(img, tf.uint8)    \n",
    "    return img\n",
    "\n",
    "def process_image_valid(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    # img = tf.cast(img, tf.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# Define your dataset processing function\n",
    "def process_path_train(file_path, targets):\n",
    "    img = process_image(file_path)\n",
    "    return (img, img), targets\n",
    "\n",
    "\n",
    "def process_path_valid(file_path, targets):\n",
    "    img = process_image_valid(file_path)\n",
    "    return (img, img), targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[mean_columns]\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n",
    "\n",
    "y_train_transformed = y_train.copy()\n",
    "y_valid_transformed = y_valid.copy()\n",
    "\n",
    "\n",
    "for target, log_base in log_transforms.items():\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "        y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "    elif log_base == 'sqrt':\n",
    "        y_train_transformed[target] = np.sqrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.sqrt(y_valid[target])\n",
    "\n",
    "    elif log_base == 'cbrt':\n",
    "        y_train_transformed[target] = np.cbrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.cbrt(y_valid[target])\n",
    "\n",
    "    else:\n",
    "        y_train_transformed[target] = y_train[target]\n",
    "        y_valid_transformed[target] = y_valid[target]    \n",
    "\n",
    "for target, scaler in scaler_transforms.items():\n",
    "    if scaler is not None:\n",
    "        y_train_transformed[target] = scaler.transform(y_train_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "        y_valid_transformed[target] = scaler.transform(y_valid_transformed[target].values.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 50000 \n",
    "EPOCHS = 8\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, y_train_transformed.values))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, y_valid_transformed.values))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "len_train = len(train_dataset) * EPOCHS\n",
    "\n",
    "print(f'LR schedule steps: {len_train}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####  KAKSIHAARAINEN AVG MAX IMAGELLA #####\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "\n",
    "# Asetetaan syötteet\n",
    "image_input_avg = Input(shape=(480, 480, 3), name='image_input_avg')\n",
    "image_input_max = Input(shape=(480, 480, 3), name='image_input_max')\n",
    "\n",
    "# Luo perus EfficientNetV2M mallit\n",
    "eff_avg_base = EfficientNetV2M(weights='imagenet', include_top=False, pooling='avg', input_tensor=image_input_avg)\n",
    "eff_max_base = EfficientNetV2M(weights='imagenet', include_top=False, pooling='max', input_tensor=image_input_max)\n",
    "\n",
    "# Kloonaa ja nimeä uudelleen mallit\n",
    "def clone_and_rename(model, prefix):\n",
    "    # Kloonaa malli ja nimeä kaikki kerrokset uudelleen\n",
    "    for layer in model.layers:\n",
    "        layer._name = prefix + '_' + layer.name\n",
    "    return model\n",
    "\n",
    "eff_avg_cloned = clone_and_rename(eff_avg_base, 'eff_avg')\n",
    "eff_max_cloned = clone_and_rename(eff_max_base, 'eff_max')\n",
    "\n",
    "# Luo Model-instanssit uudelleen kloonatuille malleille\n",
    "model_avg = Model(inputs=image_input_avg, outputs=eff_avg_cloned.output, name='model_avg')\n",
    "model_max = Model(inputs=image_input_max, outputs=eff_max_cloned.output, name='model_max')\n",
    "\n",
    "model_avg.trainable = True\n",
    "for layer in model_avg.layers[:-34]:\n",
    "    layer.trainable = False\n",
    "model_max.trainable = True\n",
    "for layer in model_max.layers[:-34]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Käytä NAS-mallia, jos se on määritelty\n",
    "nas_output = nas_model([model_avg.output, model_max.output])\n",
    "\n",
    "# Rakenna lopullinen malli\n",
    "finetune_model = Model(inputs=[image_input_avg, image_input_max], outputs=nas_output, name='finetune_model')\n",
    "# finetune_model.summary()\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    5e-4,    \n",
    "    alpha=0.05,\n",
    "    name=\"CosineDecay\",\n",
    "    decay_steps=len_train\n",
    ")\n",
    "\n",
    "# Aseta oppimisnopeuden aikataulu\n",
    "finetune_model.compile(optimizer=optimizers.RMSprop(learning_rate=lr_schedule), loss='mae', metrics=['mse', 'mae', 'mape', r2_score_tf])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_count = sum([tf.size(v).numpy() for v in finetune_model.trainable_weights])\n",
    "non_trainable_count = sum([tf.size(v).numpy() for v in finetune_model.non_trainable_weights])\n",
    "print(f\"Total parameters: {trainable_count + non_trainable_count:,}\")\n",
    "print(f\"Trainable parameters: {trainable_count:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n",
    "\n",
    "print(f'Total parameters from EfficientNetV2M: { (trainable_count + non_trainable_count) - (trainable_count_nas + non_trainable_count_nas):,}')\n",
    "print(f'Trainable from EfficientNetV2M: {trainable_count - trainable_count_nas:,}')\n",
    "print(f'Non trainable from EfficientNetV2M: {non_trainable_count - non_trainable_count_nas:,}')\n",
    "\n",
    "print(f'Trainable fron NAS: {trainable_count_nas:,}')\n",
    "print(f'Non trainable from NAS: {non_trainable_count_nas:,}')\n",
    "print(F'Total parameters from NAS: {trainable_count_nas + non_trainable_count_nas:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainImageLoggingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, log_dir, dataset, num_images=10):\n",
    "        super().__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "        self.num_images = num_images        \n",
    "        self.dataset = dataset.unbatch().take(num_images)  # Ota vain pieni määrä kuvia loggausta varten ja pura batchit\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        with self.writer.as_default():\n",
    "            for (img_batch, _), _ in self.dataset:\n",
    "                # Olettaen, että tuplakuvat ovat identtisiä, voit loggaa vain yhden niistä\n",
    "                img_batch = tf.expand_dims(img_batch, axis=0)\n",
    "                img = tf.clip_by_value(img_batch, 0, 255)\n",
    "                img = tf.cast(img_batch, tf.uint8)                \n",
    "                tf.summary.image(\"Training Data Augmentation\", img, step=epoch, max_outputs=20)\n",
    "        self.writer.flush()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Creating logs for trial: {study_name} in ./logs/trial_{study_name}')\n",
    "log_folder = f\"./logs/trial_{study_name}\"\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "# Aseta logitiedostojen hakemisto\n",
    "tensorboard_callback = TensorBoard(log_dir=log_folder, histogram_freq=1, update_freq='epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./NN_search/testifinetus_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mae\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,  verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m      3\u001b[0m     tensorboard_callback,\n\u001b[1;32m      4\u001b[0m     TrainImageLoggingCallback(log_folder, train_dataset)    \n\u001b[1;32m      5\u001b[0m ]\n\u001b[0;32m----> 7\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mfinetune_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=f'./NN_search/testifinetus_{study_name}.h5', monitor='val_mae', save_best_only=True, save_weights_only=True, mode = 'min',  verbose = 1),\n",
    "    tensorboard_callback,\n",
    "    TrainImageLoggingCallback(log_folder, train_dataset)    \n",
    "]\n",
    "\n",
    "history = finetune_model.fit(train_dataset, validation_data=valid_dataset, epochs=EPOCHS, verbose=1, callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model.load_weights(f'./NN_search/testifinetus_{study_name}.h5')\n",
    "# finetune_model.save(f'./NN_search/koe', save_format='tf') # TODO tässä ongelmaa vielä, mutta ei väliä. Malli on jo olemassa ja sitä voi käyttää. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, log_base in log_transforms.items():\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_plot[target] = np.log(train_plot[target]) / np.log(log_base)\n",
    "        \n",
    "    elif log_base == 'sqrt':\n",
    "        train_plot[target] = np.sqrt(train_plot[target])\n",
    "        \n",
    "    elif log_base == 'cbrt':\n",
    "        train_plot[target] = np.cbrt(train_plot[target])\n",
    "        \n",
    "    else:\n",
    "        train_plot[target] = train_plot[target]\n",
    "        \n",
    "for target, scaler in scaler_transforms.items():\n",
    "    if scaler is not None:\n",
    "        train_plot[target] = scaler.transform(train_plot[target].values.reshape(-1, 1)).flatten()\n",
    "        train_plot[target] = scaler.transform(train_plot[target].values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_transforms)\n",
    "print(scaler_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, columns_names):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Setting up a grid of plots with 2 columns\n",
    "    n_cols = 6\n",
    "    n_rows = len(columns_names) // n_cols + (len(columns_names) % n_cols > 0)\n",
    "\n",
    "    for i, col in enumerate(columns_names):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_original, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_plot, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training_name = './data/results_finetune_images.pickle'\n",
    "\n",
    "if os.path.exists(results_training_name):\n",
    "    results_training = pd.read_pickle(results_training_name)\n",
    "else:\n",
    "    columns = ['Train R2', 'Train MSE', 'Train MAE', 'Train MAPE', 'Valid R2', 'Valid MSE', 'Valid MAE', 'Valid MAPE', 'Train preds Desc', 'Valid preds Desc', 'Test preds Desc' , 'Original data Desc' 'Kaggle R2', 'Scalers', 'Log/Pot transforms']\n",
    "    results_training = pd.DataFrame(columns = columns)\n",
    "    results_training.index.name = 'Study name'\n",
    "\n",
    "study_name_result = f'{study_name}_finetuned'\n",
    "\n",
    "if study_name_result not in results_training.index:    \n",
    "    results_training.loc[study_name] = [None]*len(results_training.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, y_train_transformed.values))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, y_valid_transformed.values))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TRAINING DATA TEST\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "train_pred = finetune_model.predict(train_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        train_pred[:, i] = scaler.inverse_transform(train_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_pred[:, i] = np.power(log_base, train_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        train_pred[:, i] = np.square(train_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        train_pred[:, i] = np.power(train_pred[:, i], 3)\n",
    "\n",
    "R2_train = r2_score(y_train, train_pred)\n",
    "MSE_train = mean_squared_error(y_train, train_pred)\n",
    "MAE_train = mean_absolute_error(y_train, train_pred)\n",
    "MAPE_train = mean_absolute_percentage_error(y_train, train_pred)\n",
    "\n",
    "print(f'Train scores:\\nR2 : {R2_train:.5f}, MSE : {MSE_train:.5f}, MAE : {MAE_train:.5f}, MAPE : {MAPE_train:.5f}')\n",
    "results_training.at[study_name_result, 'Train R2'] = R2_train\n",
    "results_training.at[study_name_result, 'Train MSE'] = MSE_train\n",
    "results_training.at[study_name_result, 'Train MAE'] = MAE_train\n",
    "results_training.at[study_name_result, 'Train MAPE'] = MAPE_train\n",
    "\n",
    "trainining_preds_desc = pd.DataFrame(train_pred, columns = mean_columns).describe().to_json()\n",
    "results_training.at[study_name_result, 'Train preds Desc'] = trainining_preds_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## VALIDATION DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "valid_pred = finetune_model.predict(valid_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        valid_pred[:, i] = scaler.inverse_transform(valid_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        valid_pred[:, i] = np.square(valid_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        valid_pred[:, i] = np.power(valid_pred[:, i], 3)\n",
    "\n",
    "R2_valid = r2_score(y_valid, valid_pred)\n",
    "MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "MAPE_valid = mean_absolute_percentage_error(y_valid, valid_pred)\n",
    "\n",
    "print(f'Valid scores:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}, MAPE : {MAPE_valid:.5f}')\n",
    "results_training.at[study_name_result, 'Valid R2'] = R2_valid\n",
    "results_training.at[study_name_result, 'Valid MSE'] = MSE_valid\n",
    "results_training.at[study_name_result, 'Valid MAE'] = MAE_valid\n",
    "results_training.at[study_name_result, 'Valid MAPE'] = MAPE_valid\n",
    "\n",
    "valid_preds_desc = pd.DataFrame(valid_pred, columns = mean_columns).describe().to_json()\n",
    "results_training.at[study_name_result, 'Valid preds Desc'] = valid_preds_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "submission_df = test_df_copy[['id']].copy()\n",
    "\n",
    "\n",
    "\n",
    "test_images_path = test_df_copy['image_path'].values\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_images_path)\n",
    "test_dataset = test_dataset.map(process_path_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "predictions = finetune_model.predict(test_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        predictions[:, i] = scaler.inverse_transform(predictions[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target: : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        predictions[:, i] = np.power(log_base, predictions[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        predictions[:, i] = np.square(predictions[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        predictions[:, i] = np.power(predictions[:, i], 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_columns = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n",
    "\n",
    "submission_df[target_columns] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_preds_desc = submission_df[target_columns].describe().to_json()\n",
    "results_training.at[study_name_result, 'Test preds Desc'] = test_preds_desc \n",
    "\n",
    "original_data_desc = train_original[mean_columns].describe().to_json()\n",
    "results_training.at[study_name_result, 'Original data Desc'] = original_data_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{str(log_transforms.items())}')\n",
    "print(f'{str(scaler_transforms.items())}')\n",
    "\n",
    "results_training.at[study_name_result, 'Scalers'] = f'{scaler_transforms}'\n",
    "results_training.at[study_name_result, 'Log/Pot transforms'] = f'{str(log_transforms.items())}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training.at[study_name_result, 'Kaggle R2'] = None\n",
    "\n",
    "for index, row in results_training.iterrows():\n",
    "    print(f\"Study Name: {index}\")\n",
    "    print(f'Kaggle R2: {row[\"Kaggle R2\"]}')\n",
    "    print(f\"Train R2: {row['Train R2']}, Train MSE: {row['Train MSE']}, Train MAE : {row['Train MAE']}, Train MAPE: {row['Train MAPE']}\")\n",
    "    print(f'Valid R2: {row[\"Valid R2\"]}, Valid MSE: {row[\"Valid MSE\"]}, Valid MAE: {row[\"Valid MAE\"]}, Valid MAPE: {row[\"Valid MAPE\"]}')\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Train preds Description:\")\n",
    "    display(pd.read_json(row['Train preds Desc']))\n",
    "    print(\"Valid preds Description:\")\n",
    "    display(pd.read_json(row['Valid preds Desc']))\n",
    "    print(\"Test preds Description:\")\n",
    "    display(pd.read_json(row['Test preds Desc']))\n",
    "    print(\"Original data Description:\")\n",
    "    display(pd.read_json(row['Original data Desc']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df.info())\n",
    "\n",
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_training_name, 'wb') as f:\n",
    "    results_training.to_pickle(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
