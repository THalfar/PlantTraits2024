{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gc\n",
    "from gcvit import GCViTBase\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Concatenate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '426_convnextbase_003_998_1'\n",
    "\n",
    "pickle_file_path = f'./data/test_{study_name}.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "pickle_file_path = f'./data/train_{study_name}.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_row', None) \n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_study_name = '511_cgvit_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO CGViT jossa global average ja gloval maxpooling lopussa yhteen tökätty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-11 13:52:25.133250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.194512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.194577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.198367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.198436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.198478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.352688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.352794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.352804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-11 13:52:25.352851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-11 13:52:25.352913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:07:00.0, compute capability: 8.6\n",
      "2024-05-11 13:52:25.754362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2024-05-11 13:52:26.259216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer patch_embed\n",
      "Layer pos_drop\n",
      "Layer levels_0\n",
      "Layer levels_1\n",
      "Layer levels_2\n",
      "Layer levels_3\n",
      "Layer norm\n",
      "Layer pool\n",
      "Layer head\n"
     ]
    }
   ],
   "source": [
    "base_model = GCViTBase(pretrain=True, input_shape=(480, 480, 3), resize_query=True)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    print(f'Layer {layer.name}')\n",
    "\n",
    "feature_layer = base_model.get_layer('pool')\n",
    "\n",
    "avg_pool = GlobalAveragePooling2D()(feature_layer.output)\n",
    "max_pool = GlobalMaxPooling2D()(feature_layer.output)\n",
    "\n",
    "output = Concatenate([avg_pool, max_pool])\n",
    "\n",
    "model = tf.keras.Model(base_model.input, output)\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(f'Layer {layer.name}')\n",
    "    print(f'Layer {layer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'include_top')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize the base model\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mGCViTTiny\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 'include_top=False' is an assumption\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Assuming you know the name or index of the second to last layer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# This is an example, you need to replace 'second_to_last_layer_name' with the actual name or index\u001b[39;00m\n\u001b[1;32m     13\u001b[0m second_to_last_output \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39mget_layer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond_to_last_layer_name\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/gcvit/models/gcvit.py:307\u001b[0m, in \u001b[0;36mGCViTTiny\u001b[0;34m(input_shape, pretrain, from_kaggle, resize_query, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m config \u001b[38;5;241m=\u001b[39m NAME2CONFIG[name]\n\u001b[1;32m    306\u001b[0m ckpt_link \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_weights.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(BASE_URL, TAG, name)\n\u001b[0;32m--> 307\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGCViT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m model(\n\u001b[1;32m    309\u001b[0m     tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(shape\u001b[38;5;241m=\u001b[39minput_shape)[\n\u001b[1;32m    310\u001b[0m         tf\u001b[38;5;241m.\u001b[39mnewaxis,\n\u001b[1;32m    311\u001b[0m     ]\n\u001b[1;32m    312\u001b[0m )\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrain:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/gcvit/models/gcvit.py:100\u001b[0m, in \u001b[0;36mGCViT.__init__\u001b[0;34m(self, window_size, dim, depths, num_heads, drop_rate, mlp_ratio, qkv_bias, qk_scale, attn_drop, path_drop, layer_scale, resize_query, global_pool, num_classes, head_act, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     83\u001b[0m     window_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     99\u001b[0m ):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size \u001b[38;5;241m=\u001b[39m window_size\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim \u001b[38;5;241m=\u001b[39m dim\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kwarg \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwarg \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 514\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'include_top')"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "batch_size = 16\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "# Initialize the GCViT model\n",
    "base_model = GCViTBase(pretrain=True, input_shape=(480, 480, 3), resize_query=True)\n",
    "\n",
    "# Define the pooling layers\n",
    "global_avg_pooling = tf.keras.layers.GlobalAveragePooling2D()\n",
    "global_max_pooling = tf.keras.layers.GlobalMaxPooling2D()\n",
    "concatenate_layer = tf.keras.layers.Concatenate()\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "def extract_features_batch(image_paths):\n",
    "    img_batch = np.stack([load_and_preprocess_image(img_path) for img_path in image_paths])\n",
    "    features = base_model.predict(img_batch, batch_size=batch_size)\n",
    "    \n",
    "    # Apply pooling\n",
    "    avg_pooled = global_avg_pooling(features)\n",
    "    max_pooled = global_max_pooling(features)\n",
    "    \n",
    "    # Concatenate features\n",
    "    concatenated_features = concatenate_layer([avg_pooled, max_pooled])\n",
    "    return concatenated_features\n",
    "\n",
    "# Assume 'train_df' is your DataFrame containing image paths\n",
    "image_paths = train_df['image_path'].values\n",
    "\n",
    "features_list = []\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    batch_features = extract_features_batch(batch_paths)\n",
    "    features_list.append(batch_features)\n",
    "    if i // batch_size % 30 == 0:  # Clear session every 30 batches\n",
    "        tf.keras.backend.clear_session()\n",
    "        print(f'Clearing session')\n",
    "\n",
    "# Stack all batch features\n",
    "all_features = np.vstack(features_list)\n",
    "# Assuming you want to store these features as a list in your DataFrame\n",
    "train_df['features'] = list(all_features)\n",
    "\n",
    "print(\"Feature extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Specify the file path to save the pickle file\n",
    "pickle_file_path = './data/train_convnextbase_df_003_998.pickle'\n",
    "\n",
    "# Save the train_df dataframe as a pickle file\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(train_df, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "\n",
    "\n",
    "base_model = ConvNeXtBase(weights='imagenet', include_top=False, pooling='avg')\n",
    "base_model.trainable = False\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_features_batch(image_paths):\n",
    "    img_batch = np.stack([load_and_preprocess_image(img_path) for img_path in image_paths])\n",
    "    features = base_model.predict(img_batch)        \n",
    "    return features\n",
    "\n",
    "image_paths = test_df['image_path'].values\n",
    "\n",
    "features_list = []\n",
    "j = 0\n",
    "for i in range(0, len(image_paths), batch_size):\n",
    "    batch_paths = image_paths[i:i+batch_size]\n",
    "    batch_features = extract_features_batch(batch_paths)\n",
    "    features_list.append(batch_features)\n",
    "    j += 1\n",
    "    if j % 30 == 0:\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        print(f'Clearing session')\n",
    "\n",
    "all_features = np.vstack(features_list)\n",
    "test_df['features_avg'] = list(all_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df.shape)\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = './data/test_convnextbase_df_003_998.pickle'\n",
    "\n",
    "# # Save the train_df dataframe as a pickle file\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n",
    "\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
