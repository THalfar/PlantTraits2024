{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2M\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(f'Current GPU allocator: {os.getenv(\"TF_GPU_ALLOCATOR\")}')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(f'Setting memory growth for {gpu}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '417_nas_all_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "\n",
    "\n",
    "selected_features_pickle_path = './data/selected_features_list.pickle'\n",
    "with open(selected_features_pickle_path, 'rb') as f:\n",
    "    FEATURE_COLS = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_columns = [col for col in train_df.columns if col.endswith('_sd')]\n",
    "train_df.drop(columns=sd_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = './data/train_images/'\n",
    "test_images_path = './data/test_images/'    \n",
    "\n",
    "train_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(train_images_path, f'{x}.jpeg'))\n",
    "test_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(test_images_path, f'{x}.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in mean_columns:\n",
    "    lower_quantile = train_df[column].quantile(0.005)\n",
    "    upper_quantile = train_df[column].quantile(0.985)  \n",
    "    train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, trait in enumerate(mean_columns):\n",
    "\n",
    "    # Determine the bin edges dynamically based on the distribution of traits\n",
    "    bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, 5 + 1))\n",
    "    train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n",
    "\n",
    "# Concatenate the bins into a final bin\n",
    "train_df[\"final_bin\"] = (\n",
    "    train_df[[f\"bin_{i}\" for i in range(len(mean_columns))]]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "\n",
    "# Perform the stratified split using final bin\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n",
    "    train_df.loc[valid_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "sample_df = train_df.copy()\n",
    "train_df = sample_df[sample_df.fold != 3]\n",
    "valid_df = sample_df[sample_df.fold == 3]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "\n",
    "train_df[FEATURE_COLS] = scaler.fit_transform(train_df[FEATURE_COLS].values)\n",
    "valid_df[FEATURE_COLS] = scaler.transform(valid_df[FEATURE_COLS].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "directory_path = './NN_search'\n",
    "pattern = f\"{directory_path}/{study_name}_best_val_*.h5\"\n",
    "\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "max_r2_score = float('-inf')\n",
    "best_model = None\n",
    "\n",
    "# Käy läpi jokainen tiedosto ja etsi suurin r2_score_inv\n",
    "for file in files:\n",
    "    value = float(file.split('best_val')[1].split('_')[1])\n",
    "    if value > max_r2_score:\n",
    "        max_r2_score = value\n",
    "        best_model = file\n",
    "\n",
    "\n",
    "# Tulosta suurin löydetty r2_score_inv ja vastaava tiedosto\n",
    "print(f\"Best R2-score: {max_r2_score:.5f}\")\n",
    "if best_model:\n",
    "    print(f\"Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"No best model found\")\n",
    "\n",
    "best_log_transforms_name =  f'./NN_search/{study_name}_{max_r2_score:.5f}_best_log_transforms.pickle'\n",
    "best_scalers_name = f'./NN_search/{study_name}_{max_r2_score:.5f}_best_scalers.pickle'\n",
    "\n",
    "print(f'Opening log transforms from {best_log_transforms_name}')\n",
    "with open(best_log_transforms_name, 'rb') as f:\n",
    "    log_transforms = pickle.load(f)\n",
    "\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = tf.image.random_contrast(img, lower=0.5, upper=1.5)    \n",
    "    img = tf.image.random_crop(img, size=[480, 480, 3])  \n",
    "    return img\n",
    "\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    img = augment_image(img)  \n",
    "    return img\n",
    "\n",
    "def process_image_valid(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# Define your dataset processing function\n",
    "def process_path_train(file_path, tabular_data, targets):\n",
    "    img = process_image(file_path)\n",
    "    return (img, img, tabular_data), targets\n",
    "\n",
    "\n",
    "def process_path_valid(file_path, tabular_data, targets):\n",
    "    img = process_image_valid(file_path)\n",
    "    return (img, img, tabular_data), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[mean_columns]\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n",
    "\n",
    "y_train_transformed = y_train.copy()\n",
    "y_valid_transformed = y_valid.copy()\n",
    "\n",
    "\n",
    "for target, log_base in log_transforms.items():\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "        y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "    elif log_base == 'sqrt':\n",
    "        y_train_transformed[target] = np.sqrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.sqrt(y_valid[target])\n",
    "\n",
    "    elif log_base == 'cbrt':\n",
    "        y_train_transformed[target] = np.cbrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.cbrt(y_valid[target])\n",
    "\n",
    "    else:\n",
    "        y_train_transformed[target] = y_train[target]\n",
    "        y_valid_transformed[target] = y_valid[target]    \n",
    "\n",
    "for target, scaler in scaler_transforms.items():\n",
    "    if scaler is not None:\n",
    "        y_train_transformed[target] = scaler.fit_transform(y_train_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "        y_valid_transformed[target] = scaler.transform(y_valid_transformed[target].values.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tabular = train_df[FEATURE_COLS].values\n",
    "valid_tabular = valid_df[FEATURE_COLS].values\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, train_tabular, y_train_transformed.values))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, valid_tabular, y_valid_transformed.values))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# Aseta logitiedostojen hakemisto\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, update_freq='epoch')\n",
    "\n",
    "\n",
    "class CustomLearningRateSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, warmup_steps, warmup_target, decay_steps, alpha):\n",
    "        super(CustomLearningRateSchedule, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_target = warmup_target\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def __call__(self, step):\n",
    "        linear_warmup = self.initial_learning_rate + step / self.warmup_steps * (self.warmup_target - self.initial_learning_rate)\n",
    "        cosine_decay = self.alpha + (self.warmup_target - self.alpha) * 0.5 * (1 + tf.cos(tf.constant * (step - self.warmup_steps) / self.decay_steps))\n",
    "        return tf.where(step < self.warmup_steps, linear_warmup, cosine_decay)        \n",
    "        \n",
    "\n",
    "# Käytä mukautettua oppimisnopeuden aikataulua\n",
    "lr_schedule_custom = CustomLearningRateSchedule(\n",
    "    initial_learning_rate=0.0001,\n",
    "    warmup_steps=3,\n",
    "    warmup_target=0.0005,\n",
    "    decay_steps=17,\n",
    "    alpha=0.00001\n",
    ")\n",
    "\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Haetaan nykyinen oppimisnopeus optimizerilta\n",
    "        lr = self.model.optimizer._decayed_lr(tf.float32).numpy()\n",
    "        print(f\"Learning rate at epoch {epoch + 1} is {lr}\")\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "image_input_avg = Input(shape=(480, 480, 3), name='image_input_avg')\n",
    "image_input_max = Input(shape=(480, 480, 3), name='image_input_max')\n",
    "tabular_input = Input(shape=(len(FEATURE_COLS),), name='tabular_input')\n",
    "\n",
    "eff_avg = EfficientNetV2M(weights='imagenet', include_top=False, pooling='avg', input_tensor=image_input_avg)\n",
    "\n",
    "eff_avg.trainable = True \n",
    "for layer in eff_avg.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "eff_avg_output = eff_avg(image_input_avg)\n",
    "\n",
    "model_avg  = Model(inputs=image_input_avg, outputs = eff_avg_output, name='model_avg')\n",
    "\n",
    "eff_max = EfficientNetV2M(weights='imagenet', include_top=False, pooling='max', input_tensor=image_input_max)\n",
    "\n",
    "for layer in eff_max.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "eff_max_output = eff_max(image_input_max)\n",
    "\n",
    "model_max = Model(inputs=image_input_max, outputs = eff_max_output, name='model_max')\n",
    "\n",
    "model_avg = model_avg(image_input_avg)\n",
    "model_max = model_max(image_input_max)\n",
    "\n",
    "concatenated_img = Concatenate()([model_avg, model_max])\n",
    "\n",
    "img_layer = Dense(396, activation='LeakyReLU', kernel_initializer = 'glorot_uniform')(concatenated_img)\n",
    "img_layer = layers.BatchNormalization()(img_layer)\n",
    "img_layer = Dropout(0.7)(img_layer)\n",
    "\n",
    "tab_layer = Dense(419, activation='relu', kernel_initializer = 'random_normal')(tabular_input)\n",
    "tab_layer = layers.BatchNormalization()(tab_layer)\n",
    "tab_layer = Dropout(0.2)(tab_layer)\n",
    "\n",
    "concatanate_all = Concatenate()([img_layer, tab_layer])\n",
    "\n",
    "dense_layer = Dense(378, activation='elu', kernel_initializer = 'lecun_uniform')(concatanate_all)\n",
    "dense_layer = Dropout(0.1)(dense_layer)\n",
    "\n",
    "output_layer = Dense(6, activation='linear')(dense_layer)\n",
    "\n",
    "model = Model(inputs=[image_input_avg, image_input_max, tabular_input], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer= optimizers.RMSprop(learning_rate = lr_schedule_custom), loss='mse', metrics=['mse', 'mae', r2_score_tf])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=f'./NN_search/testifinetus_best_val.h5', monitor='val_r2_score_tf', save_best_only=True, mode = 'max', save_weights_only=True),\n",
    "    LearningRateLogger(),\n",
    "    tensorboard_callback\n",
    "]\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=valid_dataset, epochs=20, verbose=1, callbacks=callbacks)\n",
    "model.load_weights(f'./NN_search/testifinetus_best_val.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tabular = train_df[FEATURE_COLS].values\n",
    "valid_tabular = valid_df[FEATURE_COLS].values\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, train_tabular, y_train_transformed.values))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, valid_tabular, y_valid_transformed.values))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "## TEST DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "train_pred = model.predict(train_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        train_pred[:, i] = scaler.inverse_transform(train_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_pred[:, i] = np.power(log_base, train_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        train_pred[:, i] = np.square(train_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        train_pred[:, i] = np.power(train_pred[:, i], 3)\n",
    "\n",
    "R2_train = r2_score(y_train, train_pred)\n",
    "MSE_train = mean_squared_error(y_train, train_pred)\n",
    "MAE_train = mean_absolute_error(y_train, train_pred)\n",
    "\n",
    "print(f'Train scores:\\nR2 : {R2_train:.5f}, MSE : {MSE_train:.5f}, MAE : {MAE_train:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## VALIDATION DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "valid_pred = best_model.predict(valid_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        valid_pred[:, i] = scaler.inverse_transform(valid_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        valid_pred[:, i] = np.square(valid_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        valid_pred[:, i] = np.power(valid_pred[:, i], 3)\n",
    "\n",
    "R2_valid = r2_score(y_valid, valid_pred)\n",
    "MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "\n",
    "print(f'Valid scores:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
