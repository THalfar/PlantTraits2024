{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 18:25:24.990472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 18:25:25.615565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU allocator: cuda_malloc_async\n",
      "Setting memory growth for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 18:25:26.177901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-17 18:25:26.202337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-17 18:25:26.202407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2M\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(f'Current GPU allocator: {os.getenv(\"TF_GPU_ALLOCATOR\")}')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(f'Setting memory growth for {gpu}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '417_nas_all_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "\n",
    "\n",
    "selected_features_pickle_path = './data/selected_features_list.pickle'\n",
    "with open(selected_features_pickle_path, 'rb') as f:\n",
    "    FEATURE_COLS = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('./data/train.csv')\n",
    "test_df = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_columns = [col for col in train_df.columns if col.endswith('_sd')]\n",
    "train_df.drop(columns=sd_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_path = './data/train_images/'\n",
    "test_images_path = './data/test_images/'    \n",
    "\n",
    "train_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(train_images_path, f'{x}.jpeg'))\n",
    "test_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(test_images_path, f'{x}.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in mean_columns:\n",
    "    lower_quantile = train_df[column].quantile(0.005)\n",
    "    upper_quantile = train_df[column].quantile(0.985)  \n",
    "    train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/miniconda3/envs/tf/lib/python3.9/site-packages/sklearn/model_selection/_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for i, trait in enumerate(mean_columns):\n",
    "\n",
    "    # Determine the bin edges dynamically based on the distribution of traits\n",
    "    bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, 5 + 1))\n",
    "    train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n",
    "\n",
    "# Concatenate the bins into a final bin\n",
    "train_df[\"final_bin\"] = (\n",
    "    train_df[[f\"bin_{i}\" for i in range(len(mean_columns))]]\n",
    "    .astype(str)\n",
    "    .agg(\"\".join, axis=1)\n",
    ")\n",
    "\n",
    "# Perform the stratified split using final bin\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n",
    "    train_df.loc[valid_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Num Train: 39335 | Num Valid: 9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1026380/2079233736.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df[FEATURE_COLS] = scaler.fit_transform(train_df[FEATURE_COLS].values)\n",
      "/tmp/ipykernel_1026380/2079233736.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_df[FEATURE_COLS] = scaler.transform(valid_df[FEATURE_COLS].values)\n"
     ]
    }
   ],
   "source": [
    "scaler = RobustScaler()\n",
    "\n",
    "sample_df = train_df.copy()\n",
    "train_df = sample_df[sample_df.fold != 3]\n",
    "valid_df = sample_df[sample_df.fold == 3]\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n",
    "\n",
    "\n",
    "train_df[FEATURE_COLS] = scaler.fit_transform(train_df[FEATURE_COLS].values)\n",
    "valid_df[FEATURE_COLS] = scaler.transform(valid_df[FEATURE_COLS].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R2-score: 0.29580\n",
      "Best model: ./NN_search/417_nas_all_3_best_val_0.29580_model.h5\n",
      "Opening log transforms from ./NN_search/417_nas_all_3_0.29580_best_log_transforms.pickle\n",
      "Opening scalers from ./NN_search/417_nas_all_3_0.29580_best_scalers.pickle\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "directory_path = './NN_search'\n",
    "pattern = f\"{directory_path}/{study_name}_best_val_*.h5\"\n",
    "\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "max_r2_score = float('-inf')\n",
    "best_model = None\n",
    "\n",
    "# Käy läpi jokainen tiedosto ja etsi suurin r2_score_inv\n",
    "for file in files:\n",
    "    value = float(file.split('best_val')[1].split('_')[1])\n",
    "    if value > max_r2_score:\n",
    "        max_r2_score = value\n",
    "        best_model = file\n",
    "\n",
    "\n",
    "# Tulosta suurin löydetty r2_score_inv ja vastaava tiedosto\n",
    "print(f\"Best R2-score: {max_r2_score:.5f}\")\n",
    "if best_model:\n",
    "    print(f\"Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"No best model found\")\n",
    "\n",
    "best_log_transforms_name =  f'./NN_search/{study_name}_{max_r2_score:.5f}_best_log_transforms.pickle'\n",
    "best_scalers_name = f'./NN_search/{study_name}_{max_r2_score:.5f}_best_scalers.pickle'\n",
    "\n",
    "print(f'Opening log transforms from {best_log_transforms_name}')\n",
    "with open(best_log_transforms_name, 'rb') as f:\n",
    "    log_transforms = pickle.load(f)\n",
    "\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scaler are: {scaler_transforms}')\n",
    "print(f'Log transforms are: {log_transforms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    img = tf.image.random_brightness(img, max_delta=0.2)\n",
    "    img = tf.image.random_contrast(img, lower=0.5, upper=1.5)    \n",
    "    img = tf.image.random_crop(img, size=[480, 480, 3])  \n",
    "    return img\n",
    "\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    img = augment_image(img)  \n",
    "    return img\n",
    "\n",
    "def process_image_valid(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "# Define your dataset processing function\n",
    "def process_path_train(file_path, tabular_data, targets):\n",
    "    img = process_image(file_path)\n",
    "    return (img, img, tabular_data), targets\n",
    "\n",
    "\n",
    "def process_path_valid(file_path, tabular_data, targets):\n",
    "    img = process_image_valid(file_path)\n",
    "    return (img, img, tabular_data), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[mean_columns]\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n",
    "\n",
    "y_train_transformed = y_train.copy()\n",
    "y_valid_transformed = y_valid.copy()\n",
    "\n",
    "\n",
    "for target, log_base in log_transforms.items():\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "        y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "    elif log_base == 'sqrt':\n",
    "        y_train_transformed[target] = np.sqrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.sqrt(y_valid[target])\n",
    "\n",
    "    elif log_base == 'cbrt':\n",
    "        y_train_transformed[target] = np.cbrt(y_train[target])\n",
    "        y_valid_transformed[target] = np.cbrt(y_valid[target])\n",
    "\n",
    "    else:\n",
    "        y_train_transformed[target] = y_train[target]\n",
    "        y_valid_transformed[target] = y_valid[target]    \n",
    "\n",
    "for target, scaler in scaler_transforms.items():\n",
    "    if scaler is not None:\n",
    "        y_train_transformed[target] = scaler.fit_transform(y_train_transformed[target].values.reshape(-1, 1)).flatten()\n",
    "        y_valid_transformed[target] = scaler.transform(y_valid_transformed[target].values.reshape(-1, 1)).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 18:25:28.068285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-17 18:25:28.068399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-17 18:25:28.068449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-17 18:25:28.183716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-17 18:25:28.183794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-17 18:25:28.183803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-17 18:25:28.183857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-17 18:25:28.183873: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:226] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-04-17 18:25:28.184182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 50000 \n",
    "\n",
    "train_tabular = train_df[FEATURE_COLS].values\n",
    "valid_tabular = valid_df[FEATURE_COLS].values\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, train_tabular, y_train_transformed.values))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, valid_tabular, y_valid_transformed.values))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " image_input_avg (InputLayer)   [(None, 480, 480, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " image_input_max (InputLayer)   [(None, 480, 480, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " model_avg (Functional)         (None, 1280)         53150388    ['image_input_avg[0][0]']        \n",
      "                                                                                                  \n",
      " model_max (Functional)         (None, 1280)         53150388    ['image_input_max[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 2560)         0           ['model_avg[0][0]',              \n",
      "                                                                  'model_max[0][0]']              \n",
      "                                                                                                  \n",
      " tabular_input (InputLayer)     [(None, 81)]         0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 396)          1014156     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 419)          34358       ['tabular_input[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 396)         1584        ['dense[0][0]']                  \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 419)         1676        ['dense_1[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 396)          0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 419)          0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 815)          0           ['dropout[0][0]',                \n",
      "                                                                  'dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 378)          308448      ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 378)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 6)            2274        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 107,663,272\n",
      "Trainable params: 2,676,706\n",
      "Non-trainable params: 104,986,566\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# Aseta logitiedostojen hakemisto\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs', histogram_freq=1, update_freq='epoch')\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class EpochBasedLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, initial_learning_rate, warmup_epochs, warmup_target, total_epochs, alpha):\n",
    "        super(EpochBasedLRSchedule, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_target = warmup_target\n",
    "        self.total_epochs = total_epochs\n",
    "        self.alpha = alpha\n",
    "        self.current_epoch = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        self.current_epoch.assign(epoch)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = tf.cast(self.current_epoch, tf.float32)\n",
    "        linear_warmup = self.initial_learning_rate + (epoch / self.warmup_epochs) * (self.warmup_target - self.initial_learning_rate)\n",
    "        cosine_decay = self.alpha + (self.warmup_target - self.alpha) * 0.5 * (1 + tf.cos(tf.constant(np.pi, dtype=tf.float32) * (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)))\n",
    "        learning_rate = tf.where(epoch < self.warmup_epochs, linear_warmup, cosine_decay)\n",
    "        return learning_rate\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"warmup_epochs\": self.warmup_epochs,\n",
    "            \"warmup_target\": self.warmup_target,\n",
    "            \"total_epochs\": self.total_epochs,\n",
    "            \"alpha\": self.alpha\n",
    "        }\n",
    "\n",
    "# Käytä mukautettua oppimisnopeuden aikataulua\n",
    "lr_schedule_custom = EpochBasedLRSchedule(\n",
    "    initial_learning_rate=0.0001,\n",
    "    warmup_epochs=3,\n",
    "    warmup_target=0.0005,\n",
    "    total_epochs=20,\n",
    "    alpha=0.00001\n",
    ")\n",
    "\n",
    "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Haetaan nykyinen oppimisnopeus optimizerilta\n",
    "        lr = self.model.optimizer._decayed_lr(tf.float32).numpy()\n",
    "        print(f\"Learning rate at epoch {epoch+1} is {lr}\")\n",
    "\n",
    "class UpdateLRScheduleCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, lr_schedule):\n",
    "        self.lr_schedule = lr_schedule\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.lr_schedule.on_epoch_begin(epoch)\n",
    "\n",
    "# Luo callback-olio\n",
    "update_lr_schedule = UpdateLRScheduleCallback(lr_schedule_custom)\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "image_input_avg = Input(shape=(480, 480, 3), name='image_input_avg')\n",
    "image_input_max = Input(shape=(480, 480, 3), name='image_input_max')\n",
    "tabular_input = Input(shape=(len(FEATURE_COLS),), name='tabular_input')\n",
    "\n",
    "eff_avg = EfficientNetV2M(weights='imagenet', include_top=False, pooling='avg', input_tensor=image_input_avg)\n",
    "\n",
    "eff_avg.trainable = True \n",
    "for layer in eff_avg.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "eff_avg_output = eff_avg(image_input_avg)\n",
    "\n",
    "model_avg  = Model(inputs=image_input_avg, outputs = eff_avg_output, name='model_avg')\n",
    "\n",
    "eff_max = EfficientNetV2M(weights='imagenet', include_top=False, pooling='max', input_tensor=image_input_max)\n",
    "\n",
    "for layer in eff_max.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "eff_max_output = eff_max(image_input_max)\n",
    "\n",
    "model_max = Model(inputs=image_input_max, outputs = eff_max_output, name='model_max')\n",
    "\n",
    "model_avg = model_avg(image_input_avg)\n",
    "model_max = model_max(image_input_max)\n",
    "\n",
    "concatenated_img = Concatenate()([model_avg, model_max])\n",
    "\n",
    "img_layer = Dense(396, activation='LeakyReLU', kernel_initializer = 'glorot_uniform')(concatenated_img)\n",
    "img_layer = layers.BatchNormalization()(img_layer)\n",
    "img_layer = Dropout(0.7)(img_layer)\n",
    "\n",
    "tab_layer = Dense(419, activation='relu', kernel_initializer = 'random_normal')(tabular_input)\n",
    "tab_layer = layers.BatchNormalization()(tab_layer)\n",
    "tab_layer = Dropout(0.2)(tab_layer)\n",
    "\n",
    "concatanate_all = Concatenate()([img_layer, tab_layer])\n",
    "\n",
    "dense_layer = Dense(378, activation='elu', kernel_initializer = 'lecun_uniform')(concatanate_all)\n",
    "dense_layer = Dropout(0.1)(dense_layer)\n",
    "\n",
    "output_layer = Dense(6, activation='linear')(dense_layer)\n",
    "\n",
    "model = Model(inputs=[image_input_avg, image_input_max, tabular_input], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer= optimizers.RMSprop(learning_rate = lr_schedule_custom), loss='mse', metrics=['mse', 'mae', r2_score_tf])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 18:25:43.917405: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype double and shape [39335,6]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2024-04-17 18:25:43.917714: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [39335,81]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-04-17 18:26:06.944777: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/model_avg/efficientnetv2-m/block1b_drop/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-04-17 18:26:10.173689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2024-04-17 18:26:10.182022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615/615 [==============================] - ETA: 0s - loss: 1.2794 - mse: 1.2794 - mae: 0.8621 - r2_score_tf: 0.0097"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 18:37:37.272428: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [9833]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-04-17 18:37:37.272724: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype double and shape [9833,6]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_r2_score_tf improved from -inf to 0.11837, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 0 is 9.999999747378752e-05\n",
      "615/615 [==============================] - 898s 1s/step - loss: 1.2794 - mse: 1.2794 - mae: 0.8621 - r2_score_tf: 0.0097 - val_loss: 0.5848 - val_mse: 0.5848 - val_mae: 0.5679 - val_r2_score_tf: 0.1184\n",
      "Epoch 2/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.7790 - mse: 0.7790 - mae: 0.6700 - r2_score_tf: 0.0406\n",
      "Epoch 2: val_r2_score_tf improved from 0.11837 to 0.13142, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 1 is 0.00023333332501351833\n",
      "615/615 [==============================] - 859s 1s/step - loss: 0.7790 - mse: 0.7790 - mae: 0.6700 - r2_score_tf: 0.0406 - val_loss: 0.5113 - val_mse: 0.5113 - val_mae: 0.5167 - val_r2_score_tf: 0.1314\n",
      "Epoch 3/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.5966 - mse: 0.5966 - mae: 0.5722 - r2_score_tf: 0.0895\n",
      "Epoch 3: val_r2_score_tf improved from 0.13142 to 0.19102, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 2 is 0.000366666674381122\n",
      "615/615 [==============================] - 859s 1s/step - loss: 0.5966 - mse: 0.5966 - mae: 0.5722 - r2_score_tf: 0.0895 - val_loss: 0.4597 - val_mse: 0.4597 - val_mae: 0.4806 - val_r2_score_tf: 0.1910\n",
      "Epoch 4/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.4712 - mse: 0.4712 - mae: 0.4928 - r2_score_tf: 0.1830\n",
      "Epoch 4: val_r2_score_tf improved from 0.19102 to 0.26122, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 3 is 0.0005000000237487257\n",
      "615/615 [==============================] - 859s 1s/step - loss: 0.4712 - mse: 0.4712 - mae: 0.4928 - r2_score_tf: 0.1830 - val_loss: 0.4135 - val_mse: 0.4135 - val_mae: 0.4505 - val_r2_score_tf: 0.2612\n",
      "Epoch 5/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.4128 - mse: 0.4128 - mae: 0.4537 - r2_score_tf: 0.2650\n",
      "Epoch 5: val_r2_score_tf improved from 0.26122 to 0.31492, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 4 is 0.0004958283971063793\n",
      "615/615 [==============================] - 852s 1s/step - loss: 0.4128 - mse: 0.4128 - mae: 0.4537 - r2_score_tf: 0.2650 - val_loss: 0.3843 - val_mse: 0.3843 - val_mae: 0.4310 - val_r2_score_tf: 0.3149\n",
      "Epoch 6/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3925 - mse: 0.3925 - mae: 0.4400 - r2_score_tf: 0.3017\n",
      "Epoch 6: val_r2_score_tf did not improve from 0.31492\n",
      "Learning rate at epoch 5 is 0.00048345568939112127\n",
      "615/615 [==============================] - 855s 1s/step - loss: 0.3925 - mse: 0.3925 - mae: 0.4400 - r2_score_tf: 0.3017 - val_loss: 0.3859 - val_mse: 0.3859 - val_mae: 0.4284 - val_r2_score_tf: 0.3146\n",
      "Epoch 7/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3823 - mse: 0.3823 - mae: 0.4336 - r2_score_tf: 0.3173\n",
      "Epoch 7: val_r2_score_tf improved from 0.31492 to 0.33095, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 6 is 0.00046330317854881287\n",
      "615/615 [==============================] - 857s 1s/step - loss: 0.3823 - mse: 0.3823 - mae: 0.4336 - r2_score_tf: 0.3173 - val_loss: 0.3740 - val_mse: 0.3740 - val_mae: 0.4238 - val_r2_score_tf: 0.3309\n",
      "Epoch 8/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3730 - mse: 0.3730 - mae: 0.4281 - r2_score_tf: 0.3306\n",
      "Epoch 8: val_r2_score_tf improved from 0.33095 to 0.34154, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 7 is 0.0004360571620054543\n",
      "615/615 [==============================] - 852s 1s/step - loss: 0.3730 - mse: 0.3730 - mae: 0.4281 - r2_score_tf: 0.3306 - val_loss: 0.3706 - val_mse: 0.3706 - val_mae: 0.4251 - val_r2_score_tf: 0.3415\n",
      "Epoch 9/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3641 - mse: 0.3641 - mae: 0.4234 - r2_score_tf: 0.3453\n",
      "Epoch 9: val_r2_score_tf improved from 0.34154 to 0.35360, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 8 is 0.0004026454989798367\n",
      "615/615 [==============================] - 856s 1s/step - loss: 0.3641 - mse: 0.3641 - mae: 0.4234 - r2_score_tf: 0.3453 - val_loss: 0.3686 - val_mse: 0.3686 - val_mae: 0.4204 - val_r2_score_tf: 0.3536\n",
      "Epoch 10/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3539 - mse: 0.3539 - mae: 0.4171 - r2_score_tf: 0.3599\n",
      "Epoch 10: val_r2_score_tf improved from 0.35360 to 0.36070, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 9 is 0.00036420588730834424\n",
      "615/615 [==============================] - 859s 1s/step - loss: 0.3539 - mse: 0.3539 - mae: 0.4171 - r2_score_tf: 0.3599 - val_loss: 0.3615 - val_mse: 0.3615 - val_mae: 0.4155 - val_r2_score_tf: 0.3607\n",
      "Epoch 11/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3460 - mse: 0.3460 - mae: 0.4121 - r2_score_tf: 0.3729\n",
      "Epoch 11: val_r2_score_tf did not improve from 0.36070\n",
      "Learning rate at epoch 10 is 0.0003220474172849208\n",
      "615/615 [==============================] - 855s 1s/step - loss: 0.3460 - mse: 0.3460 - mae: 0.4121 - r2_score_tf: 0.3729 - val_loss: 0.3656 - val_mse: 0.3656 - val_mae: 0.4228 - val_r2_score_tf: 0.3304\n",
      "Epoch 12/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3365 - mse: 0.3365 - mae: 0.4064 - r2_score_tf: 0.3884\n",
      "Epoch 12: val_r2_score_tf improved from 0.36070 to 0.36821, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 11 is 0.00027760572265833616\n",
      "615/615 [==============================] - 857s 1s/step - loss: 0.3365 - mse: 0.3365 - mae: 0.4064 - r2_score_tf: 0.3884 - val_loss: 0.3589 - val_mse: 0.3589 - val_mae: 0.4147 - val_r2_score_tf: 0.3682\n",
      "Epoch 13/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3273 - mse: 0.3273 - mae: 0.4010 - r2_score_tf: 0.4020\n",
      "Epoch 13: val_r2_score_tf improved from 0.36821 to 0.37381, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 12 is 0.00023239426082000136\n",
      "615/615 [==============================] - 854s 1s/step - loss: 0.3273 - mse: 0.3273 - mae: 0.4010 - r2_score_tf: 0.4020 - val_loss: 0.3591 - val_mse: 0.3591 - val_mae: 0.4113 - val_r2_score_tf: 0.3738\n",
      "Epoch 14/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3197 - mse: 0.3197 - mae: 0.3963 - r2_score_tf: 0.4127\n",
      "Epoch 14: val_r2_score_tf improved from 0.37381 to 0.37397, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 13 is 0.00018795256619341671\n",
      "615/615 [==============================] - 858s 1s/step - loss: 0.3197 - mse: 0.3197 - mae: 0.3963 - r2_score_tf: 0.4127 - val_loss: 0.3567 - val_mse: 0.3567 - val_mae: 0.4154 - val_r2_score_tf: 0.3740\n",
      "Epoch 15/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3120 - mse: 0.3120 - mae: 0.3922 - r2_score_tf: 0.4231\n",
      "Epoch 15: val_r2_score_tf improved from 0.37397 to 0.38139, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 14 is 0.00014579406706616282\n",
      "615/615 [==============================] - 842s 1s/step - loss: 0.3120 - mse: 0.3120 - mae: 0.3922 - r2_score_tf: 0.4231 - val_loss: 0.3521 - val_mse: 0.3521 - val_mae: 0.4098 - val_r2_score_tf: 0.3814\n",
      "Epoch 16/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.3045 - mse: 0.3045 - mae: 0.3874 - r2_score_tf: 0.4364\n",
      "Epoch 16: val_r2_score_tf improved from 0.38139 to 0.38340, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 15 is 0.00010735449905041605\n",
      "615/615 [==============================] - 837s 1s/step - loss: 0.3045 - mse: 0.3045 - mae: 0.3874 - r2_score_tf: 0.4364 - val_loss: 0.3508 - val_mse: 0.3508 - val_mae: 0.4097 - val_r2_score_tf: 0.3834\n",
      "Epoch 17/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.2985 - mse: 0.2985 - mae: 0.3834 - r2_score_tf: 0.4474\n",
      "Epoch 17: val_r2_score_tf improved from 0.38340 to 0.38416, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 16 is 7.394279236905277e-05\n",
      "615/615 [==============================] - 836s 1s/step - loss: 0.2985 - mse: 0.2985 - mae: 0.3834 - r2_score_tf: 0.4474 - val_loss: 0.3512 - val_mse: 0.3512 - val_mae: 0.4095 - val_r2_score_tf: 0.3842\n",
      "Epoch 18/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.2951 - mse: 0.2951 - mae: 0.3813 - r2_score_tf: 0.4519\n",
      "Epoch 18: val_r2_score_tf improved from 0.38416 to 0.38737, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 17 is 4.669676491175778e-05\n",
      "615/615 [==============================] - 836s 1s/step - loss: 0.2951 - mse: 0.2951 - mae: 0.3813 - r2_score_tf: 0.4519 - val_loss: 0.3502 - val_mse: 0.3502 - val_mae: 0.4080 - val_r2_score_tf: 0.3874\n",
      "Epoch 19/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.2935 - mse: 0.2935 - mae: 0.3805 - r2_score_tf: 0.4536\n",
      "Epoch 19: val_r2_score_tf improved from 0.38737 to 0.38815, saving model to ./NN_search/testifinetus_best_val.h5\n",
      "Learning rate at epoch 18 is 2.6544288630248047e-05\n",
      "615/615 [==============================] - 838s 1s/step - loss: 0.2935 - mse: 0.2935 - mae: 0.3805 - r2_score_tf: 0.4536 - val_loss: 0.3496 - val_mse: 0.3496 - val_mae: 0.4081 - val_r2_score_tf: 0.3882\n",
      "Epoch 20/20\n",
      "615/615 [==============================] - ETA: 0s - loss: 0.2907 - mse: 0.2907 - mae: 0.3786 - r2_score_tf: 0.4597\n",
      "Epoch 20: val_r2_score_tf did not improve from 0.38815\n",
      "Learning rate at epoch 19 is 1.4171590009937063e-05\n",
      "615/615 [==============================] - 841s 1s/step - loss: 0.2907 - mse: 0.2907 - mae: 0.3786 - r2_score_tf: 0.4597 - val_loss: 0.3504 - val_mse: 0.3504 - val_mae: 0.4084 - val_r2_score_tf: 0.3861\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=f'./NN_search/testifinetus_best_val.h5', monitor='val_r2_score_tf', save_best_only=True, mode = 'max',  verbose = 1),\n",
    "    LearningRateLogger(),\n",
    "    tensorboard_callback,\n",
    "    update_lr_schedule\n",
    "]\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=valid_dataset, epochs=20, verbose=1, callbacks=callbacks)\n",
    "# model.load_weights(f'./NN_search/testifinetus_best_val.h5')\n",
    "from tensoflor.keras.models import load_model\n",
    "model = load_model(f'./NN_search/testifinetus_best_val.h5', custom_objects={'r2_score_tf': r2_score_tf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Learning rate schedule 'EpochBasedLRSchedule' must override `get_config()` in order to be serializable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 32\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28msuper\u001b[39m(EpochBasedLRSchedule, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_learning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitial_learning_rate,\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m     30\u001b[0m         }\n\u001b[0;32m---> 32\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./NN_search/finetunetest1_model.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/optimizers/schedules/learning_rate_schedule.py:82\u001b[0m, in \u001b[0;36mLearningRateSchedule.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;129m@abc\u001b[39m\u001b[38;5;241m.\u001b[39mabstractmethod\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearning rate schedule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust override `get_config()` in order to be serializable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Learning rate schedule 'EpochBasedLRSchedule' must override `get_config()` in order to be serializable."
     ]
    }
   ],
   "source": [
    "\n",
    "class EpochBasedLRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, initial_learning_rate, warmup_epochs, warmup_target, total_epochs, alpha):\n",
    "        super(EpochBasedLRSchedule, self).__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.warmup_target = warmup_target\n",
    "        self.total_epochs = total_epochs\n",
    "        self.alpha = alpha\n",
    "        self.current_epoch = tf.Variable(0, dtype=tf.int64, trainable=False)\n",
    "\n",
    "    def on_epoch_begin(self, epoch):\n",
    "        self.current_epoch.assign(epoch)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = tf.cast(self.current_epoch, tf.float32)\n",
    "        linear_warmup = self.initial_learning_rate + (epoch / self.warmup_epochs) * (self.warmup_target - self.initial_learning_rate)\n",
    "        cosine_decay = self.alpha + (self.warmup_target - self.alpha) * 0.5 * (1 + tf.cos(tf.constant(np.pi, dtype=tf.float32) * (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)))\n",
    "        learning_rate = tf.where(epoch < self.warmup_epochs, linear_warmup, cosine_decay)\n",
    "        return learning_rate\n",
    "    \n",
    "    def get_config(self):\n",
    "        super(EpochBasedLRSchedule, self)\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"warmup_epochs\": self.warmup_epochs,\n",
    "            \"warmup_target\": self.warmup_target,\n",
    "            \"total_epochs\": self.total_epochs,\n",
    "            \"alpha\": self.alpha\n",
    "        }\n",
    "\n",
    "model.summary()\n",
    "model.save(f'./NN_search/finetunetest1_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 23:34:30.949720: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype string and shape [39335]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2024-04-17 23:34:30.950086: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype double and shape [39335,6]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615/615 [==============================] - 645s 1s/step\n",
      "Scaler transforming target : X4_mean with scaler : PowerTransformer()\n",
      "Scaler transforming target : X11_mean with scaler : StandardScaler()\n",
      "Scaler transforming target : X18_mean with scaler : None\n",
      "Scaler transforming target : X50_mean with scaler : None\n",
      "Scaler transforming target : X26_mean with scaler : MinMaxScaler()\n",
      "Scaler transforming target : X3112_mean with scaler : None\n",
      "Logpot transforming target : X4_mean, log transform : 5\n",
      "Logpot transforming target : X11_mean, log transform : 15\n",
      "Logpot transforming target : X18_mean, log transform : sqrt\n",
      "Logpot transforming target : X50_mean, log transform : None\n",
      "Logpot transforming target : X26_mean, log transform : cbrt\n",
      "Logpot transforming target : X3112_mean, log transform : 15\n",
      "Train scores:\n",
      "R2 : 0.43560, MSE : 576052.93545, MAE : 160.91438\n"
     ]
    }
   ],
   "source": [
    "train_tabular = train_df[FEATURE_COLS].values\n",
    "valid_tabular = valid_df[FEATURE_COLS].values\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, train_tabular, y_train_transformed.values))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, valid_tabular, y_valid_transformed.values))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer, QuantileTransformer, RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "## TEST DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "train_pred = model.predict(train_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        train_pred[:, i] = scaler.inverse_transform(train_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        train_pred[:, i] = np.power(log_base, train_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        train_pred[:, i] = np.square(train_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        train_pred[:, i] = np.power(train_pred[:, i], 3)\n",
    "\n",
    "R2_train = r2_score(y_train, train_pred)\n",
    "MSE_train = mean_squared_error(y_train, train_pred)\n",
    "MAE_train = mean_absolute_error(y_train, train_pred)\n",
    "\n",
    "print(f'Train scores:\\nR2 : {R2_train:.5f}, MSE : {MSE_train:.5f}, MAE : {MAE_train:.5f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 23:46:15.788967: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [9833,81]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-04-17 23:46:15.789290: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype double and shape [9833,6]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154/154 [==============================] - 159s 1s/step\n",
      "Scaler transforming target : X4_mean with scaler : PowerTransformer()\n",
      "Scaler transforming target : X11_mean with scaler : StandardScaler()\n",
      "Scaler transforming target : X18_mean with scaler : None\n",
      "Scaler transforming target : X50_mean with scaler : None\n",
      "Scaler transforming target : X26_mean with scaler : MinMaxScaler()\n",
      "Scaler transforming target : X3112_mean with scaler : None\n",
      "Valid scores:\n",
      "R2 : 0.30615, MSE : 639503.31029, MAE : 173.09033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## VALIDATION DATA TEST\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "valid_pred = model.predict(valid_dataset, verbose=1)\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Scaler transforming target : {target} with scaler : {scaler_transforms[target]}')\n",
    "    scaler = scaler_transforms[target]\n",
    "    if scaler is not None:\n",
    "        valid_pred[:, i] = scaler.inverse_transform(valid_pred[:, i].reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    log_base = log_transforms[target]\n",
    "    if log_base is not None and log_base != 'sqrt' and log_base != 'cbrt':\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "    elif log_base == 'sqrt':\n",
    "        valid_pred[:, i] = np.square(valid_pred[:, i])\n",
    "    elif log_base == 'cbrt':\n",
    "        valid_pred[:, i] = np.power(valid_pred[:, i], 3)\n",
    "\n",
    "R2_valid = r2_score(y_valid, valid_pred)\n",
    "MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "\n",
    "print(f'Valid scores:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
