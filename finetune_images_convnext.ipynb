{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 16:24:41.775155: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-05 16:24:42.812948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/tobias/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/tobias/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.12.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n",
      "2024-05-05 16:24:44.121595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU allocator: cuda_malloc_async\n",
      "Setting memory growth for PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 16:24:44.262528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 16:24:44.262596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,  RobustScaler\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Input, Dense, Concatenate, Dropout\n",
    "from keras import regularizers, layers, optimizers, initializers\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetV2M, ConvNeXtBase\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "print(f'Current GPU allocator: {os.getenv(\"TF_GPU_ALLOCATOR\")}')\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            print(f'Setting memory growth for {gpu}')\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = '429_convnextBase_003_998_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Aseta näyttämään rajoittamaton määrä sarakkeita\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_file_path = './data/test_convnextbase_df_003_998.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "pickle_file_path = './data/train_convnextbase_df_003_998.pickle'\n",
    "\n",
    "with open(pickle_file_path, 'rb') as f:\n",
    "    train_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING IMAGE AUGEMENTATION\n",
    "# train_df = train_df.sample(1000)\n",
    "# test_df = test_df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = pd.read_csv('./data/test.csv')\n",
    "FEATURE_COLS = feat.columns[1:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_images_path = './data/train_images/'\n",
    "# test_images_path = './data/test_images/'    \n",
    "\n",
    "# train_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(train_images_path, f'{x}.jpeg'))\n",
    "# test_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(test_images_path, f'{x}.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in mean_columns:\n",
    "#     lower_quantile = train_df[column].quantile(0.005)\n",
    "#     upper_quantile = train_df[column].quantile(0.985)  \n",
    "#     train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# for i, trait in enumerate(mean_columns):\n",
    "\n",
    "#     # Determine the bin edges dynamically based on the distribution of traits\n",
    "#     bin_edges = np.percentile(train_df[trait], np.linspace(0, 100, 5 + 1))\n",
    "#     train_df[f\"bin_{i}\"] = np.digitize(train_df[trait], bin_edges)\n",
    "\n",
    "# # Concatenate the bins into a final bin\n",
    "# train_df[\"final_bin\"] = (\n",
    "#     train_df[[f\"bin_{i}\" for i in range(len(mean_columns))]]\n",
    "#     .astype(str)\n",
    "#     .agg(\"\".join, axis=1)\n",
    "# )\n",
    "\n",
    "# # Perform the stratified split using final bin\n",
    "# train_df = train_df.reset_index(drop=True)\n",
    "# for fold, (train_idx, valid_idx) in enumerate(skf.split(train_df, train_df[\"final_bin\"])):\n",
    "#     train_df.loc[valid_idx, \"fold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Num Train: 43088 | Num Valid: 10772\n"
     ]
    }
   ],
   "source": [
    "scaler_feat = RobustScaler()\n",
    "\n",
    "train_original = train_df.copy()\n",
    "train_plot = train_df.copy()\n",
    "sample_df = train_df.copy()\n",
    "\n",
    "train_df = sample_df[sample_df.fold != 1]\n",
    "valid_df = sample_df[sample_df.fold == 1]\n",
    "\n",
    "print(f\"# Num Train: {len(train_df)} | Num Valid: {len(valid_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best R2-score: 0.39703\n",
      "Best model: ./NN_search/429_convnextBase_003_998_1_best_val_0.39703_model.h5\n",
      "Opening log transforms from ./NN_search/429_convnextBase_003_998_1_0.39703_best_log_transforms.pickle\n",
      "Opening scalers from ./NN_search/429_convnextBase_003_998_1_0.39703_best_scalers.pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 16:24:58.936479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 16:24:58.936584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 16:24:58.936632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 16:24:59.112248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 16:24:59.112345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 16:24:59.112354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-05 16:24:59.112401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:07:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-05 16:24:59.112418: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:226] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-05-05 16:24:59.112629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7537 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " image_avg (InputLayer)      [(None, 1024)]            0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 1024)             4096      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2694)              2761350   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2694)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 963)               2595285   \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 963)              3852      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 963)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 713)               687332    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 713)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 4284      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,056,199\n",
      "Trainable params: 6,052,225\n",
      "Non-trainable params: 3,974\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "directory_path = './NN_search'\n",
    "pattern = f\"{directory_path}/{study_name}*.h5\"\n",
    "\n",
    "files = glob.glob(pattern)\n",
    "\n",
    "max_r2_score = float('-inf')\n",
    "best_model = None\n",
    "\n",
    "# Käy läpi jokainen tiedosto ja etsi suurin r2_score_inv\n",
    "for file in files:\n",
    "    value = float(file.split('best_val')[1].split('_')[1])\n",
    "    if value > max_r2_score:\n",
    "        max_r2_score = value\n",
    "        best_model = file\n",
    "\n",
    "\n",
    "# Tulosta suurin löydetty r2_score_inv ja vastaava tiedosto\n",
    "print(f\"Best R2-score: {max_r2_score:.5f}\")\n",
    "if best_model:\n",
    "    print(f\"Best model: {best_model}\")\n",
    "else:\n",
    "    print(\"No best model found\")\n",
    "\n",
    "best_log_transforms_name =  f'./NN_search/{study_name}_{max_r2_score:.5f}_best_log_transforms.pickle'\n",
    "best_scalers_name = f'./NN_search/{study_name}_{max_r2_score:.5f}_best_scalers.pickle'\n",
    "\n",
    "print(f'Opening log transforms from {best_log_transforms_name}')\n",
    "with open(best_log_transforms_name, 'rb') as f:\n",
    "    log_transforms = pickle.load(f)\n",
    "\n",
    "print(f'Opening scalers from {best_scalers_name}')\n",
    "with open(best_scalers_name, 'rb') as f:\n",
    "    scaler_transforms = pickle.load(f)\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "    \n",
    "custom_objects = {\"r2_score_tf\": r2_score_tf}\n",
    "\n",
    "nas_model  = tf.keras.models.load_model(best_model, custom_objects=custom_objects)\n",
    "\n",
    "nas_model.summary()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_features = nas_model.layers[-2].output\n",
    "nas_features = Model(inputs=nas_model.input, outputs=nas_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in nas_features.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nas_features.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainable_count_nas = sum([tf.size(v).numpy() for v in nas_model.trainable_weights])\n",
    "non_trainable_count_nas = sum([tf.size(v).numpy() for v in nas_model.non_trainable_weights])\n",
    "print(f\"Total parameters nas: {trainable_count_nas + non_trainable_count_nas:,}\")\n",
    "print(f\"Trainable parameters nas: {trainable_count_nas:,}\")\n",
    "print(f\"Non-trainable parameters nas: {non_trainable_count_nas:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scaler are: {scaler_transforms}')\n",
    "print(f'Log transforms are: {log_transforms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image(img):\n",
    "\n",
    "    img = img / 255.0\n",
    "    \n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    \n",
    "    img = tf.image.rot90(img, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32))\n",
    "\n",
    "    img = tf.image.random_brightness(img, max_delta=0.1)\n",
    "    img = tf.image.random_hue(img, max_delta=0.1)\n",
    "    img = tf.image.random_saturation(img, lower=0.9, upper=1.1)\n",
    "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
    "\n",
    "    img = tf.image.random_jpeg_quality(img, min_jpeg_quality=85, max_jpeg_quality=100)\n",
    "\n",
    "\n",
    "    crop_size = tf.random.uniform(shape=[], minval=420, maxval=480, dtype=tf.int32)\n",
    "    img = tf.image.random_crop(img, size=[crop_size, crop_size, 3])\n",
    "    img = tf.image.resize(img, [480, 480]) \n",
    "\n",
    "\n",
    "    angle = tf.random.uniform([], minval=-np.pi/8, maxval=np.pi/8, dtype=tf.float32)\n",
    "    img = tfa.image.rotate(img, angles=angle)\n",
    "    \n",
    "    img = tf.image.resize(img, [480, 480]) \n",
    "    img = img * 255.0\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "def process_image(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    img = augment_image(img)  \n",
    "    # img = tf.cast(img, tf.uint8)    \n",
    "\n",
    "    # tf.print(\"Final min and max in process_images:\", tf.reduce_min(img), tf.reduce_max(img))\n",
    "    # tf.print(\"Image type: \", img.dtype)\n",
    "\n",
    "    return img\n",
    "\n",
    "def process_image_valid(file_path):\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (480, 480))\n",
    "    # tf.print(\"Final min and max in process_image_valid:\", tf.reduce_min(img), tf.reduce_max(img))\n",
    "    # img = tf.cast(img, tf.uint8)\n",
    "    return img\n",
    "\n",
    "# Define your dataset processing function\n",
    "def process_path_train(file_path, targets):\n",
    "    img = process_image(file_path)\n",
    "    return img, targets\n",
    "\n",
    "\n",
    "def process_path_valid(file_path, targets):\n",
    "    img = process_image_valid(file_path)\n",
    "    return img, targets\n",
    "\n",
    "def process_path_test(file_path, dummy):\n",
    "    img = process_image_valid(file_path)    \n",
    "    return img, dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[mean_columns]\n",
    "y_valid = valid_df[mean_columns]\n",
    "\n",
    "\n",
    "y_train_transformed = y_train.copy()\n",
    "y_valid_transformed = y_valid.copy()\n",
    "\n",
    "for target, log_base in log_transforms.items():\n",
    "\n",
    "    y_train_transformed[target] = np.log(y_train[target]) / np.log(log_base)\n",
    "    y_valid_transformed[target] = np.log(y_valid[target]) / np.log(log_base)\n",
    "\n",
    "\n",
    "y_train_transformed = scaler_transforms.fit_transform(y_train_transformed)\n",
    "y_valid_transformed = scaler_transforms.transform(y_valid_transformed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len dataset: 7182\n",
      "Len all train: 86184\n"
     ]
    }
   ],
   "source": [
    "BUFFER_SIZE = 55000 \n",
    "EPOCHS = 12\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "train_images_path = train_df['image_path'].values\n",
    "valid_images_path = valid_df['image_path'].values\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images_path, y_train_transformed))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_images_path, y_valid_transformed))\n",
    "\n",
    "train_dataset = train_dataset.map(process_path_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.map(process_path_valid, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "len_dataset = len(train_dataset) \n",
    "len_all_train = len(train_dataset) * EPOCHS\n",
    "\n",
    "print(f'Len dataset: {len_dataset}')\n",
    "print(f'Len all train: {len_all_train}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCosineDecayWithWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_learning_rate, decay_steps, alpha=0.0, warmup_learning_rate=0.0, warmup_steps=0, name=None):\n",
    "        super().__init__()\n",
    "        self.initial_learning_rate = initial_learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.alpha = alpha\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.name = name\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"CustomCosineDecayWithWarmup\"):\n",
    "            # Lämpenemisvaihe\n",
    "            learning_rate = tf.cond(\n",
    "                step < self.warmup_steps,\n",
    "                lambda: self.warmup_learning_rate + step / self.warmup_steps * (self.initial_learning_rate - self.warmup_learning_rate),\n",
    "                lambda: self.initial_learning_rate\n",
    "            )\n",
    "            # Kosinilasku lämpenemisen jälkeen\n",
    "            cosine_decay = tf.keras.optimizers.schedules.CosineDecay(\n",
    "                initial_learning_rate=self.initial_learning_rate,\n",
    "                decay_steps=self.decay_steps,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            decayed_learning_rate = cosine_decay(step - self.warmup_steps)\n",
    "            \n",
    "            return tf.cond(step < self.warmup_steps, lambda: learning_rate, lambda: decayed_learning_rate)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"initial_learning_rate\": self.initial_learning_rate,\n",
    "            \"decay_steps\": self.decay_steps,\n",
    "            \"alpha\": self.alpha,\n",
    "            \"warmup_learning_rate\": self.warmup_learning_rate,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "    try: \n",
    "        ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "        ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "        r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "        r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "        return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "    except Exception as e:\n",
    "        # print(f'Error in r2_score_tf: {e}')\n",
    "        return float('-inf')\n",
    "\n",
    "\n",
    "\n",
    "image_input_avg = Input(shape=(480, 480, 3), name='image_input_avg')\n",
    "\n",
    "eff_avg_base =  ConvNeXtBase(weights='imagenet', include_top=False, pooling='avg', input_tensor=image_input_avg)\n",
    "\n",
    "eff_avg_base.trainable = True\n",
    "for layer in eff_avg_base.layers[:-150]:\n",
    "    layer.trainable = False\n",
    "\n",
    "nas_features_layer = nas_features(eff_avg_base.output)\n",
    "pass_through_layer = Lambda(lambda x: x, name='nas_features')(nas_features_layer)\n",
    "\n",
    "nas_output = Dense(6, activation='linear', name='final_tune_output')(pass_through_layer)\n",
    "\n",
    "finetune_model = Model(inputs=image_input_avg, outputs=nas_output, name='finetune_model')\n",
    "\n",
    "lr_schedule = CustomCosineDecayWithWarmup(\n",
    "    initial_learning_rate=1e-4,\n",
    "    decay_steps=len_dataset * (EPOCHS ),\n",
    "    alpha=0.0,\n",
    "    warmup_learning_rate=1e-5,\n",
    "    warmup_steps=len_dataset ,\n",
    "    name=\"CosineDecayWithWarmup\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Aseta oppimisnopeuden aikataulu\n",
    "finetune_model.compile(optimizer=optimizers.Adam(learning_rate=lr_schedule), loss='mae', metrics=['mse', 'mae', 'mape', r2_score_tf])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_count = sum([tf.size(v).numpy() for v in finetune_model.trainable_weights])\n",
    "non_trainable_count = sum([tf.size(v).numpy() for v in finetune_model.non_trainable_weights])\n",
    "print(f\"Total parameters: {trainable_count + non_trainable_count:,}\")\n",
    "print(f\"Trainable parameters: {trainable_count:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_count:,}\")\n",
    "\n",
    "print(f'Total parameters from ConvNext: { (trainable_count + non_trainable_count) - (trainable_count_nas + non_trainable_count_nas):,}')\n",
    "print(f'Trainable from ConvNext: {trainable_count - trainable_count_nas:,}')\n",
    "print(f'Non trainable from ConvNext: {non_trainable_count - non_trainable_count_nas:,}')\n",
    "\n",
    "print(f'Trainable fron NAS: {trainable_count_nas:,}')\n",
    "print(f'Non trainable from NAS: {non_trainable_count_nas:,}')\n",
    "print(F'Total parameters from NAS: {trainable_count_nas + non_trainable_count_nas:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "class TrainImageLoggingCallback(Callback):\n",
    "    def __init__(self, log_dir, data):\n",
    "        super(TrainImageLoggingCallback, self).__init__()\n",
    "        self.log_dir = log_dir\n",
    "        self.data = data\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Fetch a batch of images\n",
    "        for imgs, tar in self.data.take(1):  # Adjust depending on your dataset structure\n",
    "            \n",
    "            # augmented_images = tf.map_fn(augment_image, imgs)\n",
    "            augmented_images = tf.cast(imgs, tf.uint8)    \n",
    "        \n",
    "            # Prepare the image to write to TensorBoard\n",
    "            with self.writer.as_default():\n",
    "                tf.summary.image(\"Augmented Images\", augmented_images, step=epoch, max_outputs=20)\n",
    "\n",
    "            self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "log_folder = f\"./logs/lr_test/trial_{study_name}_503\" # MUUTA!\n",
    "\n",
    "print(f'Logging tensorboard to {log_folder}')\n",
    "os.makedirs(log_folder, exist_ok=True)\n",
    "\n",
    "# Aseta logitiedostojen hakemisto\n",
    "tensorboard_callback = TensorBoard(log_dir=log_folder, histogram_freq=1, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = './NN_search/model_epoch_{epoch:02d}.h5'\n",
    "\n",
    "model_checkpoint_callback =  tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    save_freq='epoch',  # Tallenna jokaisen eepokin jälkeen\n",
    "    verbose=1,  # Näytä viesti tallennuksesta\n",
    "    save_best_only=False  # Tallenna jokainen eepokki, älä vain parasta\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    model_checkpoint_callback,\n",
    "    tensorboard_callback,\n",
    "    TrainImageLoggingCallback(log_folder, train_dataset)    \n",
    "]\n",
    "\n",
    "history = finetune_model.fit(train_dataset, validation_data=valid_dataset, epochs=EPOCHS, verbose=1, callbacks=callbacks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 16:25:36.109085: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [10772,6]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-05-05 16:25:36.109406: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype double and shape [10772,6]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-05-05 16:25:39.701701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2024-05-05 16:25:40.447809: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x1dffadb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-05 16:25:40.447857: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n",
      "2024-05-05 16:25:41.371934: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-05-05 16:25:41.392758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:637] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1796/1796 [==============================] - 393s 213ms/step\n",
      "Model at epoch 12:\n",
      "R2 : 0.37422, MSE : 1739709.43855, MAE : 208.50698, MAPE : 1.43812\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class LayerScale(layers.Layer):\n",
    "    \"\"\"Layer scale module.\n",
    "\n",
    "    References:\n",
    "      - https://arxiv.org/abs/2103.17239\n",
    "\n",
    "    Args:\n",
    "      init_values (float): Initial value for layer scale. Should be within\n",
    "        [0, 1].\n",
    "      projection_dim (int): Projection dimensionality.\n",
    "\n",
    "    Returns:\n",
    "      Tensor multiplied to the scale.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, init_values, projection_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.init_values = init_values\n",
    "        self.projection_dim = projection_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = tf.Variable(\n",
    "            self.init_values * tf.ones((self.projection_dim,))\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        return x * self.gamma\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"init_values\": self.init_values,\n",
    "                \"projection_dim\": self.projection_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "class StochasticDepth(layers.Layer):\n",
    "    \"\"\"Stochastic Depth module.\n",
    "\n",
    "    It performs batch-wise dropping rather than sample-wise. In libraries like\n",
    "    `timm`, it's similar to `DropPath` layers that drops residual paths\n",
    "    sample-wise.\n",
    "\n",
    "    References:\n",
    "      - https://github.com/rwightman/pytorch-image-models\n",
    "\n",
    "    Args:\n",
    "      drop_path_rate (float): Probability of dropping paths. Should be within\n",
    "        [0, 1].\n",
    "\n",
    "    Returns:\n",
    "      Tensor either with the residual path dropped or kept.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_path_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.drop_path_rate = drop_path_rate\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        if training:\n",
    "            keep_prob = 1 - self.drop_path_rate\n",
    "            shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
    "            random_tensor = keep_prob + tf.random.uniform(shape, 0, 1)\n",
    "            random_tensor = tf.floor(random_tensor)\n",
    "            return (x / keep_prob) * random_tensor\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"drop_path_rate\": self.drop_path_rate})\n",
    "        return config\n",
    "\n",
    "\n",
    "def r2_score_tf(y_true, y_pred):\n",
    "\n",
    "        try: \n",
    "            ss_res = tf.reduce_sum(tf.square(y_true - y_pred), axis=0)\n",
    "            ss_tot = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true, axis=0)), axis=0)\n",
    "            r2 = 1 - ss_res/(ss_tot + tf.keras.backend.epsilon())\n",
    "            r2 = tf.where(tf.math.is_nan(r2), tf.zeros_like(r2), r2) \n",
    "            return tf.reduce_mean(tf.maximum(r2, 0.0))\n",
    "        except Exception as e:\n",
    "            # print(f'Error in r2_score_tf: {e}')\n",
    "            return float('-inf')\n",
    "\n",
    "\n",
    "    \n",
    "custom_objects = {\"r2_score_tf\": r2_score_tf, \"LayerScale\": LayerScale, \"StochasticDepth\": StochasticDepth, \"CustomCosineDecayWithWarmup\": CustomCosineDecayWithWarmup}\n",
    "\n",
    "for epoch in range(12, EPOCHS+1):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    model = tf.keras.models.load_model(f'./NN_search/model_epoch_{epoch:02d}.h5', custom_objects=custom_objects)\n",
    "    valid_pred = model.predict(valid_dataset, verbose=1)\n",
    "\n",
    "    valid_pred = scaler_transforms.inverse_transform(valid_pred)\n",
    "\n",
    "\n",
    "    for i, target in enumerate(mean_columns):\n",
    "        log_base = log_transforms[target]\n",
    "        valid_pred[:, i] = np.power(log_base, valid_pred[:, i])\n",
    "\n",
    "\n",
    "    R2_valid = r2_score(y_valid, valid_pred)\n",
    "    MSE_valid = mean_squared_error(y_valid, valid_pred)\n",
    "    MAE_valid = mean_absolute_error(y_valid, valid_pred)\n",
    "    MAPE_valid = mean_absolute_percentage_error(y_valid, valid_pred)\n",
    "    print(f'Model at epoch {epoch}:\\nR2 : {R2_valid:.5f}, MSE : {MSE_valid:.5f}, MAE : {MAE_valid:.5f}, MAPE : {MAPE_valid:.5f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_model = tf.keras.models.load_model('./NN_search/model_epoch_09.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, log_base in log_transforms.items():\n",
    "   \n",
    "    train_plot[target] = np.log(train_plot[target]) / np.log(log_base)\n",
    "   \n",
    "        \n",
    "train_plot[mean_columns] = scaler_transforms.transform(train_plot[mean_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(log_transforms)\n",
    "print(scaler_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_plot[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(df, columns_names):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "\n",
    "    # Setting up a grid of plots with 2 columns\n",
    "    n_cols = 6\n",
    "    n_rows = len(columns_names) // n_cols + (len(columns_names) % n_cols > 0)\n",
    "\n",
    "    for i, col in enumerate(columns_names):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        sns.kdeplot(df[col], bw_adjust=0.5, fill=False, color='blue')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Density')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_original, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(train_plot, mean_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_training_name = './data/results_finetune_images.pickle'\n",
    "\n",
    "if os.path.exists(results_training_name):\n",
    "    results_training = pd.read_pickle(results_training_name)\n",
    "else:\n",
    "    columns = ['Train R2', 'Train MSE', 'Train MAE', 'Train MAPE', 'Valid R2', 'Valid MSE', 'Valid MAE', 'Valid MAPE', 'Train preds Desc', 'Valid preds Desc', 'Test preds Desc' , 'Original data Desc' 'Kaggle R2', 'Scalers', 'Log/Pot transforms']\n",
    "    results_training = pd.DataFrame(columns = columns)\n",
    "    results_training.index.name = 'Study name'\n",
    "\n",
    "study_name_result = f'{study_name}_finetuned'\n",
    "\n",
    "if study_name_result not in results_training.index:    \n",
    "    results_training.loc[study_name] = [None]*len(results_training.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATA \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "gc.collect()\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "submission_df = test_df_copy[['id']].copy()\n",
    "\n",
    "test_images_path = test_df_copy['image_path'].values\n",
    "\n",
    "dummy_y = np.zeros((len(test_df_copy), 6))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_images_path, dummy_y))\n",
    "test_dataset = test_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "predictions = finetune_model.predict(test_dataset, verbose=1)\n",
    "\n",
    "predictions = scaler_transforms.inverse_transform(predictions)\n",
    "\n",
    "\n",
    "for i, target in enumerate(mean_columns):\n",
    "    print(f'Logpot transforming target: : {target}, log transform : {log_transforms[target]}')\n",
    "    log_base = log_transforms[target] \n",
    "    predictions[:, i] = np.power(log_base, predictions[:, i])\n",
    " \n",
    "\n",
    "# test_preds_desc =  pd.DataFrame(predictions, columns = mean_columns).describe().to_json()\n",
    "# results_training.at[study_name_result, 'Test preds Desc'] = test_preds_desc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[mean_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns = mean_columns)\n",
    "predictions_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_columns = ['X4', 'X11', 'X18', 'X50', 'X26', 'X3112']\n",
    "\n",
    "submission_df[target_columns] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_desc = train_original[mean_columns].describe().to_json()\n",
    "results_training.at[study_name_result, 'Original data Desc'] = original_data_desc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{str(log_transforms.items())}')\n",
    "print(f'{str(scaler_transforms)}')\n",
    "\n",
    "results_training.at[study_name_result, 'Scalers'] = f'{scaler_transforms}'\n",
    "results_training.at[study_name_result, 'Log/Pot transforms'] = f'{str(log_transforms.items())}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_training.at[study_name_result, 'Kaggle R2'] = None\n",
    "\n",
    "\n",
    "# # results_training.drop('423_std_powerlog_3', inplace=True)\n",
    "# # results_training.head()\n",
    "\n",
    "# for index, row in results_training.iterrows():\n",
    "#     print(f\"Study Name: {index}\")\n",
    "#     print(f'Kaggle R2: {row[\"Kaggle R2\"]}')\n",
    "#     print(f\"Train R2: {row['Train R2']}, Train MSE: {row['Train MSE']}, Train MAE : {row['Train MAE']}, Train MAPE: {row['Train MAPE']}\")\n",
    "#     print(f'Valid R2: {row[\"Valid R2\"]}, Valid MSE: {row[\"Valid MSE\"]}, Valid MAE: {row[\"Valid MAE\"]}, Valid MAPE: {row[\"Valid MAPE\"]}')\n",
    "#     print(\"-\" * 50)\n",
    "#     print(\"Train preds Description:\")\n",
    "#     display(pd.read_json(row['Train preds Desc']))\n",
    "#     print(\"Valid preds Description:\")\n",
    "#     display(pd.read_json(row['Valid preds Desc']))\n",
    "#     print(\"Test preds Description:\")\n",
    "#     display(pd.read_json(row['Test preds Desc']))\n",
    "#     print(\"Original data Description:\")\n",
    "#     display(pd.read_json(row['Original data Desc']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission_df.info())\n",
    "\n",
    "submission_df.to_csv('./data/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(results_training_name, 'wb') as f:\n",
    "    results_training.to_pickle(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_model_output = finetune_model.get_layer('nas_features')\n",
    "feature_model = Model(inputs=finetune_model.input, outputs=feature_model_output.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_paths = train_original['image_path'].values\n",
    "dummy_y = np.zeros((len(train_original), 6))\n",
    "\n",
    "train_all_dataset = tf.data.Dataset.from_tensor_slices((train_all_paths, dummy_y))\n",
    "train_all_dataset = train_all_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_all_dataset = train_all_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "train_features = feature_model.predict(train_all_dataset, verbose=1)\n",
    "\n",
    "train_original[f'model_features_{study_name}'] = train_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = f'./data/train_{study_name}.pickle'\n",
    "\n",
    "print(f'Saving train_df to {pickle_file_path}')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(train_original, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all_paths = test_df['image_path'].values\n",
    "dummy_y = np.zeros((len(test_df), 6))\n",
    "\n",
    "test_all_dataset = tf.data.Dataset.from_tensor_slices((test_all_paths, dummy_y))\n",
    "test_all_dataset = test_all_dataset.map(process_path_test, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test_all_dataset = test_all_dataset.batch(32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_features = feature_model.predict(test_all_dataset, verbose=1)\n",
    "\n",
    "test_df[f'model_features_{study_name}'] = test_features.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[f'model_features_{study_name}'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file_path = f'./data/test_{study_name}.pickle'\n",
    "\n",
    "print(f'Saving test_df to {pickle_file_path}')\n",
    "with open(pickle_file_path, 'wb') as f:\n",
    "    pickle.dump(test_df, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
